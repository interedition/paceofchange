{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defactoring Pace of Change\n",
    "\n",
    "\n",
    "The code expressed below has nine steps:\n",
    "* [Importing Libraries](#Importing-Libraries) - Loads the necessary Python libraries needed for the analysis.\n",
    "* [Setting Parameters](#Setting-Parameters) - Specifies parameters for the loading, cleaning, and labeling of data as well as sets conditions for the logistic regression.\n",
    "* [Preparing Metadata](#Preparing-MetaData) - Generates a list of *.tsv files from the `poems/` directory. \n",
    "    * [Cleaning Metadata](#Cleaning-Metadata) - Loads the metadata file, `poemetadata.csv` and performs some cleaning of the metadata to make labeling easier.\n",
    "    * [Sorting Training Data](#Sorting-Data) - Sort the volumes into two bins, reviewed and not reviewed using the cleaned metadata.\n",
    "* [Transforming Words into Features](#Transforming-Words-into-Features) - Identifies the 3,200 most common words in the corpus. Those most common words will be the features for the regression.\n",
    "    * [Filtering Authors](#Filtering-Authors) - Removes poems by authors who have been reviewed.\n",
    "    * [Filtering Words](#Filtering-Words) - Remove any words from the poem data that are not in the most-common feature list.\n",
    "* [Training Predictive Models](#Training-Predictive-Models) - Run a separate logistic regression for each volume, using a single volume as held-out data and measure each model's predictive power.\n",
    "* [Modeling Coefficients](#Modeling-Coefficients) - Run a single logistic regression over all the data to inspect the salient coefficients.\n",
    "* [Plotting Results](#Plotting-Results) - Generate a plot showing the accuracy of the predictive models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "This section loads a series of libraries used in the Pace of Change analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING IMPORT\n",
    "import os\n",
    "import csv \n",
    "import random \n",
    "from collections import Counter \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from multiprocessing import Pool \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin examination of the code by importing a series of Python libraries into working memory. This is the boundary between the layers of bespoke code and existing general purpose (os, csv, random) and scientific computing libraries (numpy, pandas, sklearn). Following from Hinsen’s model of layers of scientific software, what is missing is the inclusion of libraries from the *disciplinary* layer. The most specific library in terms of use in the Hinsen model is the LogisticRegression model from Scikit-Learn, but we’d argue this lives in layer two, scientific software, because it  is broadly applicable across a variety of disciplines. This begs the question, what or where are the disciplinary Python libraries for literary history or digital humanities? What functions would the perform? What domain specific tasks or methods need to encoded into a disciplinary library? Perhaps it is too early for such libraries to exist as the practices of computational and data intensive research are still new (in literary history)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Parameters\n",
    "\n",
    "The first section of the code sets a series of parameters specifying what data to process, where data are located, and parameters for the logistic regression. While there is no complex logic or work being done in this section, many assumptions and important distinctions that shape the execution of subsequent code are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PATHS.\n",
    "\n",
    "sourcefolder = 'poems/'\n",
    "extension = '.poe.tsv'\n",
    "classpath = 'poemeta.csv'\n",
    "outputpath = 'mainmodelpredictions.csv'\n",
    "\n",
    "## EXCLUSIONS.\n",
    "\n",
    "excludeif = dict()\n",
    "excludeif['pubname'] = 'TEM'\n",
    "# We're not using reviews from Tait's.\n",
    "\n",
    "excludeif['recept'] = 'addcanon'\n",
    "# We don't ordinarily include canonical volumes that were not in either sample.\n",
    "# These are included only if we're testing the canon specifically.\n",
    "\n",
    "excludeifnot = dict()\n",
    "excludeabove = dict()\n",
    "excludebelow = dict()\n",
    "\n",
    "excludebelow['firstpub'] = 1700\n",
    "excludeabove['firstpub'] = 1950\n",
    "sizecap = 360\n",
    "\n",
    "# For more historically-interesting kinds of questions, we can limit the part\n",
    "# of the dataset that gets TRAINED on, while permitting the whole dataset to\n",
    "# be PREDICTED. (Note that we always exclude authors from their own training\n",
    "# set; this is in addition to that.) The variables futurethreshold and\n",
    "# pastthreshold set the chronological limits of the training set, inclusive\n",
    "# of the threshold itself.\n",
    "\n",
    "## THRESHOLDS\n",
    "\n",
    "futurethreshold = 1925\n",
    "pastthreshold = 1800\n",
    "\n",
    "# CLASSIFY CONDITIONS\n",
    "\n",
    "positive_class = 'rev'\n",
    "category2sorton = 'reviewed'\n",
    "datetype = 'firstpub'\n",
    "numfeatures = 3200\n",
    "regularization = .00007\n",
    "\n",
    "\n",
    "paths = (sourcefolder, extension, classpath, outputpath)\n",
    "exclusions = (excludeif, \n",
    "              excludeifnot, \n",
    "              excludebelow, \n",
    "              excludeabove, \n",
    "              sizecap)\n",
    "thresholds = (pastthreshold, \n",
    "              futurethreshold)\n",
    "classifyconditions = (category2sorton, \n",
    "                      positive_class, \n",
    "                      datetype, \n",
    "                      numfeatures, \n",
    "                      regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters defined in the code cell above are a set of knobs and switches used to tweak the performance and execution of the computational modeling process. Underwood and Sellers have collected the parameters into four categories: *paths*, *exclusions*, *thresholds*, and *classifyconditions*. These categories are simultaneously distinguished discursively through the code comments (the lines beginning with a #) and technologically through four variable assignments, `exclusions`, `thresholds`, and `classifyconditions`. Because technically speaking the grouping of parameters is not strictly necessary, each of these four variables embody stylistic choices of the authors as a means of organizing and structuring the information they are encoding in Python.    \n",
    "\n",
    "The variables in `paths` specify the location of the data and metadata files as well as where to write the output files at the completion of the analysis. The variables in `exclusions` specify data and types of data to be excluded from the analysis, such as reviews from *Tait's Endinburgh Magazine* (https://en.wikipedia.org/wiki/Tait%27s_Edinburgh_Magazine), which we infer from the author's comments. Additional exclusions specify temporal boundaries from 1700 to 1950. A further set of two variables in `thresholds` also articulates a temporal boundary from 1800 to 1925. The comments indicate this distinguishes the temporal window for datasets using in *training* versus those used during *prediction.* The variables in `classifyconditions` are important parameters for the logistic regression, specifying the number of variables to train the model upon as well as setting the regularization parameter (`regularization`) for the logistic regression. What is not well documented here, is why the value .00007 was chosen over other values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Metadata\n",
    "\n",
    "With the preparation of metadata we begin to see some logical work of Pace of Change being conducted. The code in this section has two subsections, one to clean the metadata and another to sort the training data. All of the work in this section focuses on preparing the metadata, identified in the `classpath` variable and the filenames of the individual data files in the `sourcefolder`. The main task of this section is to organize the metadata of the volumes and their associated labels (positive or negative) for training the logistic regression. All of the code in this section attends to the cleanliness of the metadata; we will not start digging into the data itself until the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION\n",
    "### def create_model(paths, exclusions, thresholds, classifyconditions):\n",
    "''' This is the main function in the module.\n",
    "It can be called externally; it's also called\n",
    "if the module is run directly.\n",
    "'''\n",
    "verbose = False\n",
    "\n",
    "if not sourcefolder.endswith('/'):\n",
    "    sourcefolder = sourcefolder + '/'\n",
    "\n",
    "# This just makes things easier.\n",
    "\n",
    "# Get a list of files.\n",
    "allthefiles = os.listdir(sourcefolder)\n",
    "# random.shuffle(allthefiles)\n",
    "\n",
    "volumeIDs = list()\n",
    "volumepaths = list()\n",
    "\n",
    "for filename in allthefiles:\n",
    "\n",
    "    if filename.endswith(extension):\n",
    "        volID = filename.replace(extension, \"\")\n",
    "        # The volume ID is basically the filename minus its extension.\n",
    "        # Extensions are likely to be long enough that there is little\n",
    "        # danger of accidental occurrence inside a filename. E.g.\n",
    "        # '.fic.tsv'\n",
    "        path = sourcefolder + filename\n",
    "        volumeIDs.append(volID)\n",
    "        volumepaths.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code assembles a list of volume identifiers (`volumeIDs`) and file paths (`volumepaths`) by readings the directory listing of files in the `poems/` directory (`sourcefolder`). The filenames are in and of themselves a source of metadata, but as we will see in the code below, they need to be reconciled with the metadata stored separately from the data files. \n",
    "\n",
    "We are curious about the contents of the `volumeIDs` and `volumepaths` variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first item in volumeIDs is:  loc.ark+=13960=t5n881k59\n",
      "The first item in volumepaths is:  poems/loc.ark+=13960=t5n881k59.poe.tsv\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### Inspect the two variables defined in the codecell above.\n",
    "### We know they are lists so lets just look at the first item.\n",
    "print(\"The first item in volumeIDs is: \", volumeIDs[0])\n",
    "print(\"The first item in volumepaths is: \",volumepaths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code has created an alignment between identifiers in the metadata records and the filename identifiers of the TSV data files themselves (located in the `poems/` folder). These identifiers, `dul1.ark+=13960=t5fb5xg2z`, are the threads that stitch together the various representations of the (meta)data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION DEFINITION\n",
    "### we need these helper functions for execute the next code cell\n",
    "\n",
    "def dirty_pairtree(htid):\n",
    "    period = htid.find('.')\n",
    "    prefix = htid[0:period]\n",
    "    postfix = htid[(period+1): ]\n",
    "    if '=' in postfix:\n",
    "        postfix = postfix.replace('+',':')\n",
    "        postfix = postfix.replace('=','/')\n",
    "    dirtyname = prefix + \".\" + postfix\n",
    "    return dirtyname\n",
    "\n",
    "def forceint(astring):\n",
    "    try:\n",
    "        intval = int(astring)\n",
    "    except:\n",
    "        intval = 0\n",
    "\n",
    "    return intval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell defines two functions used in the code below. The first is `dirty_pairtree()`, which cleans up the identifiers in the data. This issue arises from the fact that the HathiTrust (where Underwood and Sellers got their data from) uses IDs that cannot be expressed on the filesystem and Underwood and Sellers encoded ID metadata in filenames. The “/” and “:” characters in the IDs cannot be part of a file name. So, because the volumes are stored as individual files they have a “+” and an “=” instead. However, the IDs are stored in the original format in the metadata file so the IDS have to be transformed back into the original HathiTrust format.  The second function is called `forceint()` and transforms numbers expressed as Python strings into the python integer data type with a bit of error handling in the case values that throw and error when being cast as an integer.\n",
    "\n",
    "What does the the metadata look like? We can inspect the beginning of file to get a sense of the material conditions of the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The metadata files has 728 rows and 22 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>actualdate</th>\n",
       "      <th>inferreddate</th>\n",
       "      <th>firstpub</th>\n",
       "      <th>recept</th>\n",
       "      <th>recordid</th>\n",
       "      <th>OCLC</th>\n",
       "      <th>author</th>\n",
       "      <th>imprint</th>\n",
       "      <th>enumcron</th>\n",
       "      <th>...</th>\n",
       "      <th>judge</th>\n",
       "      <th>impaud</th>\n",
       "      <th>yrrev</th>\n",
       "      <th>pubname</th>\n",
       "      <th>birth</th>\n",
       "      <th>gender</th>\n",
       "      <th>nationality</th>\n",
       "      <th>othername</th>\n",
       "      <th>notes</th>\n",
       "      <th>canon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t8sb4zz1q</td>\n",
       "      <td>1921</td>\n",
       "      <td>1921</td>\n",
       "      <td>1921</td>\n",
       "      <td>addcanon</td>\n",
       "      <td>537314.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lawrence, D. H.</td>\n",
       "      <td>New York;T. Seltzer;1921.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1885</td>\n",
       "      <td>m</td>\n",
       "      <td>uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uc1.b3342759</td>\n",
       "      <td>1919</td>\n",
       "      <td>1919</td>\n",
       "      <td>1919</td>\n",
       "      <td>random</td>\n",
       "      <td>7930862.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wigren, Bessie C.</td>\n",
       "      <td>Boston;The Poet Lore Company;c1919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1874</td>\n",
       "      <td>f</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uc1.b4100590</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>6154122.0</td>\n",
       "      <td>2143179.0</td>\n",
       "      <td>Waugh, Alec,</td>\n",
       "      <td>London;G. Richards;1918.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>EGO</td>\n",
       "      <td>1898</td>\n",
       "      <td>m</td>\n",
       "      <td>uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uc1.b3340220</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>7917249.0</td>\n",
       "      <td>12688503.0</td>\n",
       "      <td>Nightingale, M.</td>\n",
       "      <td>Oxford [Oxfordshire;B.H. Blackwell;1918.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>EGO</td>\n",
       "      <td>1879</td>\n",
       "      <td>f</td>\n",
       "      <td>uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uc2.ark+=13960=t0ft8gj1k</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>7657411.0</td>\n",
       "      <td>2518108.0</td>\n",
       "      <td>Faber, Geoffrey,</td>\n",
       "      <td>Oxford;B. H. Blackwell;New York;Longmans, Gree...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>EGO</td>\n",
       "      <td>1889</td>\n",
       "      <td>m</td>\n",
       "      <td>uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docid actualdate  inferreddate  firstpub    recept  \\\n",
       "0  loc.ark+=13960=t8sb4zz1q       1921          1921      1921  addcanon   \n",
       "1              uc1.b3342759       1919          1919      1919    random   \n",
       "2              uc1.b4100590       1918          1918      1918  reviewed   \n",
       "3              uc1.b3340220       1918          1918      1918  reviewed   \n",
       "4  uc2.ark+=13960=t0ft8gj1k       1918          1918      1918  reviewed   \n",
       "\n",
       "    recordid        OCLC             author  \\\n",
       "0   537314.0         NaN    Lawrence, D. H.   \n",
       "1  7930862.0         NaN  Wigren, Bessie C.   \n",
       "2  6154122.0   2143179.0       Waugh, Alec,   \n",
       "3  7917249.0  12688503.0    Nightingale, M.   \n",
       "4  7657411.0   2518108.0   Faber, Geoffrey,   \n",
       "\n",
       "                                             imprint enumcron  ...  judge  \\\n",
       "0                          New York;T. Seltzer;1921.      NaN  ...    NaN   \n",
       "1                 Boston;The Poet Lore Company;c1919      NaN  ...    NaN   \n",
       "2                           London;G. Richards;1918.      NaN  ...    NaN   \n",
       "3           Oxford [Oxfordshire;B.H. Blackwell;1918.      NaN  ...    neg   \n",
       "4  Oxford;B. H. Blackwell;New York;Longmans, Gree...      NaN  ...    NaN   \n",
       "\n",
       "  impaud   yrrev pubname  birth gender nationality othername notes canon  \n",
       "0    NaN     NaN     NaN   1885      m          uk       NaN   NaN     y  \n",
       "1    NaN     NaN     NaN   1874      f          us       NaN   NaN   NaN  \n",
       "2    NaN  1918.0     EGO   1898      m          uk       NaN   NaN   NaN  \n",
       "3    NaN  1919.0     EGO   1879      f          uk       NaN   NaN   NaN  \n",
       "4    NaN  1918.0     EGO   1889      m          uk       NaN   NaN   NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "metadata_file = pd.read_csv(classpath)\n",
    "#print(metadata_file.shape)\n",
    "print(\"The metadata files has {} rows and {} columns.\".format(\n",
    "    *metadata_file.shape))\n",
    "metadata_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using *defactoring inspection* we can actually look at the metadata file and inspect the first five rows of metadata. By blending the original code with our inspection code and narrative the metadata becomes less of a conceptual abstraction and more of a tangible, material object that we can interrogate. Here we can see the file has 728 rows and 22 columns as well as the contents of the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poemeta.csv\n",
      "DEFACTORING: Excluding volume with id loc.ark:/13960/t8sb4zz1q\n",
      "DEFACTORING: Excluding volume with id mdp.39015013402501\n",
      "DEFACTORING: Excluding volume with id mdp.39015011913525\n",
      "DEFACTORING: Excluding volume with id hardywessexpoems189.hardywessexpoems1898\n",
      "DEFACTORING: Excluding volume with id gerardmhopkins191.gerardmhopkins1918\n",
      "DEFACTORING: Excluding volume with id loc.ark:/13960/t3fx82c2q\n",
      "DEFACTORING: Excluding volume with id emilydickinso.emilydickinson\n",
      "DEFACTORING: Excluding volume with id ellisbell184.ellisbell1848\n",
      "We have 8 volumes in missing in metadata, and\n",
      "0 volumes missing in the directory.\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING FUNCTION \n",
    "### def get_metadata(classpath, volumeIDs, excludeif, excludeifnot, excludebelow, excludeabove):\n",
    "'''\n",
    "As the name would imply, this gets metadata matching a given set of volume\n",
    "IDs. It returns a dictionary containing only those volumes that were present\n",
    "both in metadata and in the data folder.\n",
    "\n",
    "It also accepts four dictionaries containing criteria that will exclude volumes\n",
    "from the modeling process.\n",
    "'''\n",
    "print(classpath)\n",
    "metadict = dict()\n",
    "\n",
    "with open(classpath, encoding = 'utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    anonctr = 0\n",
    "\n",
    "    for row in reader:\n",
    "        volid = dirty_pairtree(row['docid'])\n",
    "        theclass = row['recept'].strip()\n",
    "\n",
    "        # I've put 'remove' in the reception column for certain\n",
    "        # things that are anomalous.\n",
    "        if theclass == 'remove':\n",
    "            continue\n",
    "\n",
    "        bail = False\n",
    "        for key, value in excludeif.items():\n",
    "            if row[key] == value:\n",
    "                bail = True\n",
    "        for key, value in excludeifnot.items():\n",
    "            if row[key] != value:\n",
    "                bail = True\n",
    "        for key, value in excludebelow.items():\n",
    "            if forceint(row[key]) < value:\n",
    "                bail = True\n",
    "        for key, value in excludeabove.items():\n",
    "            if forceint(row[key]) > value:\n",
    "                bail = True\n",
    "\n",
    "        if bail:\n",
    "            print(\"DEFACTORING: Excluding volume with id \"+volid) ### DEFACTORING CODE\n",
    "            continue\n",
    "\n",
    "        birthdate = forceint(row['birth'])\n",
    "\n",
    "        pubdate = forceint(row['inferreddate'])\n",
    "\n",
    "        gender = row['gender'].rstrip()\n",
    "        nation = row['nationality'].rstrip()\n",
    "\n",
    "        #if pubdate >= 1880:\n",
    "            #continue\n",
    "\n",
    "        if nation == 'ca':\n",
    "            nation = 'us'\n",
    "        elif nation == 'ir':\n",
    "            nation = 'uk'\n",
    "        # I hope none of my Canadian or Irish friends notice this.\n",
    "\n",
    "        notes = row['notes'].lower()\n",
    "        author = row['author']\n",
    "        if len(author) < 1 or author == '<blank>':\n",
    "            author = \"anonymous\" + str(anonctr)\n",
    "            anonctr += 1\n",
    "\n",
    "        title = row['title']\n",
    "        canon = row['canon']\n",
    "\n",
    "        # I'm creating two distinct columns to indicate kinds of\n",
    "        # literary distinction. The reviewed column is based purely\n",
    "        # on the question of whether this work was in fact in our\n",
    "        # sample of contemporaneous reviews. The obscure column incorporates\n",
    "        # information from post-hoc biographies, which trumps\n",
    "        # the question of reviewing when they conflict.\n",
    "\n",
    "        if theclass == 'random':\n",
    "            obscure = 'obscure'\n",
    "            reviewed = 'not'\n",
    "        elif theclass == 'reviewed':\n",
    "            obscure = 'known'\n",
    "            reviewed = 'rev'\n",
    "        elif theclass == 'addcanon':\n",
    "            print(\"DEFACTORING: adding volume\") ### DEFACTORING CODE\n",
    "            obscure = 'known'\n",
    "            reviewed = 'addedbecausecanon'\n",
    "        else:\n",
    "            print(\"Missing class\" + theclass)\n",
    "\n",
    "        if notes == 'well-known':\n",
    "            obscure = 'known'\n",
    "        if notes == 'obscure':\n",
    "            obscure = 'obscure'\n",
    "\n",
    "        if canon == 'y':\n",
    "            if theclass == 'addcanon':\n",
    "                actually = 'Norton, added'\n",
    "            else:\n",
    "                actually = 'Norton, in-set'\n",
    "        elif reviewed == 'rev':\n",
    "            actually = 'reviewed'\n",
    "        else:\n",
    "            actually = 'random'\n",
    "\n",
    "        metadict[volid] = dict()\n",
    "        metadict[volid]['reviewed'] = reviewed\n",
    "        metadict[volid]['obscure'] = obscure\n",
    "        metadict[volid]['pubdate'] = pubdate\n",
    "        metadict[volid]['birthdate'] = birthdate\n",
    "        metadict[volid]['gender'] = gender\n",
    "        metadict[volid]['nation'] = nation\n",
    "        metadict[volid]['author'] = author\n",
    "        metadict[volid]['title'] = title\n",
    "        metadict[volid]['canonicity'] = actually\n",
    "        metadict[volid]['pubname'] = row['pubname']\n",
    "        metadict[volid]['firstpub'] = forceint(row['firstpub'])\n",
    "\n",
    "# These come in as dirty pairtree; we need to make them clean.\n",
    "\n",
    "cleanmetadict = dict()\n",
    "allidsinmeta = set([x for x in metadict.keys()])\n",
    "allidsindir = set([dirty_pairtree(x) for x in volumeIDs])\n",
    "missinginmeta = len(allidsindir - allidsinmeta)\n",
    "missingindir = len(allidsinmeta - allidsindir)\n",
    "print(\"We have \" \n",
    "      + str(missinginmeta) \n",
    "      + \" volumes in missing in metadata, and\")\n",
    "print(str(missingindir) + \" volumes missing in the directory.\")\n",
    "print(allidsinmeta - allidsindir)\n",
    "\n",
    "for anid in volumeIDs:\n",
    "    dirtyid = dirty_pairtree(anid)\n",
    "    if dirtyid in metadict:\n",
    "        cleanmetadict[anid] = metadict[dirtyid]\n",
    "\n",
    "# Now that we have a list of volumes with metadata, we can select the groups of IDs\n",
    "# that we actually intend to contrast. If we want to us more or less everything,\n",
    "# this may not be necessary. But in some cases we want to use randomly sampled subsets.\n",
    "\n",
    "# The default condition here is\n",
    "\n",
    "# category2sorton = 'reviewed'\n",
    "# positive_class = 'rev'\n",
    "# sizecap = 350\n",
    "# A sizecap less than one means, no sizecap.\n",
    "\n",
    "### DEFACTORING FUNCTION CALL\n",
    "### IDsToUse, classdictionary = metafilter.label_classes(metadict, category2sorton, positive_class, sizecap)\n",
    "\n",
    "### DEFACTORING NAMESPACE \n",
    "metadict = cleanmetadict  # put the data into the global namespace so execution can continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above code cell is large due to a long `for` loop processing each row of the metadata file. At a high level, the code in this cell loads the metadata and determines which volumes to exclude in the analysis. It does this by loading the poemeta.csv file and excluding rows based upon the parameters specified in the `excludeif`, `excludeifnot`, `excludeabove`, and `excludebelow` variables. This process removed 8 volume designations from the (meta)data. The resulting output immediately above is a mixture of the author’s code and our own DEFACTORING inspection statements (marked with the comment ### DEFACTORING). We have added a print statement so we can see the IDs of the volumes being excluded in the code.\n",
    "\n",
    "Beyond filtering out excluded (meta)data, this code also makes a series of normalizing decisions, that is, there are more conceptual distinctions being made (or unmade) in this code. First is the normalization of nationality, which is a clinical way of saying Underwood and Sellers lump Canada with the United States and Ireland with the UK. Nationality is not a factor in the Pace of Change analysis, but it is interesting to see this code here, it implies this code was used in other explorations of the data. Additionally, this code cell splits the `recept` column of the metadata file into two columns, `obscure` and `reviewed`. From what we can tell from the code and the comments, there are poems that were reviewed, and there are poems that were too obscure. Lastly, there are poems that are not in the set of reviewed poems but are nevertheless part of the canon. In the latter case the poems are set to be “known” `(obscure = 'known')`. According to the author's comment this trumps the conflict when the author is known but not explicitly in the reviewed set.\n",
    "\n",
    "We know that poems with the addcanon in the `recept` column are being excluded because they are included in the `excludeif` dictionary. But why? The code in the code cell above provides somewhat of an explanation:\n",
    "\n",
    "```\n",
    "excludeif['recept'] = 'addcanon'\n",
    "# We don't ordinarily include canonical volumes that were not in either sample.\n",
    "# These are included only if we're testing the canon specifically.\n",
    "```\n",
    "What might be important to note here is how \"great debates\" in literary history about literary prestige, obscurity, and the canon are being ascribed in the code without much fanfare. There is a hard decision being made (in the blink of an eye) about the status of particular literary works. Most of these are, we suspect, fairly uncontroversial distinctions that accord with the broader community, but the code and computation enforce clear and unambiguous decisions for each an every volume. These hard decisions are pragmatically necessary to get to more interesting analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OCLC                                         NaN\n",
       "actualdate                                  1860\n",
       "author                     Drake, Joseph Rodman,\n",
       "birth                                       1795\n",
       "canon                                        NaN\n",
       "docid                             wu.89099921512\n",
       "enumcron                                     NaN\n",
       "firstpub                                    1860\n",
       "gender                                         m\n",
       "impaud                                       NaN\n",
       "imprint         New York;The Bradford Club;1860.\n",
       "inferreddate                                1860\n",
       "judge                                        NaN\n",
       "nationality                                   us\n",
       "notes                                        NaN\n",
       "othername                                    NaN\n",
       "pubname                                      NaN\n",
       "pubrev                                       NaN\n",
       "recept                                    random\n",
       "recordid                              8.9986e+06\n",
       "title                               The croakers\n",
       "yrrev                                        NaN\n",
       "Name: 534, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "# Examine the original metadata file \n",
    "\n",
    "defactoring_volume_id = 'wu.89099921512'\n",
    "\n",
    "the_croakers_metadata = (\n",
    "    metadata_file\n",
    "    .loc[metadata_file['docid'] == defactoring_volume_id]\n",
    "    .squeeze()\n",
    "    .sort_index()\n",
    ")\n",
    "the_croakers_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Drake, Joseph Rodman,',\n",
       " 'birthdate': 1795,\n",
       " 'canonicity': 'random',\n",
       " 'firstpub': 1860,\n",
       " 'gender': 'm',\n",
       " 'nation': 'us',\n",
       " 'obscure': 'obscure',\n",
       " 'pubdate': 1860,\n",
       " 'pubname': '',\n",
       " 'reviewed': 'not',\n",
       " 'title': 'The croakers'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "# Examine the cleaned metadata\n",
    "cleanmetadict[defactoring_volume_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspection above shows the data expressed in the CSV file has been transformed into into a Python dictionary, `cleanmetadict`, with additional columns for expressing more granularity about the categorizations for each poetry volume. We also observe the raw metadata csv file has additional columns that are not reflected in the Python dictionary. What we see reflected in `cleanmetadict` is only the metadata necessary for the analysis with any dirty or unnecessary information removed. Furthermore, the metadata now lives in a native Python data structure, a dictionary, making it easier to access specific volumes and manipulate in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION\n",
    "### def label_classes(metadict, category2sorton, positive_class, sizecap):\n",
    "''' This takes as input the metadata dictionary generated\n",
    "by get_metadata. It subsets that dictionary into a\n",
    "positive class and a negative class. Instances that belong\n",
    "to neither class get ignored.\n",
    "'''\n",
    "\n",
    "all_instances = set([x for x in metadict.keys()])\n",
    "\n",
    "# The first stage is to find positive instances.\n",
    "\n",
    "all_positives = set()\n",
    "\n",
    "for key, value in metadict.items():\n",
    "    if value[category2sorton] == positive_class:\n",
    "        all_positives.add(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code reads the metadata properties and puts a subset of all entries into a variable, `all_positives`, which will contain all of the volume ids for reviewed poems. If the `reviewed` column has a value of ‘rev’, then it is selected for inclusion. The name and value of the property are parameterized however, so technically it is more correct, but more opaque as well, to state: if a poem’s metadata has the value 'rev' (specified by the `positive_class` variable) for the reviewed property (specified by the `category2sorton` variable) then it is labeled as a positive. Having thus collected all the reviewed poems into the set named `all_positives`, the next cell populates the variable `all_negatives` with all the instances not in the positive set by subtracting the set of positives from the set of all instances by applying a basic mathematical set operation (-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_negatives = all_instances - all_positives\n",
    "iterator = list(all_negatives)\n",
    "for item in iterator:\n",
    "    if metadict[item]['reviewed'] == 'addedbecausecanon':\n",
    "        all_negatives.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative labels are assigned to all instances that are not in the set of positive instances. This section includes additional code that filters out any item with `addedbecausecannon` set for the `reviewed` property, but this code should never execute because, as we have seen above, the canon should already be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    }
   ],
   "source": [
    "if sizecap > 0 and len(all_positives) > sizecap:\n",
    "    positives = random.sample(all_positives, sizecap)\n",
    "else:\n",
    "    positives = list(all_positives)\n",
    "    print(len(all_positives))\n",
    "\n",
    "# If there's a sizecap we also want to ensure classes have\n",
    "# matching sizes and roughly equal distributions over time.\n",
    "\n",
    "numpositives = len(all_positives)\n",
    "\n",
    "if sizecap > 0 and len(all_negatives) > numpositives:\n",
    "    if not 'date' in category2sorton:\n",
    "        available_negatives = list(all_negatives)\n",
    "        negatives = list()\n",
    "\n",
    "        for anid in positives:\n",
    "            date = metadict[anid]['pubdate']\n",
    "\n",
    "            available_negatives = sort_by_proximity(available_negatives, \n",
    "                                                    metadict, date)\n",
    "            selected_id = available_negatives.pop(0)\n",
    "            negatives.append(selected_id)\n",
    "\n",
    "    else:\n",
    "        # if we're dividing classes by date, we obvs don't want to\n",
    "        # ensure equal distributions over time.\n",
    "\n",
    "        negatives = random.sample(all_negatives, sizecap)\n",
    "\n",
    "else:\n",
    "    negatives = list(all_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code in the cell above does not actually execute because the number of entries in the `all_positives` and `all_negatives` lists are not greater than `sizecap`. The conditional statements on line 1 and line 12 will not be true so the the accompanying blocks of code never execute. If the `sizecap` variable was smaller, or the number of entries larger, this code would use random sampling to select smaller number of entries from the positives entries.\n",
    "\n",
    "Looking at the block of code for the negative entries is a bit more interesting. This block of code (from lines 13 to 29) makes an unexecuted reference to a function `sort_by_proximity` that samples from the negative elements with an equal distribution based upon some function of proximity. Because this code is not executing we are not going to spend more time and analytical attention to exactly how this function operates. Furthermore, we have not included the code for `sort_by_proximity()` in the notebook because it is not part of the execution path we are tracing. In the code’s garden of forking paths, this is a path not taken.\n",
    "\n",
    "These issues of code that is not executed and functions that are not called point to properties of code that make it complex and therefore difficult to review or critique. Code has a textual and a processual dimension (Hiller 2015, Van Zundert 2016). The code-as-text is what we can see and read in the source code, the processual dimension of code is tied to its execution as a software program. Code critique moves in between these two modes of existence of code. We are here, as code critics, not simply looking at code-as-text. That is, we are in this case reviewing a *live* execution of the code. This is extremely significant and, we’d argue, distinguishes defactoring as more than analysing code-as-text; we are analyzing the code, the data, and their interaction in the computation. Leveraging the affordances of the Jupyter notebook platform allow us the ability to interact with the execution environment described in the code. At each step of the incremental process building this environment we can ask it questions by inspecting the state of variables (or even change them). This is more than simply treating the code as a text, the code is but one part of a complex assemblage we have been manipulating with the authors’ code (and some of our own). However, it is also not a complete inspection of all the ways in which the code can be possibly executed. As we *defactor* the authors’s code, we make choices about how much of the code to include for the argument we are trying to make (and for the sake of our time and attention). Thus we are dealing with a code-criticism conundrum: What is the required or adequate breadth and depth of the representation of the code, and subsequently of the critique? The decision to include or not include `sort_by_proximity()` is a breadth issue. How broad should we be in including code that does not execute? Note that we are including code from a conditional block that does not execute, but are not going out the additional step to include non-executed functions defined elsewhere in the code. The decision to include or not include code from the standard library, code not written by the authors, is a depth issue. While there are many functions we are stepping over, like `len`, `list`, `append`, `pop`, `random.sample`, we argue there is no need to step into these functions because, following Hinsen's model, they are not part of the bespoke code of Pace of Change. Again, this raises the problematic issue of our decision to step over `sort_by_proximity()` even though it was written by the authors' for this particular project.\n",
    "\n",
    "Obviously the ‘rules of the game’ for defactoring are not quite clear yet. Therefore we are more or less ‘feeling’ our way through an emerging methodology for code criticism. As we see vestiges of the authors’ evolution in thinking in their code, this notebook is capturing the evolution of our thinking about defactoring as a practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We have 360 positive, and\n",
      "360 negative instances.\n"
     ]
    }
   ],
   "source": [
    "# Now we have two lists of ids.\n",
    "\n",
    "IDsToUse = set()\n",
    "classdictionary = dict()\n",
    "\n",
    "print()\n",
    "print(\"We have \" + str(len(positives)) + \" positive, and\")\n",
    "print(str(len(negatives)) + \" negative instances.\")\n",
    "\n",
    "for anid in positives:\n",
    "    IDsToUse.add(anid)\n",
    "    classdictionary[anid] = 1\n",
    "\n",
    "for anid in negatives:\n",
    "    IDsToUse.add(anid)\n",
    "    classdictionary[anid] = 0\n",
    "\n",
    "for key, value in metadict.items():\n",
    "    if value['reviewed'] == 'addedbecausecanon':\n",
    "        print(\"DEFACTORING: Adding cannon supplement\") ### DEFACTORING CODE\n",
    "        IDsToUse.add(key)\n",
    "        classdictionary[key] = 0\n",
    "# We add the canon supplement, but don't train on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are seeing yet another instance of metadata being shaped and transformed in preparation for analysis. The code first prints out the number of positive and negative instances by checking the length (using `len()`) of the volume ids stored in the `positives` and `negatives` variables. Two loops iterate over these lists and populate two more variables, `IDsToUse` and `classdictionary`. The first, `IDsToUse` contains a master list of all the volume identifiers to be used in the analysis. It is of the Python set datatype, meaning there will be no duplicate identifiers in the set list. The second, `classdictionary` is a dictionary that allows a simple lookup to see if a volume ID is in the positive or negative class–as indicated by a 0 or a 1. There is a final loop whose logic checks to see if any volumes have a specific metadata flag reviewed with a value of `addedbecausecanon`. We have added a defactoring statement to see if this logic is ever triggered. The output indicates the `if` statement’s conditions were never satisfied, no volumes were added because of cannon.\n",
    "\n",
    "We have come to the end of the preparing metadata section. All of the code up to this point has focused on loading, normalizing, and transforming the metadata–namely the identifiers of the volumes to be analyzed. Based upon the values in the metadata fields and assumptions built into the logic of the code, the authors have assembled the list of volume ids and their associated class. Because this is a *supervised* machine learning exercise, the authors need labeled data to train the model. All of the work in this section of the code was dedicated to assigning a class label (positive or negative) to the identifiers of the data files. The next section dives into the actual data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Words into Features\n",
    "\n",
    "Now that we know exactly which volumes of poetry the code will be analyzing, we can venture into the datafiles and begin the work of transforming the volume data files into a data structure suitable for analysis. The logistic regression requires the data to be in a specific shape, a matrix of binary features. This section does the work of *getting the data into shape*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION DEFINITIONS\n",
    "### We need to define the infer_date function\n",
    "\n",
    "def infer_date(metadictentry, datetype):\n",
    "    if datetype == 'pubdate':\n",
    "        return metadictentry[datetype]\n",
    "    elif datetype == 'firstpub':\n",
    "        firstpub = metadictentry['firstpub']\n",
    "        if firstpub > 1700 and firstpub < 1950:\n",
    "            return firstpub\n",
    "        else:\n",
    "            return metadictentry['pubdate']\n",
    "    else:\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell defines a helper function, `infer_date()`, which is used in the code below to deal with differences in the `pubdate` and `firstpub` columns in the metadata. When `firstpub` falls between 1700 and 1950 the codes uses that as the date, otherwise it returns the value in `pubdate` (or it exists the script in the case of bad data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a vocabulary list and a volsize dict\n",
    "wordcounts = Counter()\n",
    "\n",
    "volspresent = list()\n",
    "orderedIDs = list()\n",
    "\n",
    "positivecounts = dict()\n",
    "negativecounts = dict()\n",
    "\n",
    "for volid, volpath in zip(volumeIDs, volumepaths):\n",
    "    if volid not in IDsToUse:\n",
    "        continue\n",
    "    else:\n",
    "        volspresent.append((volid, volpath))\n",
    "        orderedIDs.append(volid)\n",
    "\n",
    "    date = infer_date(metadict[volid], datetype)\n",
    "    if date < pastthreshold or date > futurethreshold:\n",
    "        continue\n",
    "    else:\n",
    "        with open(volpath, encoding = 'utf-8') as f:\n",
    "            for line in f:\n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) > 2 or len(fields) < 2:\n",
    "                    # print(line)\n",
    "                    continue\n",
    "                word = fields[0]\n",
    "                if len(word) > 0 and word[0].isalpha():\n",
    "                    count = int(fields[1])\n",
    "                    wordcounts[word] += 1\n",
    "                    # for initial feature selection we use the number of\n",
    "                    # *documents* that contain a given word,\n",
    "                    # so it's just +=1.\n",
    "\n",
    "vocablist = [x[0] for x in wordcounts.most_common(numfeatures)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important section because it contains the code that opens the data files and selects the word-features used in the logistic regression. The main block of code in the cell above loops over each data file (representing a poem) and counts the number of instances of each word. Like the metadata file, we can use defactoring to inspect a datafile to get a sense of what these data look like before they are transformed by the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>3093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>1263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>!</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>in</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>to</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Word  Count\n",
       "0    ,   3093\n",
       "1  the   1263\n",
       "2  and    988\n",
       "3   of    876\n",
       "4    .    745\n",
       "5    !    640\n",
       "6   in    521\n",
       "7    a    511\n",
       "8   to    480\n",
       "9    i    423"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "data_file = pd.read_csv(volumepaths[0], \n",
    "                        delimiter='\\t', \n",
    "                        names=[\"Word\", \"Count\"])\n",
    "data_file.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows the first 10 lines of one of the poem data files. As we can plainly see, the volume has already been pre-processed into a list of words and their frequencies. This particular volume has 2,745 commas and 1445 instances of the word “the.” The authors’ code parses each of these files and assembles a vocabulary list of the 3200 most common words (as specified by the `numfeatures` variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word      Count\n",
      "---------------\n",
      "is          720\n",
      "but         720\n",
      "to          720\n",
      "in          720\n",
      "for         720\n",
      "and         720\n",
      "a           720\n",
      "all         720\n",
      "not         720\n",
      "of          720\n",
      "that        720\n",
      "at          720\n",
      "as          720\n",
      "by          720\n",
      "with        720\n",
      "on          720\n",
      "i           720\n",
      "the         720\n",
      "they        719\n",
      "when        719\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"Word      Count\")\n",
    "print(\"---------------\")\n",
    "for word, count in wordcounts.most_common(n=20):\n",
    "    print(\"{:8}  {:5}\".format(word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it might seem strange that the count is 720 for all of the top words in the corpus. However, when we dig deeper into the code we can see that the authors are not tabulating the total word frequencies across all volumes in the corpus, rather they are associating words and the number of volumes. The code loops over each file, opening it, and parses each line by splitting on the tab character (“”). What is interesting is that Underwood and Sellers are only paying attention to the word and ignoring the frequency within the volume. They check to see if the word is longer than zero and use the `isalpha()` function to make sure the characters are alphabetic as opposed to punctuation. The comments in the code explain that the authors are just using the “number of documents that contain a given word”. \n",
    "\n",
    "The authors are selecting their list of features (stored in the `vocablist` variable) by selecting words ranked by the number of documents in which they appear. The total number of documents we are working with is 720, so the table we generated above tells us that the top ten words appear in all of the documents. If we look at more than just the top ten, we can start to see the distribution of words in documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFDCAYAAAA9EqfKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X100+X9//FX0oBYStv0hmIrioUiN1ZQYXJfxGxu6jz9\nMoXJ0AEOp6iduHmouC9sh6ndtBTRYp1DPIMzpzuj3RzTHWNHOVpvKogiKoqCwuGmNwmFQktpku8f\n/MjPSpC0SZvk6vPxl7nySfLu27QvrutzZ/H5fD4BAABjWSNdAAAA6FqEPQAAhiPsAQAwHGEPAIDh\nCHsAAAxH2AMAYDjCHgAAwxH2AAAYjrAHAMBwhD0AAIazRbqAcNq3b99pY2lpaaqvr49ANeagh6Gj\nh6Gjh6Ghf6GLxh5mZmYGtR0zewAADEfYAwBgOMIeAADDEfYAABiOsAcAwHCEPQAAhiPsAQAwHGEP\nAIDhCHsAAAxH2AMAYLhuuVzuqlWrtGXLFiUlJam4uFiStHbtWm3evFk2m00ZGRlasGCB+vbtK0kq\nLy9XZWWlrFar5s6dq9GjR3dHmQAAGKlbwn7q1Kn6/ve/r9LSUv/YpZdeqlmzZikuLk7r1q1TeXm5\nZs+erb1796q6ulrLly+X2+3WsmXL9Pjjj8tqjY5FCM/8GwKOxz3zz26uBACA4HRLgo4YMUIJCQnt\nxkaNGqW4uDhJ0tChQ+VyuSRJNTU1mjBhgnr16qX+/ftrwIAB2rlzZ3eUCQCAkaLirneVlZWaMGGC\nJMnlciknJ8f/XEpKiv8fAt/kdDrldDolSUVFRUpLSzttG5vNFnC8sw6eYTycnxFtwt3Dnogeho4e\nhob+hS6WexjxsF+/fr3i4uI0efLkDr/W4XDI4XD4Hwe69WB33ZIw2m57GE7ReFvHWEMPQ0cPQ0P/\nQheNPYyJW9xu3LhRmzdvVkFBgSwWi6STM/mGhgb/Ni6XSykpKZEqEQCAmBexsN+6dav+8Y9/aNGi\nRTrnnHP842PGjFF1dbVOnDih2tpa7d+/X0OGDIlUmQAAxLxuWcZfsWKFPvroIx05ckR33HGHZsyY\nofLycrW1tWnZsmWSpJycHN1+++0aOHCgxo8fr/vuu09Wq1W33XZb1ByJDwBALOqWsL/33ntPG5s2\nbdoZt58+fbqmT5/elSUBANBjMGUGAMBwhD0AAIYj7AEAMFzEz7M3BZfRBQBEK2b2AAAYjrAHAMBw\nhD0AAIYj7AEAMBxhDwCA4Qh7AAAMR9gDAGA4wh4AAMMR9gAAGI6wBwDAcIQ9AACGI+wBADAcYQ8A\ngOEIewAADEfYAwBgOMIeAADDEfYAABiOsAcAwHCEPQAAhiPsAQAwnC3SBUQzz/wbIl0CAAAhY2YP\nAIDhCHsAAAxH2AMAYDjCHgAAwxH2AAAYjrAHAMBwhD0AAIYj7AEAMBwX1YmAM12sJ+6Zf3ZzJQCA\nnqBbwn7VqlXasmWLkpKSVFxcLElqampSSUmJ6urqlJ6eroULFyohIUGSVF5ersrKSlmtVs2dO1ej\nR4/ujjIBADBStyzjT506VYsXL243VlFRodzcXK1cuVK5ubmqqKiQJO3du1fV1dVavny5HnzwQa1e\nvVper7c7ygQAwEjdEvYjRozwz9pPqampUV5eniQpLy9PNTU1/vEJEyaoV69e6t+/vwYMGKCdO3d2\nR5kAABgpYvvsGxsbZbfbJUnJyclqbGyUJLlcLuXk5Pi3S0lJkcvlCvgeTqdTTqdTklRUVKS0tLTT\ntrHZbAHHg3GwU69qL9Bnn+l9O1tnVwulhziJHoaOHoaG/oUulnsYFQfoWSwWWSyWDr/O4XDI4XD4\nH9fX15+2TVpaWsDx7tKRz45knd8m0j00AT0MHT0MDf0LXTT2MDMzM6jtInbqXVJSktxutyTJ7XYr\nMTFR0smZfENDg387l8ullJSUiNQIAIAJIhb2Y8aMUVVVlSSpqqpKY8eO9Y9XV1frxIkTqq2t1f79\n+zVkyJBIlQkAQMzrlmX8FStW6KOPPtKRI0d0xx13aMaMGcrPz1dJSYkqKyv9p95J0sCBAzV+/Hjd\nd999slqtuu2222S1cu0fAAA6q1vC/t577w04vmTJkoDj06dP1/Tp07uyJAAAegymzAAAGI6wBwDA\ncIQ9AACGI+wBADAcYQ8AgOEIewAADEfYAwBgOMIeAADDEfYAABguKu56ZzLP/BsiXQIAoIdjZg8A\ngOEIewAADEfYAwBgOMIeAADDEfYAABiOsAcAwHCEPQAAhiPsAQAwHGEPAIDhCHsAAAxH2AMAYDjC\nHgAAwxH2AAAYjrAHAMBwhD0AAIYj7AEAMJwt0gWgczzzbwg4HvfMP7u5EgBAtGNmDwCA4ZjZR5FA\ns3Vm6gCAUDGzBwDAcIQ9AACGI+wBADAc++wNEy37/aOlDgBAFIT9v/71L1VWVspisWjgwIFasGCB\nWltbVVJSorq6OqWnp2vhwoVKSEiIdKkAAMSkiC7ju1wuvfzyyyoqKlJxcbG8Xq+qq6tVUVGh3Nxc\nrVy5Urm5uaqoqIhkmQAAxLSI77P3er1qbW2Vx+NRa2ur7Ha7ampqlJeXJ0nKy8tTTU1NhKsEACB2\nRXQZPyUlRT/84Q915513qnfv3ho1apRGjRqlxsZG2e12SVJycrIaGxsjWSYAADEtomHf1NSkmpoa\nlZaWKj4+XsuXL9emTZvabWOxWGSxWAK+3ul0yul0SpKKioqUlpZ22jY2my3geDAOdupV4XWm2jtS\nW2d//lM608NA9YVaRywL5XuIk+hhaOhf6GK5hxEN+23btql///5KTEyUJF155ZX69NNPlZSUJLfb\nLbvdLrfb7X/+mxwOhxwOh/9xfX39adukpaUFHI8V4ag91PcIVw9j+f9DqGL9exgN6GFo6F/oorGH\nmZmZQW0X0X32aWlp+uyzz3T8+HH5fD5t27ZNWVlZGjNmjKqqqiRJVVVVGjt2bCTLBAAgpkV0Zp+T\nk6Nx48Zp0aJFiouL06BBg+RwONTS0qKSkhJVVlb6T70DAACdE/Hz7GfMmKEZM2a0G+vVq5eWLFkS\noYoAADBLxE+9AwAAXYuwBwDAcIQ9AACGi/g+e3y7QDeUAQCgI5jZAwBgOMIeAADDEfYAABiOsAcA\nwHCEPQAAhiPsAQAwHGEPAIDhCHsAAAxH2AMAYDjCHgAAwwUd9m+++WbA8bfeeitsxQAAgPALOuzL\nysoCjj/99NNhKwYAAITfWW+Ec/DgQUmS1+tVbW2tfD5fu+d69+7dddUBAICQnTXsCwoK/P99zz33\ntHsuOTlZN910U/irAgAAYXPWsH/hhRckSUuXLtVvf/vbLi8IAACEV9D77Al6AABi01ln9qfU1tbq\n+eef1+7du9XS0tLuuaeeeirshQEAgPAIOuwff/xxZWRk6NZbb9U555zTlTUBAIAwCjrs9+7dq2XL\nlslq5To86BzP/BuC3jbumX92YSUA0LMEndzDhw/X7t27u7AUAADQFYKe2aenp+uhhx7Sd77zHSUn\nJ7d7bubMmWEvDAAAhEfQYX/8+HFdccUV8ng8amho6MqaEGHfXG4/qDMvq3dkaR4AEBlBh/2CBQu6\nsg4AANBFgg77U5fNDSQjIyMsxQAAgPALOuy/ftncbzp1lT0AABB9gg77bwb6oUOH9Le//U3Dhw8P\ne1GIPuybB4DY1emT5pOTkzVnzhz95S9/CWc9AAAgzIKe2Qeyb98+HT9+PFy1AH5nWkngYjsA0HFB\nh/2SJUtksVj8j48fP649e/boxhtv7JLCAABAeAQd9tOmTWv3uE+fPrrwwgt13nnnhb0ohBezZADo\n2YIO+6lTp3ZJAUePHlVZWZn27Nkji8WiO++8U5mZmSopKVFdXZ3S09O1cOFCJSQkdMnnAwBguqDD\nvq2tTevXr9emTZvkdrtlt9s1ZcoUTZ8+XTZb53f9r1mzRqNHj9Yvf/lLtbW16fjx4yovL1dubq7y\n8/NVUVGhiooKzZ49u9OfAQBATxb00fjr1q3Ttm3bNH/+fD366KOaP3++PvzwQ61bt67TH37s2DF9\n/PHH/l0ENptNffv2VU1NjfLy8iRJeXl5qqmp6fRnAADQ0wU9JX/rrbf06KOPql+/fpKkzMxMXXTR\nRbr//vs1Z86cTn14bW2tEhMTtWrVKn355ZfKzs7WnDlz1NjYKLvdLunkKX6NjY0BX+90OuV0OiVJ\nRUVFSktLO20bm80WcDwYZ75moBnO1Jdo/rk78v/y4P9MCDieUV4drnKCFsr3ECfRw9DQv9DFcg+D\nDnufzxf2D/d4PNq1a5fmzZunnJwcrVmzRhUVFe22sVgs7c4C+DqHwyGHw+F/XF9ff9o2aWlpAccR\nuF/RLhw1R+Ln5nsYOnoYGvoXumjsYWZmZlDbBb2MP378eP3+97/X1q1btXfvXm3dulWPPvqoxo0b\n1+kiU1NTlZqaqpycHEnSuHHjtGvXLiUlJcntdkuS3G63EhMTO/0ZAAD0dEHP7GfPnq2///3vWr16\ntdxut1JSUjRx4kT96Ec/6vSHJycnKzU1Vfv27VNmZqa2bdum888/X+eff76qqqqUn5+vqqoqjR07\nttOfAbNwGiEAdNxZw/6TTz7R5s2b9ZOf/EQzZ87UzJkz/c+tW7dOX3zxhYYOHdrpAubNm6eVK1eq\nra1N/fv314IFC+Tz+VRSUqLKykr/qXcAAKBzzhr25eXluuaaawI+d8kll2j9+vUqLCzsdAGDBg1S\nUVHRaeNLlizp9HsCAID/76z77Hfv3q3Ro0cHfC43N1e7du0Ke1EAACB8zhr2zc3NamtrC/icx+NR\nc3Nz2IsCAADhc9awz8rK0vvvvx/wuffff19ZWVlhLwoAAITPWffZX3fddfrjH/8or9ersWPHymq1\nyuv1qqamRqtXr9att97aHXUCABAzAp05FMmzhs4a9pMmTdKhQ4dUWlqqEydOKDExUYcPH1avXr00\nY8YMTZo0qTvqBAAAnRTUefbXX3+9pk2bpk8//VRNTU1KSEjQ0KFDFR8f39X1AQCAEAV9UZ34+Pgz\nHpUPAACiV9CXywUAALGJsAcAwHCEPQAAhiPsAQAwHGEPAIDhgj4aH+Y50+1iAQBmYWYPAIDhCHsA\nAAxH2AMAYDj22QP/T7TduAIAwoWZPQAAhmNmD3yLM52xwIwfQCxhZg8AgOGY2cMI3X3NgO6e8bPC\nACAUzOwBADAcYQ8AgOEIewAADMc+e/Q4PfWeAB3Z788xAoBZmNkDAGA4wh4AAMOxjA9ECJfnBdBd\nmNkDAGA4wh4AAMMR9gAAGI599kAP11NPRQR6Emb2AAAYjpk9EEWYZQPoClER9l6vV4WFhUpJSVFh\nYaGamppUUlKiuro6paena+HChUpISIh0mQAAxKSoCPt///vfysrKUnNzsySpoqJCubm5ys/PV0VF\nhSoqKjR79uwIVwlEH87VBxCMiO+zb2ho0JYtW3T11Vf7x2pqapSXlydJysvLU01NTaTKAwAg5kV8\nZv/cc89p9uzZ/lm9JDU2Nsput0uSkpOT1djYGPC1TqdTTqdTklRUVKS0tLTTtrHZbAHHg3GwU69C\nT3am71qg72FXfb/OVEM4Pq+zv0vhEMrvMuhfOHSkh4F+3yL6+xOxT5a0efNmJSUlKTs7W9u3bw+4\njcVikcViCficw+GQw+HwP66vrz9tm7S0tIDjQFc403etO7+HXfk5kfxd4nc5NPQvdKH2sCv6n5mZ\nGdR2EQ37HTt26N1339V7772n1tZWNTc3a+XKlUpKSpLb7Zbdbpfb7VZiYmIkywQAIKBYOYMmomE/\na9YszZo1S5K0fft2vfTSSyooKNDatWtVVVWl/Px8VVVVaezYsZEsEwCAmBbxA/QCyc/P1wcffKCC\nggJt27ZN+fn5kS4JAICYFfED9E4ZOXKkRo4cKUnq16+flixZEuGKAAAwQ1TO7AEAQPgQ9gAAGI6w\nBwDAcFGzzx5AeMTKqUAAug8zewAADMfMHggjbkwDIBoxswcAwHCEPQAAhiPsAQAwHPvsgS7mmX8D\nt0sGEFHM7AEAMBwzewAh4ywERIszXWeip38fmdkDAGA4ZvYAgA7pqSs5B/9nQqRL6DRm9gAAGI6w\nBwDAcCzjA+gS3X2gVEdvANQTlp1N0VN3G4QTM3sAAAzHzB5Aj9SR2WJHVg2YcSIaMbMHAMBwzOwB\nAPiajh7/EQuY2QMAYDhm9gBgoG/OTk/djIljCnomZvYAABiOmT0AdAPTzxXnBjTRjZk9AACGY2YP\nIGhddZRytMwKo/0o7J66OhDqtmBmDwCA8ZjZA+hW4ZiRmT6rY4aLcGNmDwCA4Qh7AAAMxzI+APRw\n7AowHzN7AAAMR9gDAGC4iC7j19fXq7S0VIcOHZLFYpHD4dC1116rpqYmlZSUqK6uTunp6Vq4cKES\nEhIiWSoAADEromEfFxenW265RdnZ2WpublZhYaEuvfRSbdy4Ubm5ucrPz1dFRYUqKio0e/bsSJYK\nAEZg/3zPFNFlfLvdruzsbEnSueeeq6ysLLlcLtXU1CgvL0+SlJeXp5qamkiWCQBATIuao/Fra2u1\na9cuDRkyRI2NjbLb7ZKk5ORkNTY2BnyN0+mU0+mUJBUVFSktLe20bWw2W8DxYBw8+yYAutDXZ6Gx\n8vt4pr83gervyLaxqiN/f7vy546GOjqbReEQFWHf0tKi4uJizZkzR/Hx8e2es1gsslgsAV/ncDjk\ncDj8j+vr60/bJi0tLeA4AHSFjvy96Ql/m6LlZ4yGOrqihszMzKC2i3jYt7W1qbi4WJMnT9aVV14p\nSUpKSpLb7Zbdbpfb7VZiYmKEqwSA4HCp2+hk+k2Eziai++x9Pp/KysqUlZWl66+/3j8+ZswYVVVV\nSZKqqqo0duzYSJUIAEDMi+jMfseOHdq0aZMuuOAC3X///ZKkm2++Wfn5+SopKVFlZaX/1DsAAMKp\nJ62sRDTshw0bphdffDHgc0uWLOnmagAAMBNX0AMAwHCEPQAAhiPsAQAwHGEPAIDhCHsAAAwX8Yvq\nAAB6lp50ylu0YGYPAIDhCHsAAAxH2AMAYDj22QMAugz756MDM3sAAAxH2AMAYDjCHgAAwxH2AAAY\njrAHAMBwhD0AAIYj7AEAMBxhDwCA4Qh7AAAMR9gDAGA4wh4AAMMR9gAAGI6wBwDAcIQ9AACGI+wB\nADAcYQ8AgOEIewAADEfYAwBgOMIeAADDEfYAABiOsAcAwHCEPQAAhiPsAQAwnC3SBXybrVu3as2a\nNfJ6vbr66quVn58f6ZIAAIg5UTuz93q9Wr16tRYvXqySkhK98cYb2rt3b6TLAgAg5kRt2O/cuVMD\nBgxQRkaGbDabJkyYoJqamkiXBQBAzInasHe5XEpNTfU/Tk1NlcvlimBFAADEpqjeZ382TqdTTqdT\nklRUVKTMzMyA251p/Kw2vNvZ0gAAiBpRO7NPSUlRQ0OD/3FDQ4NSUlLabeNwOFRUVKSioqIzvk9h\nYWGX1dhT0MPQ0cPQ0cPQ0L/QxXIPozbsBw8erP3796u2tlZtbW2qrq7WmDFjIl0WAAAxJ2qX8ePi\n4jRv3jw99NBD8nq9uuqqqzRw4MBIlwUAQMyJ2rCXpMsvv1yXX355SO/hcDjCVE3PRQ9DRw9DRw9D\nQ/9CF8s9tPh8Pl+kiwAAAF0navfZAwCA8IjqZfyzqa+vV2lpqQ4dOiSLxSKHw6Frr71WTU1NKikp\nUV1dndLT07Vw4UIlJCRIksrLy1VZWSmr1aq5c+dq9OjREf4pIqu1tVVLly5VW1ubPB6Pxo0bpxkz\nZtDDTvB6vSosLFRKSooKCwvpYQfddddd6tOnj6xWq+Li4lRUVEQPO+jo0aMqKyvTnj17ZLFYdOed\ndyozM5MeBmnfvn0qKSnxP66trdWMGTOUl5cX+z30xTCXy+X7/PPPfT6fz3fs2DFfQUGBb8+ePb61\na9f6ysvLfT6fz1deXu5bu3atz+fz+fbs2eP71a9+5WttbfUdPHjQd/fdd/s8Hk/E6o8GXq/X19zc\n7PP5fL4TJ074HnjgAd+OHTvoYSe89NJLvhUrVvgeeeQRn8/no4cdtGDBAl9jY2O7MXrYMU888YTP\n6XT6fL6Tv89NTU30sJM8Ho/vZz/7ma+2ttaIHsb0Mr7dbld2drYk6dxzz1VWVpZcLpdqamqUl5cn\nScrLy/NfZrempkYTJkxQr1691L9/fw0YMEA7d+6MWP3RwGKxqE+fPpIkj8cjj8cji8VCDzuooaFB\nW7Zs0dVXX+0fo4eho4fBO3bsmD7++GNNmzZNkmSz2dS3b1962Enbtm3TgAEDlJ6ebkQPY3oZ/+tq\na2u1a9cuDRkyRI2NjbLb7ZKk5ORkNTY2Sjp5Cd6cnBz/a1JSUrgEr04uPy9atEgHDhzQNddco5yc\nHHrYQc8995xmz56t5uZm/xg97Lhly5bJarXqu9/9rhwOBz3sgNraWiUmJmrVqlX68ssvlZ2drTlz\n5tDDTnrjjTc0ceJESWb8LhsR9i0tLSouLtacOXMUHx/f7jmLxSKLxRKhymKD1WrVo48+qqNHj+qx\nxx7TV1991e55evjtNm/erKSkJGVnZ2v79u0Bt6GHZ7ds2TKlpKSosbFRv/vd7067zDU9/HYej0e7\ndu3SvHnzlJOTozVr1qiioqLdNvQwOG1tbdq8ebNmzZp12nOx2sOYD/u2tjYVFxdr8uTJuvLKKyVJ\nSUlJcrvdstvtcrvdSkxMlHT6JXhdLtdpl+Dtyfr27auRI0dq69at9LADduzYoXfffVfvvfeeWltb\n1dzcrJUrV9LDDjrVg6SkJI0dO1Y7d+6khx2Qmpqq1NRU/0xz3LhxqqiooIed8N577+miiy5ScnKy\nJDMyJab32ft8PpWVlSkrK0vXX3+9f3zMmDGqqqqSJFVVVWns2LH+8erqap04cUK1tbXav3+/hgwZ\nEpHao8Xhw4d19OhRSSePzP/ggw+UlZVFDztg1qxZKisrU2lpqe69915dcsklKigooIcd0NLS4t8F\n0tLSog8++EAXXHABPeyA5ORkpaamat++fZJO7nM+//zz6WEnfH0JXzIjU2L6ojqffPKJlixZogsu\nuMC/rHLzzTcrJydHJSUlqq+vP+00ifXr1+u///2vrFar5syZo8suuyySP0LEffnllyotLZXX65XP\n59P48eN144036siRI/SwE7Zv366XXnpJhYWF9LADDh48qMcee0zSyeXoSZMmafr06fSwg3bv3q2y\nsjK1tbWpf//+WrBggXw+Hz3sgJaWFi1YsEBPPvmkf7ewCd/DmA57AABwdjG9jA8AAM6OsAcAwHCE\nPQAAhiPsAQAwHGEPAIDhCHvAIC+++KJWrlwZ9vf9zW9+o9deey3s7wugexD2QBcpLy/Xww8/3G6s\noKAg4Ngbb7zRnaX1aNu3b9cdd9wR6TKAbkXYA11k+PDh2rFjh7xeryTJ7Xb7r1/+9bEDBw5o+PDh\nHXpvn8/nfw8AOJuYvzY+EK2GDBkij8ej3bt3Kzs7Wx9//LFGjhypgwcPthvLyMjwX097x44deu65\n57Rv3z5lZmZqzpw5uvjiiyWdXEq/+OKL9dFHH+mLL75QcXGxrFarSktLtWvXLuXk5LS7eUxra6vK\nysq0detWeb1enXfeeVq0aJH/et/f5oMPPtCzzz4rt9utKVOm6OvX3vJ6vSovL9drr72m1tZWjR49\nWvPmzfNfbeyTTz7RunXrtHfvXp177rmaOXOmpk6dqt/85jeaPHmy/zbAGzdu1GuvvaZly5ZJkmbM\nmKHbbrtNGzZs0KFDh3Tttddq6tSpevLJJ7Vnzx6NGjVKBQUFstlO/tnavHmz/vrXv6qurk7nn3++\n5s+frwsvvFCSdNddd+maa67Rpk2bVFdXp9GjR+uuu+6S1+vVww8/rLa2Nt1yyy2SpMcff1wul0t/\n+tOftH//fvXu3VuTJk3ST3/605D+/wPRhLAHuojNZlNOTo4++ugjf7APGzZMdru93dipWX1TU5OK\nioo0d+5cTZw4UW+++aaKioq0cuVK9evXT5K0adMmLV68WJmZmfL5fFq6dKmGDh2qX//61/rss89U\nVFSkMWPGSDp5De9jx47pqaeeUq9evbR792717t37rHUfPnxYjz32mBYsWKAxY8bolVde0auvvqop\nU6ZIOhnSGzdu1NKlS5WUlKQnn3xSq1ev1j333KO6ujo9/PDDuv322zVu3Dg1Nze3u1HI2bz//vsq\nKipSQ0ODFi1apE8//VT33HOP+vXrpwcffFCvv/66pk6dql27dumpp57SokWLNHjwYG3atEl/+MMf\ntGLFCvXq1UuS9Oabb2rx4sXq3bu3/vd//1cbN27U9773PS1evFhPPPGEysrK/J9bXFysa6+9VlOm\nTFFLS8tpd34EYh3L+EAXGj58uD7++GNJJ2e8w4cPP21sxIgRkqQtW7ZowIABmjJliuLi4jRp0iRl\nZmZq8+bN/vebOnWqBg4cqLi4OB06dEiff/65Zs6cqV69emnEiBG64oor/NvGxcWpqalJBw4ckNVq\nVXZ29mm3gA7kvffe08CBAzVu3DjZbDZdd9117VYDXn/9dV1//fXKyMhQnz59NGvWLFVXV8vj8ej1\n119Xbm6uJk2aJJvNpn79+mnQoEFB9+uGG25QfHy8Bg4cqIEDB+rSSy9VRkaG4uPjddlll2n37t2S\nJKfTKYfDoZycHFmtVk2dOlU2m02fffaZ/71+8IMfKCUlRQkJCbriiiv8rw3EZrPpwIEDOnz4sPr0\n6aOhQ4cGXTMQC5jZA11oxIgR+s9//qOmpiYdPnxY5513npKSklRaWqqmpiZ99dVX/rB3uVxKT09v\n9/r09HS5XC7/49TUVP9/u1wu9e3bV3369Gm3fX19vSRpypQpamho0IoVK3Ts2DFNnjxZP/7xj/3L\n4GfidrvbfY7FYmn32O12t6szLS1NHo9HjY2NamhoUEZGRkda1M7X/1HRu3fv0x4fOnRIklRfX6+q\nqiq98sor/ufb2tra9eqbr/36c990xx136IUXXtDChQvVv39/3Xjjje3+4QTEOsIe6EJDhw7VsWPH\n5HQ6/fthO8/pAAAC8UlEQVTe4+PjZbfb5XQ6lZKSov79+0s6eW/st99+u93r6+vrNXr0aP/jU3d3\nlCS73a6jR4+qpaXFH/ingl46OVu96aabdNNNN6m2tlaPPPKIMjMzNW3atG+tOTk5ud3Su8/na/fY\nbrerrq6uXY1xcXFKSkpSamqqdu7cGfB9zznnHB0/ftz/+FRwd0ZqaqqmT5+u6dOnd/i1X+/hKeed\nd57uvfdeeb1evfPOO1q+fLlWr17d7h9SQCxjGR/oQr1799bgwYO1YcMGDRs2zD8+bNgwbdiwod1R\n+Jdddpn279+v119/XR6PR9XV1dq7d68uv/zygO+dnp6uwYMH68UXX1RbW5s++eSTdkv+H374ob76\n6it5vV7Fx8fLZrMFDLpvuvzyy7Vnzx69/fbb8ng8evnll9sF88SJE7VhwwbV1taqpaVFzz//vMaP\nH6+4uDhNnjxZ27Zt8y/rHzlyxL98PmjQIL3zzjs6fvy4Dhw4oMrKyo620+/qq6/Wq6++qs8++0w+\nn08tLS3asmWLmpubz/rapKQkHTlyRMeOHfOPbdq0SYcPH5bVavXv6rBa+fMIczCzB7rYiBEj9Omn\nn54W9q+88kq7sO/Xr58KCwu1Zs0aPfPMMxowYIAKCwuVmJh4xvcuKChQaWmp5s6dq6FDh2rKlCk6\nevSopJMz52eeeUYul0t9+vTR+PHj/QfZfZvExETdd999WrNmjVatWqUpU6b4VyUk6aqrrpLb7dbS\npUvV2tqqUaNGad68eZJOLuk/8MADWrt2rZ5++mnFx8dr5syZGjRokK677jp9/vnn/qPmJ02apG3b\ntnW4n5I0ePBg/fznP9ezzz7rP4J+2LBhQZ3CmJWVpYkTJ+ruu++W1+vV8uXLtXXrVv35z3/W8ePH\nlZ6erl/84hf+gxlvueUWLV68uMOnRwLRhPvZAwBgONapAAAwHGEPAIDhCHsAAAxH2AMAYDjCHgAA\nwxH2AAAYjrAHAMBwhD0AAIYj7AEAMNz/AY3sSxy9XyutAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f42f9180240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "plt.style.use('ggplot')\n",
    "ax = pd.Series([x[1] for x in wordcounts.most_common(n=3200)]).hist(\n",
    "    bins=72, \n",
    "    figsize=(8,5))\n",
    "ax.set_xlabel(\"Words  documents.\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a histogram of the top 3,200 words and how they are expressed across corpus. The spike on the right end of this chart shows there are nearly 60 words that appear in all 720 documents (as we can see in the text table above). As a whole, the higher bars on the left side of the chart indicate most of the words appear in a smaller number of documents. Here we use defactoring as a technique to investigate and even generate intermediate representations of the data, representations implicit in the data structures created by Underwood and Sellers, but not explicitly visualized in their narrative. For our purposes, this image is an interesting chapter in the story of the data precisely because it is in the middle of Underwood and Seller’s analysis. These middle states are often glossed over in the hurried rush for analysis to generate a meaningful result. Defactoring is an effort to slow down, take a breather, and reflect upon the data-work that has happened up until this point in the code. The meandering step-by-step journey through the code sometimes reveals very interesting paths not taken, such as the commented out code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocablist = binormal_select(vocablist, positivecounts, negativecounts, totalposvols, totalnegvols, 3000)\n",
    "# Feature selection is deprecated. There are cool things\n",
    "# we could do with feature selection,\n",
    "# but they'd improve accuracy by 1% at the cost of complicating our explanatory task.\n",
    "# The tradeoff isn't worth it. Explanation is more important.\n",
    "# So we just take the most common words (by number of documents containing them)\n",
    "# in the whole corpus. Technically, I suppose, we could crossvalidate that as well,\n",
    "# but *eyeroll*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author’s code above does not actually perform any work as each line has been commented out, however we include it because it points towards an execution path not taken and an interesting rationale for why it was not followed. In the “production” code the heuristic for feature selection is to simply select the 3200 most common words by their appearance in the 720 documents. This is a simple and easy technique to implement and–more importantly–explain to a literary history and digital humanities audience. Selecting the top words is a well established practice in text analysis and it has a high degree of methodologically face validity. It is a good mechanism for removing features that have diminishing returns. However, the commented code above tells a different, and methodologically significant, story.\n",
    "The comment discusses an alternative technique for feature selection using binormal selection. Because this function is commented out and not used in the analysis, we have opted to not include it as part of the defactoring. Instead, we have decided to focus on the more interesting rationale about why binormal selection is not being used in the analysis as indicated in the author’s comments:\n",
    "\n",
    "> There are cool things we could do with feature selection, but they'd improve accuracy by 1% at the cost of complicating our explanatory task.\n",
    "> The tradeoff isn't worth it. Explanation is more important.\n",
    "\n",
    "This comment reveals much about the reasoning, the effort, and energy focused on the important, but in the humanities oft neglected, work of discussing methodology. As Underwood argued in *The literary uses of high-dimensional space* (Underwood 2015b), while there is enormous potential for the application of statistical methods in humanistic fields like literary history there is resistance to these methods because there is a resistance to methodology. Underwood has described the humanities disciplines relationship to methodology as an “insistence on staging methodology as ethical struggle” (Underwood 2013). In this commented code we can see the material manifestation of Underwood’s sentiment, in this case embodied by self-censorship in the decision to not use more statistically robust techniques for feature selection. We do not argue this choice compromises the analysis or final conclusions, rather we want to highlight the practical and material ways a resistance to plurality in research methods manifests in the digital humanities. By focusing on a close reading of the code and execution environment, by *defactoring*, we provide a methodology to assist with the omnipresent *explanatory* task commensurate with the use of computational research methods in the humanities.\n",
    "\n",
    "In an algorithmic, data driven analysis, the selection of features is a *crucial* step because it affects the accuracy of the algorithm. In the digital humanities, feature selection is deeply embedded in the theory of the analysis and the context of the data. Claims made in and through this kind of analysis must attend to the representational configuration of the data. That is to say, we cannot take for granted how we have transformed data and what data are included or excluded from the analysis. Care, in the form of thorough documentation and thoughtful reflection, must be taken–especially at this unique moment in the development of digital humanities as we are still learning how algorithmic, data-driven techniques can be leveraged to better understand our objects of study. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "donttrainon = list()\n",
    "\n",
    "# Here we create a list of volumed IDs not to be used for training.\n",
    "# For instance, we have supplemented the dataset with volumes that\n",
    "# are in the Norton but that did not actually occur in random\n",
    "# sampling. We want to make predictions for these, but never use\n",
    "# them for training.\n",
    "\n",
    "for idx1, anid in enumerate(orderedIDs):\n",
    "    reviewedstatus = metadict[anid]['reviewed']\n",
    "    date = infer_date(metadict[anid], datetype)\n",
    "    if reviewedstatus == 'addedbecausecanon':\n",
    "        donttrainon.append(idx1)\n",
    "    elif date < pastthreshold or date > futurethreshold:\n",
    "        donttrainon.append(idx1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the comments describe, this block of code creates a list of volume IDs not to be used in the training. What that means *in code* is that any volume with the metadata label `addedbecauseofcanon` or with a date outside of the thresholds defined by `pastthreshold` and `futurethreshold` will be ignored. If we inspect the `donttrainon` variable we can see how many volumes satisfy these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable donttrainon contains 0 volume IDs\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"The variable donttrainon contains {} volume IDs\".format(\n",
    "    len(donttrainon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear there are no volumes to be filtered out by these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "authormatches = [list(donttrainon) for x in range(len(orderedIDs))]\n",
    "# For every index in authormatches, identify a set of indexes that have\n",
    "# the same author. Obvs, there will always be at least one.\n",
    "\n",
    "# Since we are going to use these indexes to exclude rows, we also add\n",
    "# all the ids in donttrainon to every volume\n",
    "\n",
    "for idx1, anid in enumerate(orderedIDs):\n",
    "    thisauthor = metadict[anid]['author']\n",
    "    for idx2, anotherid in enumerate(orderedIDs):\n",
    "        otherauthor = metadict[anotherid]['author']\n",
    "        if thisauthor == otherauthor and not idx2 in authormatches[idx1]:\n",
    "            authormatches[idx1].append(idx2)\n",
    "\n",
    "for alist in authormatches:\n",
    "    alist.sort(reverse = True)\n",
    "\n",
    "# I am reversing the order of indexes so that I can delete them from\n",
    "# back to front, without changing indexes yet to be deleted.\n",
    "# This will become important in the modelingprocess module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block of code Underwood and Sellers group the volumes by the same author. The list `authormatches` is a list of lists for each volume. Each sub-list contains the IDS of all the volumes by the same author. Essentially this data structure represents the potential relations of each volume to other volumes, with that relation being “other volumes by the same author.” This raises the question, how many volumes share the same author in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    579\n",
       "2     82\n",
       "3     30\n",
       "5     15\n",
       "4      8\n",
       "6      6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "# Tabular view of shared authorship\n",
    "pd.Series([len(x) for x in authormatches]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAFgCAYAAAC2QAPxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noeAAAICgQ8EBAABB5/8B58U3skrzVwEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f42f8099080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "# Barchart of shared authorship\n",
    "ax = (pd.Series([len(x) for x in authormatches])\n",
    "      .value_counts()\n",
    "      .plot(kind=\"barh\",\n",
    "           figsize=(8,5)))\n",
    "ax.set_xlabel(\"count\")\n",
    "ax.set_ylabel(\"Number of volumes with same author\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram tells us a majority of volumes are written by unique authors but that there are some authors who have written up to six volumes in the corpus. Note, we are generating this graph by counting the length of the list containing the volume IDs of other volumes by the same author. This means volumes written by the same author are counted twice. This is not an issue for the purposes of our inspection, just that the sum total number of volumes represented by this histogram is greater than 720."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING DEFINITIONS\n",
    "\n",
    "usedate = False\n",
    "# Leave this flag false unless you plan major\n",
    "# surgery to reactivate the currently-deprecated\n",
    "# option to use \"date\" as a predictive feature.\n",
    "\n",
    "def get_features(wordcounts, wordlist):\n",
    "    numwords = len(wordlist)\n",
    "    wordvec = np.zeros(numwords)\n",
    "    for idx, word in enumerate(wordlist):\n",
    "        if word in wordcounts:\n",
    "            wordvec[idx] = wordcounts[word]\n",
    "\n",
    "    return wordvec\n",
    "\n",
    "# In an earlier version of this script, we sometimes used\n",
    "# \"publication date\" as a feature, to see what would happen.\n",
    "# In the current version, we don't. Some of the functions\n",
    "# and features remain, but they are deprecated. E.g.:\n",
    "\n",
    "def get_features_with_date(wordcounts, wordlist, date, totalcount):\n",
    "    numwords = len(wordlist)\n",
    "    wordvec = np.zeros(numwords + 1)\n",
    "    for idx, word in enumerate(wordlist):\n",
    "        if word in wordcounts:\n",
    "            wordvec[idx] = wordcounts[word]\n",
    "\n",
    "    wordvec = wordvec / (totalcount + 0.0001)\n",
    "    wordvec[numwords] = date\n",
    "    return wordvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell defines two functions to be used below when opening and parsing the raw data files (in the `poems/` directory). The function `get_features()` simply takes the word counts from the parsed volume and filters out any words that are not part of `wordlist`, which contains the list of word features that had been selected for this analysis. We have also included a second function, `get_features_with_date()`, even though it is not executed. This residual code points to yet another path not taken, one that uses the volume’s publication date as a feature. As Underwood and Seller’s comment indicates, this was an experiment from an “earlier version of this script...to see what would happen.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "volsizes = dict()\n",
    "voldata = list()\n",
    "classvector = list()\n",
    "\n",
    "for volid, volpath in volspresent:\n",
    "\n",
    "    with open(volpath, encoding = 'utf-8') as f:\n",
    "        voldict = dict()\n",
    "        totalcount = 0\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) > 2 or len(fields) < 2:\n",
    "                continue\n",
    "\n",
    "            word = fields[0]\n",
    "            count = int(fields[1])\n",
    "            voldict[word] = count\n",
    "            totalcount += count\n",
    "\n",
    "    date = infer_date(metadict[volid], datetype)\n",
    "    date = date - 1700\n",
    "    if date < 0:\n",
    "        date = 0\n",
    "\n",
    "    if usedate:\n",
    "        features = get_features_with_date(voldict, \n",
    "                                          vocablist, \n",
    "                                          date, \n",
    "                                          totalcount)\n",
    "        voldata.append(features)\n",
    "    else:\n",
    "        features = get_features(voldict, vocablist)\n",
    "        voldata.append(features / (totalcount + 0.001))\n",
    "\n",
    "\n",
    "    volsizes[volid] = totalcount\n",
    "    classflag = classdictionary[volid]\n",
    "    classvector.append(classflag)\n",
    "    \n",
    "data = pd.DataFrame(voldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important code block because we are now pulling the raw data files from the poems/ directory into memory, filtering out the unselected word features, and putting the data into a vectorized data structure. The code loops over the `volspresent` variable and parses each individual volume into the `voldict` dictionary. At this stage the code is reading in all the words of a volume including their frequencies, and it is tabulating the total number of words in that volume. Once all of the data for the volume has been read into memory, the code calls the `get_features` function that throws out the words not part of the selected word features stored in the `vocablist` variable. This is where the top 3200 words are foregrounded and the remaining, less commonly used words, are discarded.\n",
    "\n",
    "At this point, any prosaic resemblance left in the data is gone and now we are dealing entirely with textual data in a numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector representation of The croakers by Drake, Joseph Rodman,\n",
      "The vector has a length of 3200.\n",
      "The first 100 elements of the vector:\n",
      "[ 199.  146.  885.  562.  176. 1152.  509.  195.  215.  956.  288.  156.\n",
      "  215.  200.  215.  159.  149. 2043.  222.   92.  171.  457.  162.  113.\n",
      "  146.  118.  335.   84.  181.   93.  102.  258.   65.  159.   79.  135.\n",
      "   72.   95.   61.   70.   84.   45.   73.   40.   44.   38.   88.   35.\n",
      "   73.   59.   74.  100.   76.   29.   54.   55.   40.   48.   51.   54.\n",
      "   21.   58.   37.   60.   63.   34.   40.   24.   63.   55.   11.   79.\n",
      "   28.   47.   21.   28.   39.   19.   15.   30.   44.   15.   23.   25.\n",
      "   81.   60.   33.   17.   15.   67.   33.   55.   31.   22.   55.   26.\n",
      "   35.   40.   14.  117.]\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"The vector representation of {} by {}\".format(\n",
    "    metadict[defactoring_volume_id]['title'], \n",
    "    metadict[defactoring_volume_id]['author']))\n",
    "print(\"The vector has a length of {}.\".format(\n",
    "    len(features)))\n",
    "print(\"The first 100 elements of the vector:\")\n",
    "print(features[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspection above shows us the last volume processed by the loop, *The Croakers* by Joseph Rodman Drake. As we can see, the words for this volume of poetry are now represented as a list of numbers (representing word frequencies). However, this list of numbers still requires additional transformation in order to be consumable by logistic regression. The word frequencies need to be normalized so they are comparable across volumes. To do this Underwood and Sellers divide the frequency of each individual word (each number in the list above) by the total number of words in that volume (the `totalcount` variable. This makes volumes of different lengths comparable by turning absolute frequencies into relative frequencies. One thing we initially did not understand is why the value of 0.001 has been added to the `totalcount` variable. When we asked, it turned out this is a “lazy” way to prevent divide-by-zero errors.\n",
    "\n",
    "The end result of the code we have executed thus far in the notebook is a very neat and tidy table of numbers between zero and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3190</th>\n",
       "      <th>3191</th>\n",
       "      <th>3192</th>\n",
       "      <th>3193</th>\n",
       "      <th>3194</th>\n",
       "      <th>3195</th>\n",
       "      <th>3196</th>\n",
       "      <th>3197</th>\n",
       "      <th>3198</th>\n",
       "      <th>3199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>0.006064</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.014809</td>\n",
       "      <td>0.005595</td>\n",
       "      <td>0.038597</td>\n",
       "      <td>0.016082</td>\n",
       "      <td>0.007203</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.020337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>0.006234</td>\n",
       "      <td>0.004711</td>\n",
       "      <td>0.013610</td>\n",
       "      <td>0.015704</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>0.045541</td>\n",
       "      <td>0.022842</td>\n",
       "      <td>0.006091</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.020320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.003458</td>\n",
       "      <td>0.020750</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.042088</td>\n",
       "      <td>0.009266</td>\n",
       "      <td>0.007243</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>0.007437</td>\n",
       "      <td>0.004857</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>0.004326</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>0.022236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>0.004548</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.020227</td>\n",
       "      <td>0.012845</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>0.026329</td>\n",
       "      <td>0.011633</td>\n",
       "      <td>0.004457</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.021849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "715  0.006064  0.004322  0.012832  0.014809  0.005595  0.038597  0.016082   \n",
       "716  0.006234  0.004711  0.013610  0.015704  0.003997  0.045541  0.022842   \n",
       "717  0.003002  0.003458  0.020750  0.018532  0.005024  0.042088  0.009266   \n",
       "718  0.007437  0.004857  0.011535  0.011839  0.003643  0.019200  0.012370   \n",
       "719  0.004548  0.003337  0.020227  0.012845  0.004022  0.026329  0.011633   \n",
       "\n",
       "         7         8         9       ...         3190      3191      3192  \\\n",
       "715  0.007203  0.003752  0.020337    ...     0.000034  0.000000  0.000067   \n",
       "716  0.006091  0.002379  0.020320    ...     0.000048  0.000048  0.000000   \n",
       "717  0.007243  0.001436  0.021403    ...     0.000000  0.000000  0.000000   \n",
       "718  0.004326  0.003794  0.022236    ...     0.000000  0.000152  0.000076   \n",
       "719  0.004457  0.004914  0.021849    ...     0.000000  0.000000  0.000000   \n",
       "\n",
       "         3193      3194      3195  3196      3197      3198      3199  \n",
       "715  0.000000  0.000067  0.000000   0.0  0.000000  0.000034  0.000000  \n",
       "716  0.000000  0.000143  0.000048   0.0  0.000143  0.000000  0.000048  \n",
       "717  0.000000  0.000000  0.000000   0.0  0.000000  0.000000  0.000261  \n",
       "718  0.000000  0.000000  0.000000   0.0  0.000076  0.000000  0.000000  \n",
       "719  0.000023  0.000046  0.000023   0.0  0.000000  0.000000  0.000069  \n",
       "\n",
       "[5 rows x 3200 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "# Normalized perspective of the data\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last row in that table, 719, is the volume we have been tracking, *The Croakers* by Joseph Rodman Drake. It is just one of 720 relatively indistinguishable rows of numbers in this representation of 19th century poetry. This is a radical transformation of the original, prosaic representation literary historians are probably used to seeing:\n",
    "\n",
    "![Screenshot of the Google Books site for The Croakers](notebook_resources/the-croakers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sextuplets = list()\n",
    "for i, volid in enumerate(orderedIDs):\n",
    "    listtoexclude = authormatches[i]\n",
    "    asixtuple = (data, \n",
    "                 classvector, \n",
    "                 listtoexclude, \n",
    "                 i, \n",
    "                 usedate, \n",
    "                 regularization)\n",
    "    sextuplets.append(asixtuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last step before Underwood and Sellers’s code moves away from the transformation of features and into the actual analysis of the data. This bit of code gathers all of the relevant data and metadata that has been cleaned and normalized in a structure suitable for performing the statistical analysis. The `sextuplets` variable is a list of 720 tuples containing six elements. Each item in the `sextuplets` list contains the necessary data structures to model each poem. The contents of each item in the list is as follows:\n",
    "\n",
    "- `data`: a normalized feature matrix. Word features are the columns and volumes are the rows with dimensions of 720 x 3200.\n",
    "- `classvector`: the classification or labels of volumes as either 'reviewed' (1) or 'random' (0).\n",
    "- `listtoexclude`: the list of poems to ignore because they are the same author.\n",
    "- ``i: the index of the volume\n",
    "- `usedate`: a flag indicating if date is a feature. It is false in this analysis.\n",
    "- `regularization`: a parameter for the logistic regression. This value was hardcoded at the beginning of the code in the Setting Parameters section.\n",
    "\n",
    "With all of the data assembled and in the right shape, a process we call *data fitness*, we can now venture into the algorithmic territory and perform the statistical analysis. As we can see, the *fitted* representation of features has traveled a great distance from the original poetry. One of the most important aspects of *distant reading* is the work of cleaning, preparing, and normalizing texts to be \"read\" by an algorithm. When considering distance, we should think not only of the perspective that we, the analyst, are reading from, but also the distance traveled in terms of successive transformations and representations of the data. If computational literary history is a triathalon, we have only completed the first endurance test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Predictive Models\n",
    "\n",
    "We are now about to dive into the very heart of the analysis, training predictive models on each volume. The code cells below do the following:\n",
    "\n",
    "\n",
    "- Iterate over every volume in the corpus. For each volume, do the following:\n",
    "    - Create a training set by removing the selected volume and other volumes by the same author from the corpus (performed by the `sliceframe()` function). In the language of predictive modeling, the removed volume is the “held out” data.\n",
    "    - Normalize the training data by computing the z-score for each feature/feature set (performed by the `normalizearray()` function).\n",
    "    - Fit the model on the training data (performed by the `model_one_volume()` function).\n",
    "    - Use the fitted model to predict the probability the (normalized) held out data was \"reviewed\" or \"random.\"\n",
    "    \n",
    "What is important to understand is that this section of the code does not train a single model, rather it trains 720 independent models–one for each volume. This process is called “leave-one-out” cross validation. As the code loops over the volumes, it holds out one volume, trains the model on all of the remaining volumes, and then uses that model to predict the status of the held out volume. Lather, Rinse, Repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING DEFINITION\n",
    "\n",
    "def sliceframe(dataframe, yvals, excludedrows, testrow):\n",
    "    numrows = len(dataframe)\n",
    "    newyvals = list(yvals)\n",
    "    for i in excludedrows:\n",
    "        del newyvals[i]\n",
    "        # NB: This only works if we assume that excluded rows\n",
    "        # has already been sorted in descending order !!!!!!!\n",
    "        # otherwise indexes will slide around as you delete\n",
    "\n",
    "    trainingset = dataframe.drop(dataframe.index[excludedrows])\n",
    "\n",
    "    newyvals = np.array(newyvals)\n",
    "    testset = dataframe.iloc[testrow]\n",
    "\n",
    "    return trainingset, newyvals, testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function prepares the data for training one model by separating volumes used for training from a single volume held out to test the predictive power of the model. The function takes a dataframe containing the feature vectors, a list of the classifications for each volume, a list of volumes to exclude (because of shared authorship), and the the index of the specific volume to be held out. It returns the dataframe with the held out volume removed (`trainingset`), a list of the known classifications (`newyvals`) corresponding to the training set, and the held-out volume that will be classified once the model has been trained (`testset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING DEFINITION\n",
    "\n",
    "def normalizearray(featurearray, usedate):\n",
    "    '''Normalizes an array by centering on means and\n",
    "    scaling by standard deviations. Also returns the\n",
    "    means and standard deviations for features.\n",
    "    '''\n",
    "\n",
    "    numinstances, numfeatures = featurearray.shape\n",
    "    means = list()\n",
    "    stdevs = list()\n",
    "    lastcolumn = numfeatures - 1\n",
    "    for featureidx in range(numfeatures):\n",
    "\n",
    "        thiscolumn = featurearray.iloc[ : , featureidx]\n",
    "        thismean = np.mean(thiscolumn)\n",
    "\n",
    "        thisstdev = np.std(thiscolumn)\n",
    "\n",
    "        if (not usedate) or featureidx != lastcolumn:\n",
    "            # If we're using date we don't normalize the last column.\n",
    "            means.append(thismean)\n",
    "            stdevs.append(thisstdev)\n",
    "            featurearray.iloc[ : , featureidx] = \\\n",
    "                (thiscolumn - thismean) / thisstdev\n",
    "        else:\n",
    "            print('FLAG')\n",
    "            means.append(thismean)\n",
    "            thisstdev = 0.1\n",
    "            stdevs.append(thisstdev)\n",
    "            featurearray.iloc[ : , featureidx] = \\\n",
    "                (thiscolumn - thismean) / thisstdev\n",
    "            # We set a small stdev for date.\n",
    "\n",
    "    return featurearray, means, stdevs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function standardizes the features by computing the z-score for the feature vectors. That is, it loops over each column of the data, subtracts the column mean from each value, and then divides that value by the standard deviation. This is an important step in the data preparation pipeline because it ensures all of the data values are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING DEFINITION\n",
    "\n",
    "def model_one_volume(data5tuple):\n",
    "    data, classvector, listtoexclude, i, usedate, regularization = \\\n",
    "        data5tuple\n",
    "    trainingset, yvals, testset = sliceframe(data, \n",
    "                                             classvector, \n",
    "                                             listtoexclude, \n",
    "                                             i)\n",
    "    newmodel = LogisticRegression(C = regularization)\n",
    "    trainingset, means, stdevs = normalizearray(trainingset, usedate)\n",
    "    newmodel.fit(trainingset, yvals)\n",
    "\n",
    "    testset = (testset - means) / stdevs\n",
    "    prediction = newmodel.predict_proba(testset.values.reshape(1, -1))[0][1]\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    # print(str(i) + \"  -  \" + str(len(listtoexclude)))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many respects, this is the most salient block of code in the entire document. The code above actually runs the logistic regression and does the algorithmic work that generates a prediction about each individual volume. This function builds upon the two previous functions to assemble a normalized set of training data (`trainingset`) distinct from the single volume to be predicted (`testset`).\n",
    "\n",
    "There are three lines of code involved in the computational modeling of the data. First, Underwood and Sellers instantiate a model object with the regularization parameter (more on that below):\n",
    "```\n",
    "newmodel = LogisticRegression(C = regularization)\n",
    "```\n",
    "Then they “fit” the model using the normalized training data:\n",
    "```\n",
    "newmodel.fit(trainingset, yvals)\n",
    "```\n",
    "Once a model has been “fit” to the data they can use that model to make predictions about unseen or held-out data. This is what they do with the `predict_proba()` function in this line:\n",
    "```\n",
    "prediction = newmodel.predict_proba(testset.reshape(1, -1))[0][1]\n",
    "```\n",
    "Those three lines are all it takes to do the computational part of of the analysis, the rest of the code up until this point has all been data preparation, cleaning, normalization, and re-shaping. This ratio of analytical to preparative code is interesting and indicates that claims that machines are eradicating scholars’ jobs are greatly exaggerated (Basken 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization and Logistic Regression\n",
    "\n",
    "The three lines of code above hide a significant amount of intellectual and computational work. The call to the `newmodel.fit()` function is a crucial step in the analytical process. Underwood and Sellers are using an implementation of Logistic Regression from the 3rd party Python library scikit-learn.\n",
    "\n",
    "At a very high level, logistic regression is a machine learning algorithm for performing classification. Logistic regression works by estimating the parameters of a function, the *hypothesis representation*, that divides a multidimensional space into two parts (note, in this case we are talking about binomial or binary logistic regression, which classifies things into one of two bins). The hypothesis representation describes a line that winds its way through the space creating what is called the *decision boundary*. Every data point that lands on one side the boundary gets one label and every data point on the other side of the boundary gets the other label. Similar to linear regression, the goal is to find the best hypothesis representation, that is, the function that best draws a line dividing the space given the known data points. Once you have a good hypothesis representation, an appropriately *fit* model, you can begin to classify *new* data by dropping data points into the multidimensional space and seeing on which side of the decision boundary they land.\n",
    "\n",
    "The key to logistic regression is estimating the parameters of the hypothesis representation–the parameters to the function that draws a line through the multidimensional space. We can derive the parameters by using the *features* of existing data combined with their known labels; this is called *training data*. The modeling process is executed by the function call to `newmodel.fit(trainingset, yvals)`. In Underwood and Sellers’s code the model uses the training data—the matrix of word features in the data variable and known labels (‘reviewed’ or ‘random’) in the classvector variable—to “learn” the parameters through a process called *coordinate descent*. How so-called optimization functions  work is well beyond the scope of the discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "\n",
    "One of the problems when fitting a logistic regression model is a tendency towards *overfitting*. Crudely this means the model, the function with the learned parameters, that you estimated have tailored themselves such that they are overly optimized to the particular training data you provided. As such, the model becomes less useful for prediction or classifying new data because they are outside the fitness of the model. An overfit model is like a snug pair of jeans, once you put on a few pounds (add new data) they don’t fit. In Underwood and Sellers’s case, they are fitting models on all volumes except one, which is held out. Then they test the predictive performance of the model by seeing if it correctly classifies the held-out volume. If they overfit the models, the model will to a terrible job guessing the status of the held out volumes.\n",
    "\n",
    "When Underwood and Sellers instantiated the model (`newmodel = LogisticRegression(C = regularization)`), they set a regularization parameter on the model. Regularization is a technique for logistic regression (and other machine learning algorithms) that smooths out the tendency toward overfitting with some more mathematical gymnastics. The diagram below shows how regularization can help with the fitness of the model:\n",
    "\n",
    "![Regularization](notebook_resources/regression_figures.png)\n",
    "*On the left side is a linear regression which doesn’t quite fit the data. In the middle is an overfit logistic regression. On right side is a regularized logistic regression.*\n",
    "\n",
    "As the diagrams show, the regularized logistic expression (the right side) does have a bit of error, there are pink and blue dots on the wrong sides of the decision boundary, but as more data get added it will generally be more right than the overfitted model as represented by the middle diagram (the squiggly decision boundary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning multiprocessing.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "450\n",
      "500\n",
      "400\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "Multiprocessing concluded.\n"
     ]
    }
   ],
   "source": [
    "# Now do leave-one-out predictions.\n",
    "print('Beginning multiprocessing.')\n",
    "\n",
    "pool = Pool(processes = 4)\n",
    "res = pool.map_async(model_one_volume, sextuplets)\n",
    "\n",
    "# After all files are processed, write metadata, errorlog, and counts of phrases.\n",
    "res.wait()\n",
    "resultlist = res.get()\n",
    "\n",
    "assert len(resultlist) == len(orderedIDs)\n",
    "\n",
    "logisticpredictions = dict()\n",
    "for i, volid in enumerate(orderedIDs):\n",
    "    logisticpredictions[volid] = resultlist[i]\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print('Multiprocessing concluded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code automates the training of 720 volumes, holding out one volume, training the model on the remaining volumes, and then making a prediction for the held-out volume. As a very computation intensive process training or fitting a logistic regression model takes time, and training 720 different models obviously takes 720 times longer. Fortunately, this is a so called embarrassingly parallel computational task and so we can train the models using parallel processing instead of one after the other. Using Python’s built in parallel processing modules, this code can speed up the process. Still, this block of code takes a fair amount of time to execute, around twenty minutes on a quad core MacBook Pro (late 2013 model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 720 predictions.\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"There are {} predictions.\".format(len(logisticpredictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What emerges from the other side of this computationally intensive task are a series of predictions, 720 to be specific, one for each of the modeled volumes. These predictions, stored in the `logisticpredictions` variable, are the model’s assertions of each volume’s reviewed status. Additionally, because we already know the status of the modeled volumes we can compare the performance of the predictive model to the “ground truth” and see if the algorithm was able to detect patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truepositives = 0\n",
    "truenegatives = 0\n",
    "falsepositives = 0\n",
    "falsenegatives = 0\n",
    "allvolumes = list()\n",
    "\n",
    "with open(outputpath, mode = 'w', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = ['volid', \n",
    "              'reviewed', \n",
    "              'obscure', \n",
    "              'pubdate', \n",
    "              'birthdate', \n",
    "              'gender', \n",
    "              'nation', \n",
    "              'allwords', \n",
    "              'logistic', \n",
    "              'author', \n",
    "              'title', \n",
    "              'pubname', \n",
    "              'actually', \n",
    "              'realclass']\n",
    "    writer.writerow(header)\n",
    "    for volid in IDsToUse:\n",
    "        metadata = metadict[volid]\n",
    "        reviewed = metadata['reviewed']\n",
    "        obscure = metadata['obscure']\n",
    "        pubdate = infer_date(metadata, datetype)\n",
    "        birthdate = metadata['birthdate']\n",
    "        gender = metadata['gender']\n",
    "        nation = metadata['nation']\n",
    "        author = metadata['author']\n",
    "        title = metadata['title']\n",
    "        canonicity = metadata['canonicity']\n",
    "        pubname = metadata['pubname']\n",
    "        allwords = volsizes[volid]\n",
    "        logistic = logisticpredictions[volid]\n",
    "        realclass = classdictionary[volid]\n",
    "        outrow = [volid, \n",
    "                  reviewed, \n",
    "                  obscure, \n",
    "                  pubdate,\n",
    "                  birthdate, \n",
    "                  gender, \n",
    "                  nation, \n",
    "                  allwords, \n",
    "                  logistic, \n",
    "                  author, \n",
    "                  title, \n",
    "                  pubname, \n",
    "                  canonicity, \n",
    "                  realclass]\n",
    "        writer.writerow(outrow)\n",
    "        allvolumes.append(outrow)\n",
    "\n",
    "        if logistic > 0.5 and classdictionary[volid] > 0.5:\n",
    "            truepositives += 1\n",
    "        elif logistic <= 0.5 and classdictionary[volid] < 0.5:\n",
    "            truenegatives += 1\n",
    "        elif logistic <= 0.5 and classdictionary[volid] > 0.5:\n",
    "            falsenegatives += 1\n",
    "        elif logistic > 0.5 and classdictionary[volid] < 0.5:\n",
    "            falsepositives += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a bit simpler than its predecessors. This block writes a CSV file to disk containing 720 rows of volume metadata, the predicted classification, and the actual classification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Coefficients\n",
    "\n",
    "The code below generates a single logistic regression model, trained on all of the data with nothing held-out. The properties of this model, the coefficients of the hypothesis representation, are interrogated to better understand the influence of individual features, words, on reviewed or unreviewed volumes. Thus this individual model is not using computational modeling to predict a phenomena, it is using the computational model to explore and explain patterns and features of the phenomena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "donttrainon.sort(reverse = True)\n",
    "trainingset, yvals, testset = sliceframe(data, \n",
    "                                         classvector, \n",
    "                                         donttrainon, \n",
    "                                         0)\n",
    "newmodel = LogisticRegression(C = regularization)\n",
    "trainingset, means, stdevs = normalizearray(trainingset, usedate)\n",
    "newmodel.fit(trainingset, yvals)\n",
    "\n",
    "coefficients = newmodel.coef_[0] * 100\n",
    "\n",
    "coefficientuples = list(zip(coefficients, \n",
    "                            (coefficients / np.array(stdevs)), \n",
    "                            vocablist + ['pub.date']))\n",
    "coefficientuples.sort()\n",
    "if verbose:\n",
    "    for coefficient, normalizedcoef, word in coefficientuples:\n",
    "        print(word + \" :  \" + str(coefficient))\n",
    "\n",
    "print()\n",
    "accuracy = (truepositives + truenegatives) / len(IDsToUse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code functions much like the code in the `model_one_volume(`) function except it only trains a single model for the purposes of investigating the impact of particular words on the prediction. By inspecting the magnitude of the coefficients Underwood and Sellers can see how particular words influenced a positive or negative prediction. Looking at the code, specifically the call to `sliceframe()` reveals this model actually does have some hold-out data, the first volume at index zero. We suspect the cost of excluding a single volume is less than the effort of re-implementing the `sliceframe()` function. The code to instantiate and train the model is identical to the code above, but instead of predicting the status of the held-out data the code extracts the coefficients from the model and puts them in the `coefficients` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3200 coefficients in this model.\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"There are {} coefficients in this model.\".format(len(coefficientuples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten items in the list of coefficients.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-0.451744515308058, -1531.5503056343666, \"i'll\"),\n",
       " (-0.42711374653208284, -7254.811741867888, 'mission'),\n",
       " (-0.4116879738928419, -862.7068493207225, 'tis'),\n",
       " (-0.37939299320003567, -1125.1778023608326, 'oft'),\n",
       " (-0.3783134051280357, -3337.335314702983, 'greet'),\n",
       " (-0.37399562365121103, -2558.9492503627807, 'wondrous'),\n",
       " (-0.3712630706903529, -222.8852087160384, 'our'),\n",
       " (-0.3695437312947107, -4356.707179481598, 'anxious'),\n",
       " (-0.35904965455922144, -2087.1252362010537, 'cheer'),\n",
       " (-0.35632804949908553, -1383.450564130784, \"we'll\")]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"First ten items in the list of coefficients.\")\n",
    "coefficientuples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ten items in the list of coefficients.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.3726635222640156, 4799.590619119897, 'brows'),\n",
       " (0.381170570302538, 224.41690606199722, 'not'),\n",
       " (0.3825388340735533, 1149.152152033426, 'half'),\n",
       " (0.38439578227360166, 398.793247462678, 'what'),\n",
       " (0.38473402074228275, 6977.874918531145, 'whereon'),\n",
       " (0.39921722974326485, 3171.5809993565736, 'sake'),\n",
       " (0.403144385914046, 2023.7759161873807, 'slow'),\n",
       " (0.40674741624813554, 3933.908677099459, 'hollow'),\n",
       " (0.41261956910143927, 1531.930520538605, 'black'),\n",
       " (0.4525125027699761, 615.0067489318402, 'eyes')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"Last ten items in the list of coefficients.\")\n",
    "coefficientuples[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients are a list of numbers, one per word feature, that determine the shape of the line through the multidimensional space. These results tell us the influence of particular words in the classification of volumes in the corpus. Looking at the normalized values helps us understand the degree to which particular words are more or less associated with reviewed or unreviewed poetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficientpath = outputpath.replace('.csv', '.coefs.csv')\n",
    "with open(coefficientpath, mode = 'w', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for triple in coefficientuples:\n",
    "        coef, normalizedcoef, word = triple\n",
    "        writer.writerow([word, coef, normalizedcoef])\n",
    "\n",
    "\n",
    "### DEFACTORING NAMESPACE\n",
    "rawaccuracy = accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates the `mainmodelcoefficients.csv` output file, which contains the word, its coefficient, and its normalized coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results\n",
    "\n",
    "The final function of the analysis is to test the accuracy of the model(s). This code produces a plot giving a sense of how the model performed compared to the known classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXyKRSLIIKfQlEokki/j/gyqTTkZDfIsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f42f90ecf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we divide the dataset with a horizontal line at 0.5, accuracy is:  0.7736111111111111\n",
      "Divided with a line fit to the data trend, it's  0.7916666666666666\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING FUNCTION PARAMETERS\n",
    "modeltype = 'linear'\n",
    "datelimits = []\n",
    "''' Takes a set of predictions produced by a model that knows nothing about date,\n",
    "and divides it along a line with a diachronic tilt. We need to do this in a way\n",
    "that doesn't violate crossvalidation. I.e., we shouldn't \"know\" anything\n",
    "that the model didn't know. We tried a couple of different ways to do this, but\n",
    "the simplest and actually most reliable is to divide the whole dataset along a\n",
    "linear central trend line for the data!\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "listofrows = list()\n",
    "classvector = list()\n",
    "\n",
    "# DEPRECATED\n",
    "# if modeltype == 'logistic' and len(datelimits) == 2:\n",
    "#     # In this case we construct a subset of data to model on.\n",
    "#     tomodeldata = list()\n",
    "#     tomodelclasses = list()\n",
    "#     pastthreshold, futurethreshold = datelimits\n",
    "\n",
    "for volume in allvolumes:\n",
    "    date = volume[3]\n",
    "    logistic = volume[8]\n",
    "    realclass = volume[13]\n",
    "    listofrows.append([logistic, date])\n",
    "    classvector.append(realclass)\n",
    "\n",
    "    # DEPRECATED\n",
    "    # if modeltype == 'logistic' and len(datelimits) == 2:\n",
    "    #     if date >= pastthreshold and date <= futurethreshold:\n",
    "    #         tomodeldata.append([logistic, date])\n",
    "    #         tomodelclasses.append(realclass)\n",
    "\n",
    "y, x = [a for a in zip(*listofrows)]\n",
    "plt.axis([min(x) - 2, max(x) + 2, min(y) - 0.02, max(y) + 0.02])\n",
    "reviewedx = list()\n",
    "reviewedy = list()\n",
    "randomx = list()\n",
    "randomy = list()\n",
    "\n",
    "for idx, reviewcode in enumerate(classvector):\n",
    "    if reviewcode == 1:\n",
    "        reviewedx.append(x[idx])\n",
    "        reviewedy.append(y[idx])\n",
    "    else:\n",
    "        randomx.append(x[idx])\n",
    "        randomy.append(y[idx])\n",
    "\n",
    "plt.plot(reviewedx, reviewedy, 'ro')\n",
    "plt.plot(randomx, randomy, 'k+')\n",
    "\n",
    "if modeltype == 'logistic':\n",
    "    # all this is DEPRECATED\n",
    "    print(\"Hey, you're attempting to use the logistic-tilt option\")\n",
    "    print(\"that we deactivated. Go in and uncomment the code.\")\n",
    "\n",
    "    # if len(datelimits) == 2:\n",
    "    #     data = pd.DataFrame(tomodeldata)\n",
    "    #     responsevariable = tomodelclasses\n",
    "    # else:\n",
    "    #     data = pd.DataFrame(listofrows)\n",
    "    #     responsevariable = classvector\n",
    "\n",
    "    # newmodel = LogisticRegression(C = 100000)\n",
    "    # newmodel.fit(data, responsevariable)\n",
    "    # coefficients = newmodel.coef_[0]\n",
    "\n",
    "    # intercept = newmodel.intercept_[0] / (-coefficients[0])\n",
    "    # slope = coefficients[1] / (-coefficients[0])\n",
    "\n",
    "    # p = np.poly1d([slope, intercept])\n",
    "\n",
    "elif modeltype == 'linear':\n",
    "    # what we actually do\n",
    "\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    slope = z[0]\n",
    "    intercept = z[1]\n",
    "\n",
    "plt.plot(x,p(x),\"b-\")\n",
    "plt.show(block = False)\n",
    "\n",
    "x = np.array(x, dtype='float64')\n",
    "y = np.array(y, dtype='float64')\n",
    "classvector = np.array(classvector)\n",
    "dividingline = intercept + (x * slope)\n",
    "predicted_as_reviewed = (y > dividingline)\n",
    "really_reviewed = (classvector == 1)\n",
    "\n",
    "accuracy = sum(predicted_as_reviewed == really_reviewed) / len(classvector)\n",
    "\n",
    "### DEFACTORING NAMESPACE\n",
    "tiltaccuracy = accuracy \n",
    "\n",
    "print('If we divide the dataset with a horizontal line at 0.5, accuracy is: ', \n",
    "      str(rawaccuracy))\n",
    "\n",
    "print(\"Divided with a line fit to the data trend, it's \", \n",
    "      str(tiltaccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above generates a best fit line for all of the results. Underwood and Sellers then selects the predicted-as-reviewed volumes, which are the volumes falling above the dividing line. The accuracy of the dividing line is the ratio between the total number of volumes predicted-as-reviewed and the number of actually reviewed volumes. Note that this is the accuracy of the dividing line, not the accuracy of the model(s) to predict the reviewed status of the poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
