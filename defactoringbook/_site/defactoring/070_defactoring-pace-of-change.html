<!DOCTYPE html>
<html lang="en">
  

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Defactoring ‘Pace of Change’</title>
  <meta name="description" content="Defactoring Pace of Change">

  <link rel="canonical" href="/YOUR%20URL//defactoring/070_defactoring-pace-of-change.html">
  <link rel="alternate" type="application/rss+xml" title="Defactoring ‘Pace of Change’" href="/YOUR%20URL//feed.xml">

  <meta property="og:url"         content="/YOUR%20URL//defactoring/070_defactoring-pace-of-change.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Defactoring ‘Pace of Change’" />
<meta property="og:description" content="Defactoring Pace of Change" />
<meta property="og:image"       content="" />


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "YOUR URL//defactoring/070_defactoring-pace-of-change.html",
  "headline":
    "Defactoring ‘Pace of Change’",
  "datePublished":
    "2019-05-03T23:54:03+02:00",
  "dateModified":
    "2019-05-03T23:54:03+02:00",
  "description":
    "Defactoring Pace of Change",
  "author": {
    "@type": "Person",
    "name": "Matthew Burton and Joris van Zundert"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "YOUR URL/",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "YOUR URL/",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">
  <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    CommonHTML: {
        linebreaks: {
            automatic: true,
        },
    },
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="/assets/js/anchor.min.js"  type="text/javascript"></script>
  <script>

initFunction(function () {
    anchors.add("main h1, main h2, main h3, main h4")
});

</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  <script src="https://unpkg.com/nbinteract-core" async></script>

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->

<script type="text/x-thebe-config">
    {
      requestKernel: true,
      binderOptions: {
        repo: 'YOUR-ORG/YOUR-REPO',
        ref: 'master',
      },
      kernelOptions: {
        name: 'python3',
      }
    }
</script>
<script src="https://unpkg.com/thebelab@0.3.3/lib/index.js"></script>
<script>
    /**
     * Add attributes to Thebelab blocks
     */

    const initThebelab = () => {
        const addThebelabToCodeCells = () => {
            console.log("Adding thebelab to code cells...");
            // If Thebelab hasn't loaded, wait a bit and try again. This
            // happens because we load ClipboardJS asynchronously.
            if (window.thebelab === undefined) {
                setTimeout(addThebelabToCodeCells, 250)
            return
            }

            // If we already detect a Thebelab cell, don't re-run
            if (document.querySelectorAll('div.thebelab-cell').length > 0) {
                return;
            }

            // Find all code cells, replace with Thebelab interactive code cells
            const codeCells = document.querySelectorAll('.input_area pre')
            codeCells.forEach((codeCell, index) => {
                const id = codeCellId(index)
                codeCell.setAttribute('data-executable', 'true')

                // Figure out the language it uses and add this too
                var parentDiv = codeCell.parentElement.parentElement;
                var arrayLength = parentDiv.classList.length;
                for (var ii = 0; ii < arrayLength; ii++) {
                    var parts = parentDiv.classList[ii].split('language-');
                    if (parts.length === 2) {
                        // If found, assign dataLanguage and break the loop
                        var dataLanguage = parts[1];
                        break;
                    }
                }
                codeCell.setAttribute('data-language', dataLanguage)

                // If the code cell is hidden, show it
                var inputCheckbox = document.querySelector(`input#hidebtn${codeCell.id}`);
                if (inputCheckbox !== null) {
                    setCodeCellVisibility(inputCheckbox, 'visible');
                }
            });

            // Remove the event listener from the page so keyboard press doesn't
            // Change page
            document.removeEventListener('keydown', initPageNav)
            keyboardListener = false;

            // Init thebelab
            thebelab.bootstrap();

            // Remove copy buttons since they won't work anymore
            const copyButtons = document.querySelectorAll('.copybtn')
            copyButtons.forEach((copyButton, index) => {
                copyButton.remove();
            });

            // Remove outputs since they'll be stale
            const outputs = document.querySelectorAll('.output *, .output')
            outputs.forEach((output, index) => {
                output.remove();
            });
        }

        // Add event listener for the function to modify code cells
        const thebelabButton = document.getElementById('interact-button-thebelab');
        if (thebelabButton === null) {
            setTimeout(initThebelab, 250)
        return
        };
        thebelabButton.addEventListener('click', addThebelabToCodeCells);
    }

    // Initialize Thebelab
    initFunction(initThebelab);
</script>


  <!-- Google analytics -->
  <script src="/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const codeCellId = index => `codecell${index}`

const clipboardButton = id =>
  `<a class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  const codeCells = document.querySelectorAll('div.highlighter-rouge:not(.output) pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.querySelector(`pre#${id} + a`) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>

  <!-- Hide cell code -->
  
<script>
/**
Add buttons to hide code cells
*/


var setCodeCellVisibility = function(inputField, kind) {
    // Update the image and class for hidden
    var id = inputField.getAttribute('data-id');
    var codeCell = document.querySelector(`#${id}`);

    if (kind === "visible") {
        codeCell.classList.remove('hidden');
        inputField.checked = true;
    } else {
        codeCell.classList.add('hidden');
        inputField.checked = false;
    }
}

var toggleCodeCellVisibility = function (event) {
    // The label is clicked, and now we decide what to do based on the input field's clicked status
    if (event.target.tagName === "LABEL") {
        var inputField = event.target.previousElementSibling;
    } else {
        // It is the span inside the target
        var inputField = event.target.parentElement.previousElementSibling;
    }

    if (inputField.checked === true) {
        setCodeCellVisibility(inputField, "visible");
    } else {
        setCodeCellVisibility(inputField, "hidden");
    }
}


// Button constructor
const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

var addHideButton = function () {
  // If a hide button is already added, don't add another
  if (document.querySelector('div.hidecode input') !== null) {
      return;
  }

  // Find the input cells and add a hide button
  document.querySelectorAll('div.input_area div.highlight').forEach(function (item, index) {
    if (!item.parentElement.classList.contains("hidecode")) {
        // Skip the cell if it doesn't have a hidecode class
        return;
    }

    const id = codeCellId(index)
    item.setAttribute('id', id);
    item.insertAdjacentHTML('afterend', hideCodeButton(id))

    // Set up the visibility toggle
    hideLink = document.querySelector(`#${id} + input + label`);
    hideLink.addEventListener('click', toggleCodeCellVisibility)
  });
}


// Initialize the hide buttos
var initHiddenCells = function () {
    // Add hide buttons to the cells
    addHideButton();

    // Toggle the code cells that should be hidden
    document.querySelectorAll('div.hidecode input').forEach(function (item) {
        setCodeCellVisibility(item, 'hidden');
        item.checked = true;
    })
}

initFunction(initHiddenCells);

</script>


  <!-- Load custom website scripts -->
  <script src="/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  

  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="/assets/js/lunr/lunr.min.js" type="text/javascript"></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>
</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  <a href="https://jupyter.org/jupyter-book/"><img src="/images/logo/logo.png" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/></a>
  <h2 class="c-sidebar__title">Defactoring ‘Pace of Change’</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/abstract.html"
        >
          
          Summary
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/repository.html"
        >
          
          GitHub repository
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__chapter"><a class="c-sidebar__entry" href="/search.html">Search</a></li>
        
      
      
        <li class="c-sidebar__divider"></li>
        
      
      
        <li><h2 class="c-sidebar__title">Article Content</li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/defactoring/010_introduction.html"
        >
          
          Introduction
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/defactoring/020_reading-and-evaluating.html"
        >
          
          Reading and Evaluating Code in Litrary Scholarship
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/defactoring/021_hinsen-s-layers-of-scientific-software.html"
                >
                  
                  Hinsen&#39;s Layers of Scientific Software
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/defactoring/030_defactoring.html"
        >
          
          Defactoring
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/defactoring/040_pace-of-change.html"
        >
          
          Pace of Change
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/defactoring/050_defactoring-in-practice.html"
        >
          
          Defactoring in Practice
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/defactoring/051_defactoring-code-from-python-files-to-the-notebook.html"
                >
                  
                  Defactoring Code From Python Files to the Notebook
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/defactoring/060_how-to-read-the-defactoring-notebook.html"
        >
          
          How to Read the Defactoring Notebook
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry c-sidebar__entry--active"
          href="/defactoring/070_defactoring-pace-of-change.html"
        >
          
          Defactoring ‘Pace of Change’
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/defactoring/080_discussion-and-conclusion.html"
        >
          
          Discussion and Conclusion
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/defactoring/081_representations-of-data-in-and-through-code.html"
                >
                  
                  Representations of Data in and Through Code
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/defactoring/082_what-defactoring-as-a-method-helps-us-understand.html"
                >
                  
                  What Defactoring as a Method Helps Us Understand
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/defactoring/083_method-made-material.html"
                >
                  
                  Method Made Material
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/defactoring/090_references.html"
        >
          
          References
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__divider"></li>
        
  </ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/jupyter/jupyter-book">Jupyter Book</a></p>
</nav>

      
      <!-- Shamelessly copied from minimal mistakes -->


<!-- TOC will only show up if it has at least one item -->


  <aside class="sidebar__right">
    <nav class="onthispage">
      <header><h4 class="nav__title"><i class="fa fa-list"></i>   On this page</h4></header>
      <ul class="toc__menu">
  <li><a href="#defactoring-pace-of-change">Defactoring Pace of Change</a>
    <ul>
      <li><a href="#importing-libraries">Importing Libraries</a></li>
      <li><a href="#setting-parameters">Setting Parameters</a></li>
      <li><a href="#preparing-metadata">Preparing Metadata</a></li>
      <li><a href="#transforming-words-into-features">Transforming Words into Features</a></li>
      <li><a href="#training-predictive-models">Training Predictive Models</a></li>
      <li><a href="#modeling-coefficients">Modeling Coefficients</a></li>
      <li><a href="#plotting-results">Plotting Results</a></li>
    </ul>
  </li>
</ul>
    </nav>
  </aside>


      
      <main class="c-textbook__page" tabindex="-1">
          <div class="o-wrapper">
            <div class="c-sidebar-toggle">
  <!-- We show the sidebar by default so we use .is-active -->
  <button
    id="js-sidebar-toggle"
    class="hamburger hamburger--arrowalt is-active"
  >
    <span class="hamburger-box">
      <span class="hamburger-inner"></span>
    </span>
    <span class="c-sidebar-toggle__label">Toggle Sidebar</span>
  </button>
</div>

            
<div class="buttons">
<a href="/content/defactoring/070_defactoring-pace-of-change.ipynb" download>
<button id="interact-button-download" class="interact-button">Download</button>
</a>

<button id="interact-button-thebelab" class="interact-button">Thebelab</button>




</div>


            <div class="c-textbook__content">
              <h2 id="defactoring-pace-of-change">Defactoring Pace of Change</h2>

<p>The code expressed below has nine steps:</p>
<ul>
  <li><a href="#Importing-Libraries">Importing Libraries</a> - Loads the necessary Python libraries needed for the analysis.</li>
  <li><a href="#Setting-Parameters">Setting Parameters</a> - Specifies parameters for the loading, cleaning, and labeling of data as well as sets conditions for the logistic regression.</li>
  <li><a href="#Preparing-MetaData">Preparing Metadata</a> - Generates a list of *.tsv files from the <code class="highlighter-rouge">poems/</code> directory.
    <ul>
      <li><a href="#Cleaning-Metadata">Cleaning Metadata</a> - Loads the metadata file, <code class="highlighter-rouge">poemetadata.csv</code> and performs some cleaning of the metadata to make labeling easier.</li>
      <li><a href="#Sorting-Data">Sorting Training Data</a> - Sort the volumes into two bins, reviewed and not reviewed using the cleaned metadata.</li>
    </ul>
  </li>
  <li><a href="#Transforming-Words-into-Features">Transforming Words into Features</a> - Identifies the 3,200 most common words in the corpus. Those most common words will be the features for the regression.
    <ul>
      <li><a href="#Filtering-Authors">Filtering Authors</a> - Removes poems by authors who have been reviewed.</li>
      <li><a href="#Filtering-Words">Filtering Words</a> - Remove any words from the poem data that are not in the most-common feature list.</li>
    </ul>
  </li>
  <li><a href="#Training-Predictive-Models">Training Predictive Models</a> - Run a separate logistic regression for each volume, using a single volume as held-out data and measure each model’s predictive power.</li>
  <li><a href="#Modeling-Coefficients">Modeling Coefficients</a> - Run a single logistic regression over all the data to inspect the salient coefficients.</li>
  <li><a href="#Plotting-Results">Plotting Results</a> - Generate a plot showing the accuracy of the predictive models.</li>
</ul>

<h3 id="importing-libraries">Importing Libraries</h3>

<p>This section loads a series of libraries used in the Pace of Change analysis.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING IMPORT</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">csv</span> 
<span class="kn">import</span> <span class="nn">random</span> 
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span> 

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span> 

<span class="kn">from</span> <span class="nn">multiprocessing</span> <span class="kn">import</span> <span class="n">Pool</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span> 
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</code></pre></div></div>

<p>We begin examination of the code by importing a series of Python libraries into working memory. This is the boundary between the layers of bespoke code and existing general purpose (os, csv, random) and scientific computing libraries (numpy, pandas, sklearn). Following from Hinsen’s model of layers of scientific software, what is missing is the inclusion of libraries from the <em>disciplinary</em> layer. The most specific library in terms of use in the Hinsen model is the LogisticRegression model from Scikit-Learn, but we’d argue this lives in layer two, scientific software, because it  is broadly applicable across a variety of disciplines. This begs the question, what or where are the disciplinary Python libraries for literary history or digital humanities? What functions would the perform? What domain specific tasks or methods need to encoded into a disciplinary library? Perhaps it is too early for such libraries to exist as the practices of computational and data intensive research are still new (in literary history).</p>

<h3 id="setting-parameters">Setting Parameters</h3>

<p>The first section of the code sets a series of parameters specifying what data to process, where data are located, and parameters for the logistic regression. While there is no complex logic or work being done in this section, many assumptions and important distinctions that shape the execution of subsequent code are defined here.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## PATHS.</span>

<span class="n">sourcefolder</span> <span class="o">=</span> <span class="s">'poems/'</span>
<span class="n">extension</span> <span class="o">=</span> <span class="s">'.poe.tsv'</span>
<span class="n">classpath</span> <span class="o">=</span> <span class="s">'poemeta.csv'</span>
<span class="n">outputpath</span> <span class="o">=</span> <span class="s">'mainmodelpredictions.csv'</span>

<span class="c">## EXCLUSIONS.</span>

<span class="n">excludeif</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">excludeif</span><span class="p">[</span><span class="s">'pubname'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'TEM'</span>
<span class="c"># We're not using reviews from Tait's.</span>

<span class="n">excludeif</span><span class="p">[</span><span class="s">'recept'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'addcanon'</span>
<span class="c"># We don't ordinarily include canonical volumes that were not in either sample.</span>
<span class="c"># These are included only if we're testing the canon specifically.</span>

<span class="n">excludeifnot</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">excludeabove</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">excludebelow</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="n">excludebelow</span><span class="p">[</span><span class="s">'firstpub'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1700</span>
<span class="n">excludeabove</span><span class="p">[</span><span class="s">'firstpub'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1950</span>
<span class="n">sizecap</span> <span class="o">=</span> <span class="mi">360</span>

<span class="c"># For more historically-interesting kinds of questions, we can limit the part</span>
<span class="c"># of the dataset that gets TRAINED on, while permitting the whole dataset to</span>
<span class="c"># be PREDICTED. (Note that we always exclude authors from their own training</span>
<span class="c"># set; this is in addition to that.) The variables futurethreshold and</span>
<span class="c"># pastthreshold set the chronological limits of the training set, inclusive</span>
<span class="c"># of the threshold itself.</span>

<span class="c">## THRESHOLDS</span>

<span class="n">futurethreshold</span> <span class="o">=</span> <span class="mi">1925</span>
<span class="n">pastthreshold</span> <span class="o">=</span> <span class="mi">1800</span>

<span class="c"># CLASSIFY CONDITIONS</span>

<span class="n">positive_class</span> <span class="o">=</span> <span class="s">'rev'</span>
<span class="n">category2sorton</span> <span class="o">=</span> <span class="s">'reviewed'</span>
<span class="n">datetype</span> <span class="o">=</span> <span class="s">'firstpub'</span>
<span class="n">numfeatures</span> <span class="o">=</span> <span class="mi">3200</span>
<span class="n">regularization</span> <span class="o">=</span> <span class="o">.</span><span class="mo">00007</span>


<span class="n">paths</span> <span class="o">=</span> <span class="p">(</span><span class="n">sourcefolder</span><span class="p">,</span> <span class="n">extension</span><span class="p">,</span> <span class="n">classpath</span><span class="p">,</span> <span class="n">outputpath</span><span class="p">)</span>
<span class="n">exclusions</span> <span class="o">=</span> <span class="p">(</span><span class="n">excludeif</span><span class="p">,</span> 
              <span class="n">excludeifnot</span><span class="p">,</span> 
              <span class="n">excludebelow</span><span class="p">,</span> 
              <span class="n">excludeabove</span><span class="p">,</span> 
              <span class="n">sizecap</span><span class="p">)</span>
<span class="n">thresholds</span> <span class="o">=</span> <span class="p">(</span><span class="n">pastthreshold</span><span class="p">,</span> 
              <span class="n">futurethreshold</span><span class="p">)</span>
<span class="n">classifyconditions</span> <span class="o">=</span> <span class="p">(</span><span class="n">category2sorton</span><span class="p">,</span> 
                      <span class="n">positive_class</span><span class="p">,</span> 
                      <span class="n">datetype</span><span class="p">,</span> 
                      <span class="n">numfeatures</span><span class="p">,</span> 
                      <span class="n">regularization</span><span class="p">)</span>
</code></pre></div></div>

<p>The parameters defined in the code cell above are a set of knobs and switches used to tweak the performance and execution of the computational modeling process. Underwood and Sellers have collected the parameters into four categories: <em>paths</em>, <em>exclusions</em>, <em>thresholds</em>, and <em>classifyconditions</em>. These categories are simultaneously distinguished discursively through the code comments (the lines beginning with a #) and technologically through four variable assignments, <code class="highlighter-rouge">exclusions</code>, <code class="highlighter-rouge">thresholds</code>, and <code class="highlighter-rouge">classifyconditions</code>. Because technically speaking the grouping of parameters is not strictly necessary, each of these four variables embody stylistic choices of the authors as a means of organizing and structuring the information they are encoding in Python.</p>

<p>The variables in <code class="highlighter-rouge">paths</code> specify the location of the data and metadata files as well as where to write the output files at the completion of the analysis. The variables in <code class="highlighter-rouge">exclusions</code> specify data and types of data to be excluded from the analysis, such as reviews from <em>Tait’s Endinburgh Magazine</em> (https://en.wikipedia.org/wiki/Tait%27s_Edinburgh_Magazine), which we infer from the author’s comments. Additional exclusions specify temporal boundaries from 1700 to 1950. A further set of two variables in <code class="highlighter-rouge">thresholds</code> also articulates a temporal boundary from 1800 to 1925. The comments indicate this distinguishes the temporal window for datasets using in <em>training</em> versus those used during <em>prediction.</em> The variables in <code class="highlighter-rouge">classifyconditions</code> are important parameters for the logistic regression, specifying the number of variables to train the model upon as well as setting the regularization parameter (<code class="highlighter-rouge">regularization</code>) for the logistic regression. What is not well documented here, is why the value .00007 was chosen over other values.</p>

<h3 id="preparing-metadata">Preparing Metadata</h3>

<p>With the preparation of metadata we begin to see some logical work of Pace of Change being conducted. The code in this section has two subsections, one to clean the metadata and another to sort the training data. All of the work in this section focuses on preparing the metadata, identified in the <code class="highlighter-rouge">classpath</code> variable and the filenames of the individual data files in the <code class="highlighter-rouge">sourcefolder</code>. The main task of this section is to organize the metadata of the volumes and their associated labels (positive or negative) for training the logistic regression. All of the code in this section attends to the cleanliness of the metadata; we will not start digging into the data itself until the next section.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING FUNCTION</span>
<span class="c">### def create_model(paths, exclusions, thresholds, classifyconditions):</span>
<span class="s">''' This is the main function in the module.
It can be called externally; it's also called
if the module is run directly.
'''</span>
<span class="n">verbose</span> <span class="o">=</span> <span class="bp">False</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">sourcefolder</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">'/'</span><span class="p">):</span>
    <span class="n">sourcefolder</span> <span class="o">=</span> <span class="n">sourcefolder</span> <span class="o">+</span> <span class="s">'/'</span>

<span class="c"># This just makes things easier.</span>

<span class="c"># Get a list of files.</span>
<span class="n">allthefiles</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">sourcefolder</span><span class="p">)</span>
<span class="c"># random.shuffle(allthefiles)</span>

<span class="n">volumeIDs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">volumepaths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">allthefiles</span><span class="p">:</span>

    <span class="k">if</span> <span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">extension</span><span class="p">):</span>
        <span class="n">volID</span> <span class="o">=</span> <span class="n">filename</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">extension</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
        <span class="c"># The volume ID is basically the filename minus its extension.</span>
        <span class="c"># Extensions are likely to be long enough that there is little</span>
        <span class="c"># danger of accidental occurrence inside a filename. E.g.</span>
        <span class="c"># '.fic.tsv'</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">sourcefolder</span> <span class="o">+</span> <span class="n">filename</span>
        <span class="n">volumeIDs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">volID</span><span class="p">)</span>
        <span class="n">volumepaths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</code></pre></div></div>

<p>This code assembles a list of volume identifiers (<code class="highlighter-rouge">volumeIDs</code>) and file paths (<code class="highlighter-rouge">volumepaths</code>) by readings the directory listing of files in the <code class="highlighter-rouge">poems/</code> directory (<code class="highlighter-rouge">sourcefolder</code>). The filenames are in and of themselves a source of metadata, but as we will see in the code below, they need to be reconciled with the metadata stored separately from the data files.</p>

<p>We are curious about the contents of the <code class="highlighter-rouge">volumeIDs</code> and <code class="highlighter-rouge">volumepaths</code> variables.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="c">### Inspect the two variables defined in the codecell above.</span>
<span class="c">### We know they are lists so lets just look at the first item.</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The first item in volumeIDs is: "</span><span class="p">,</span> <span class="n">volumeIDs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The first item in volumepaths is: "</span><span class="p">,</span><span class="n">volumepaths</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The first item in volumeIDs is:  loc.ark+=13960=t5n881k59
The first item in volumepaths is:  poems/loc.ark+=13960=t5n881k59.poe.tsv

</code></pre></div></div>

<p>The code has created an alignment between identifiers in the metadata records and the filename identifiers of the TSV data files themselves (located in the <code class="highlighter-rouge">poems/</code> folder). These identifiers, <code class="highlighter-rouge">dul1.ark+=13960=t5fb5xg2z</code>, are the threads that stitch together the various representations of the (meta)data.</p>

<h4 id="cleaning-metadata">Cleaning Metadata</h4>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING FUNCTION DEFINITION</span>
<span class="c">### we need these helper functions for execute the next code cell</span>

<span class="k">def</span> <span class="nf">dirty_pairtree</span><span class="p">(</span><span class="n">htid</span><span class="p">):</span>
    <span class="n">period</span> <span class="o">=</span> <span class="n">htid</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s">'.'</span><span class="p">)</span>
    <span class="n">prefix</span> <span class="o">=</span> <span class="n">htid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">period</span><span class="p">]</span>
    <span class="n">postfix</span> <span class="o">=</span> <span class="n">htid</span><span class="p">[(</span><span class="n">period</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span> <span class="p">]</span>
    <span class="k">if</span> <span class="s">'='</span> <span class="ow">in</span> <span class="n">postfix</span><span class="p">:</span>
        <span class="n">postfix</span> <span class="o">=</span> <span class="n">postfix</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'+'</span><span class="p">,</span><span class="s">':'</span><span class="p">)</span>
        <span class="n">postfix</span> <span class="o">=</span> <span class="n">postfix</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'='</span><span class="p">,</span><span class="s">'/'</span><span class="p">)</span>
    <span class="n">dirtyname</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s">"."</span> <span class="o">+</span> <span class="n">postfix</span>
    <span class="k">return</span> <span class="n">dirtyname</span>

<span class="k">def</span> <span class="nf">forceint</span><span class="p">(</span><span class="n">astring</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">intval</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">astring</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">intval</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">return</span> <span class="n">intval</span>
</code></pre></div></div>

<p>This code cell defines two functions used in the code below. The first is <code class="highlighter-rouge">dirty_pairtree()</code>, which cleans up the identifiers in the data. This issue arises from the fact that the HathiTrust (where Underwood and Sellers got their data from) uses IDs that cannot be expressed on the filesystem and Underwood and Sellers encoded ID metadata in filenames. The “/” and “:” characters in the IDs cannot be part of a file name. So, because the volumes are stored as individual files they have a “+” and an “=” instead. However, the IDs are stored in the original format in the metadata file so the IDS have to be transformed back into the original HathiTrust format.  The second function is called <code class="highlighter-rouge">forceint()</code> and transforms numbers expressed as Python strings into the python integer data type with a bit of error handling in the case values that throw and error when being cast as an integer.</p>

<p>What does the the metadata look like? We can inspect the beginning of file to get a sense of the material conditions of the metadata.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="n">metadata_file</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">classpath</span><span class="p">)</span>
<span class="c">#print(metadata_file.shape)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The metadata files has {} rows and {} columns."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="o">*</span><span class="n">metadata_file</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">metadata_file</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The metadata files has 728 rows and 22 columns.

</code></pre></div></div>

<div class="output output_html">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>docid</th>
      <th>actualdate</th>
      <th>inferreddate</th>
      <th>firstpub</th>
      <th>recept</th>
      <th>recordid</th>
      <th>OCLC</th>
      <th>author</th>
      <th>imprint</th>
      <th>enumcron</th>
      <th>...</th>
      <th>judge</th>
      <th>impaud</th>
      <th>yrrev</th>
      <th>pubname</th>
      <th>birth</th>
      <th>gender</th>
      <th>nationality</th>
      <th>othername</th>
      <th>notes</th>
      <th>canon</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>loc.ark+=13960=t8sb4zz1q</td>
      <td>1921</td>
      <td>1921</td>
      <td>1921</td>
      <td>addcanon</td>
      <td>537314.0</td>
      <td>NaN</td>
      <td>Lawrence, D. H.</td>
      <td>New York;T. Seltzer;1921.</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1885</td>
      <td>m</td>
      <td>uk</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>y</td>
    </tr>
    <tr>
      <th>1</th>
      <td>uc1.b3342759</td>
      <td>1919</td>
      <td>1919</td>
      <td>1919</td>
      <td>random</td>
      <td>7930862.0</td>
      <td>NaN</td>
      <td>Wigren, Bessie C.</td>
      <td>Boston;The Poet Lore Company;c1919</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1874</td>
      <td>f</td>
      <td>us</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>uc1.b4100590</td>
      <td>1918</td>
      <td>1918</td>
      <td>1918</td>
      <td>reviewed</td>
      <td>6154122.0</td>
      <td>2143179.0</td>
      <td>Waugh, Alec,</td>
      <td>London;G. Richards;1918.</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1918.0</td>
      <td>EGO</td>
      <td>1898</td>
      <td>m</td>
      <td>uk</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>uc1.b3340220</td>
      <td>1918</td>
      <td>1918</td>
      <td>1918</td>
      <td>reviewed</td>
      <td>7917249.0</td>
      <td>12688503.0</td>
      <td>Nightingale, M.</td>
      <td>Oxford [Oxfordshire;B.H. Blackwell;1918.</td>
      <td>NaN</td>
      <td>...</td>
      <td>neg</td>
      <td>NaN</td>
      <td>1919.0</td>
      <td>EGO</td>
      <td>1879</td>
      <td>f</td>
      <td>uk</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>uc2.ark+=13960=t0ft8gj1k</td>
      <td>1918</td>
      <td>1918</td>
      <td>1918</td>
      <td>reviewed</td>
      <td>7657411.0</td>
      <td>2518108.0</td>
      <td>Faber, Geoffrey,</td>
      <td>Oxford;B. H. Blackwell;New York;Longmans, Gree...</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1918.0</td>
      <td>EGO</td>
      <td>1889</td>
      <td>m</td>
      <td>uk</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 22 columns</p>
</div>
</div>

<p>Using <em>defactoring inspection</em> we can actually look at the metadata file and inspect the first five rows of metadata. By blending the original code with our inspection code and narrative the metadata becomes less of a conceptual abstraction and more of a tangible, material object that we can interrogate. Here we can see the file has 728 rows and 22 columns as well as the contents of the first five rows.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING FUNCTION </span>
<span class="c">### def get_metadata(classpath, volumeIDs, excludeif, excludeifnot, excludebelow, excludeabove):</span>
<span class="s">'''
As the name would imply, this gets metadata matching a given set of volume
IDs. It returns a dictionary containing only those volumes that were present
both in metadata and in the data folder.

It also accepts four dictionaries containing criteria that will exclude volumes
from the modeling process.
'''</span>
<span class="k">print</span><span class="p">(</span><span class="n">classpath</span><span class="p">)</span>
<span class="n">metadict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">classpath</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictReader</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="n">anonctr</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
        <span class="n">volid</span> <span class="o">=</span> <span class="n">dirty_pairtree</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s">'docid'</span><span class="p">])</span>
        <span class="n">theclass</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">'recept'</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

        <span class="c"># I've put 'remove' in the reception column for certain</span>
        <span class="c"># things that are anomalous.</span>
        <span class="k">if</span> <span class="n">theclass</span> <span class="o">==</span> <span class="s">'remove'</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="n">bail</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">excludeif</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">==</span> <span class="n">value</span><span class="p">:</span>
                <span class="n">bail</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">excludeifnot</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">!=</span> <span class="n">value</span><span class="p">:</span>
                <span class="n">bail</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">excludebelow</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">forceint</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">value</span><span class="p">:</span>
                <span class="n">bail</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">excludeabove</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">forceint</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">value</span><span class="p">:</span>
                <span class="n">bail</span> <span class="o">=</span> <span class="bp">True</span>

        <span class="k">if</span> <span class="n">bail</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"DEFACTORING: Excluding volume with id "</span><span class="o">+</span><span class="n">volid</span><span class="p">)</span> <span class="c">### DEFACTORING CODE</span>
            <span class="k">continue</span>

        <span class="n">birthdate</span> <span class="o">=</span> <span class="n">forceint</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s">'birth'</span><span class="p">])</span>

        <span class="n">pubdate</span> <span class="o">=</span> <span class="n">forceint</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s">'inferreddate'</span><span class="p">])</span>

        <span class="n">gender</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">'gender'</span><span class="p">]</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>
        <span class="n">nation</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">'nationality'</span><span class="p">]</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>

        <span class="c">#if pubdate &gt;= 1880:</span>
            <span class="c">#continue</span>

        <span class="k">if</span> <span class="n">nation</span> <span class="o">==</span> <span class="s">'ca'</span><span class="p">:</span>
            <span class="n">nation</span> <span class="o">=</span> <span class="s">'us'</span>
        <span class="k">elif</span> <span class="n">nation</span> <span class="o">==</span> <span class="s">'ir'</span><span class="p">:</span>
            <span class="n">nation</span> <span class="o">=</span> <span class="s">'uk'</span>
        <span class="c"># I hope none of my Canadian or Irish friends notice this.</span>

        <span class="n">notes</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">'notes'</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="n">author</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">'author'</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">author</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">author</span> <span class="o">==</span> <span class="s">'&lt;blank&gt;'</span><span class="p">:</span>
            <span class="n">author</span> <span class="o">=</span> <span class="s">"anonymous"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">anonctr</span><span class="p">)</span>
            <span class="n">anonctr</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">title</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">'title'</span><span class="p">]</span>
        <span class="n">canon</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">'canon'</span><span class="p">]</span>

        <span class="c"># I'm creating two distinct columns to indicate kinds of</span>
        <span class="c"># literary distinction. The reviewed column is based purely</span>
        <span class="c"># on the question of whether this work was in fact in our</span>
        <span class="c"># sample of contemporaneous reviews. The obscure column incorporates</span>
        <span class="c"># information from post-hoc biographies, which trumps</span>
        <span class="c"># the question of reviewing when they conflict.</span>

        <span class="k">if</span> <span class="n">theclass</span> <span class="o">==</span> <span class="s">'random'</span><span class="p">:</span>
            <span class="n">obscure</span> <span class="o">=</span> <span class="s">'obscure'</span>
            <span class="n">reviewed</span> <span class="o">=</span> <span class="s">'not'</span>
        <span class="k">elif</span> <span class="n">theclass</span> <span class="o">==</span> <span class="s">'reviewed'</span><span class="p">:</span>
            <span class="n">obscure</span> <span class="o">=</span> <span class="s">'known'</span>
            <span class="n">reviewed</span> <span class="o">=</span> <span class="s">'rev'</span>
        <span class="k">elif</span> <span class="n">theclass</span> <span class="o">==</span> <span class="s">'addcanon'</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"DEFACTORING: adding volume"</span><span class="p">)</span> <span class="c">### DEFACTORING CODE</span>
            <span class="n">obscure</span> <span class="o">=</span> <span class="s">'known'</span>
            <span class="n">reviewed</span> <span class="o">=</span> <span class="s">'addedbecausecanon'</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Missing class"</span> <span class="o">+</span> <span class="n">theclass</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">notes</span> <span class="o">==</span> <span class="s">'well-known'</span><span class="p">:</span>
            <span class="n">obscure</span> <span class="o">=</span> <span class="s">'known'</span>
        <span class="k">if</span> <span class="n">notes</span> <span class="o">==</span> <span class="s">'obscure'</span><span class="p">:</span>
            <span class="n">obscure</span> <span class="o">=</span> <span class="s">'obscure'</span>

        <span class="k">if</span> <span class="n">canon</span> <span class="o">==</span> <span class="s">'y'</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">theclass</span> <span class="o">==</span> <span class="s">'addcanon'</span><span class="p">:</span>
                <span class="n">actually</span> <span class="o">=</span> <span class="s">'Norton, added'</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">actually</span> <span class="o">=</span> <span class="s">'Norton, in-set'</span>
        <span class="k">elif</span> <span class="n">reviewed</span> <span class="o">==</span> <span class="s">'rev'</span><span class="p">:</span>
            <span class="n">actually</span> <span class="o">=</span> <span class="s">'reviewed'</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">actually</span> <span class="o">=</span> <span class="s">'random'</span>

        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">][</span><span class="s">'reviewed'</span><span class="p">]</span> <span class="o">=</span> <span class="n">reviewed</span>
        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">][</span><span class="s">'obscure'</span><span class="p">]</span> <span class="o">=</span> <span class="n">obscure</span>
        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">][</span><span class="s">'pubdate'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pubdate</span>
        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">][</span><span class="s">'birthdate'</span><span class="p">]</span> <span class="o">=</span> <span class="n">birthdate</span>
        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">][</span><span class="s">'gender'</span><span class="p">]</span> <span class="o">=</span> <span class="n">gender</span>
        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">][</span><span class="s">'nation'</span><span class="p">]</span> <span class="o">=</span> <span class="n">nation</span>
        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">][</span><span class="s">'author'</span><span class="p">]</span> <span class="o">=</span> <span class="n">author</span>
        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">][</span><span class="s">'title'</span><span class="p">]</span> <span class="o">=</span> <span class="n">title</span>
        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">][</span><span class="s">'canonicity'</span><span class="p">]</span> <span class="o">=</span> <span class="n">actually</span>
        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">][</span><span class="s">'pubname'</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s">'pubname'</span><span class="p">]</span>
        <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">][</span><span class="s">'firstpub'</span><span class="p">]</span> <span class="o">=</span> <span class="n">forceint</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s">'firstpub'</span><span class="p">])</span>

<span class="c"># These come in as dirty pairtree; we need to make them clean.</span>

<span class="n">cleanmetadict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">allidsinmeta</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">metadict</span><span class="o">.</span><span class="n">keys</span><span class="p">()])</span>
<span class="n">allidsindir</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">dirty_pairtree</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">volumeIDs</span><span class="p">])</span>
<span class="n">missinginmeta</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">allidsindir</span> <span class="o">-</span> <span class="n">allidsinmeta</span><span class="p">)</span>
<span class="n">missingindir</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">allidsinmeta</span> <span class="o">-</span> <span class="n">allidsindir</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"We have "</span> 
      <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">missinginmeta</span><span class="p">)</span> 
      <span class="o">+</span> <span class="s">" volumes in missing in metadata, and"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">missingindir</span><span class="p">)</span> <span class="o">+</span> <span class="s">" volumes missing in the directory."</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">allidsinmeta</span> <span class="o">-</span> <span class="n">allidsindir</span><span class="p">)</span>

<span class="k">for</span> <span class="n">anid</span> <span class="ow">in</span> <span class="n">volumeIDs</span><span class="p">:</span>
    <span class="n">dirtyid</span> <span class="o">=</span> <span class="n">dirty_pairtree</span><span class="p">(</span><span class="n">anid</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dirtyid</span> <span class="ow">in</span> <span class="n">metadict</span><span class="p">:</span>
        <span class="n">cleanmetadict</span><span class="p">[</span><span class="n">anid</span><span class="p">]</span> <span class="o">=</span> <span class="n">metadict</span><span class="p">[</span><span class="n">dirtyid</span><span class="p">]</span>

<span class="c"># Now that we have a list of volumes with metadata, we can select the groups of IDs</span>
<span class="c"># that we actually intend to contrast. If we want to us more or less everything,</span>
<span class="c"># this may not be necessary. But in some cases we want to use randomly sampled subsets.</span>

<span class="c"># The default condition here is</span>

<span class="c"># category2sorton = 'reviewed'</span>
<span class="c"># positive_class = 'rev'</span>
<span class="c"># sizecap = 350</span>
<span class="c"># A sizecap less than one means, no sizecap.</span>

<span class="c">### DEFACTORING FUNCTION CALL</span>
<span class="c">### IDsToUse, classdictionary = metafilter.label_classes(metadict, category2sorton, positive_class, sizecap)</span>

<span class="c">### DEFACTORING NAMESPACE </span>
<span class="n">metadict</span> <span class="o">=</span> <span class="n">cleanmetadict</span>  <span class="c"># put the data into the global namespace so execution can continue.</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>poemeta.csv
DEFACTORING: Excluding volume with id loc.ark:/13960/t8sb4zz1q
DEFACTORING: Excluding volume with id mdp.39015013402501
DEFACTORING: Excluding volume with id mdp.39015011913525
DEFACTORING: Excluding volume with id hardywessexpoems189.hardywessexpoems1898
DEFACTORING: Excluding volume with id gerardmhopkins191.gerardmhopkins1918
DEFACTORING: Excluding volume with id loc.ark:/13960/t3fx82c2q
DEFACTORING: Excluding volume with id emilydickinso.emilydickinson
DEFACTORING: Excluding volume with id ellisbell184.ellisbell1848
We have 8 volumes in missing in metadata, and
0 volumes missing in the directory.
set()

</code></pre></div></div>

<p>This above code cell is large due to a long <code class="highlighter-rouge">for</code> loop processing each row of the metadata file. At a high level, the code in this cell loads the metadata and determines which volumes to exclude in the analysis. It does this by loading the poemeta.csv file and excluding rows based upon the parameters specified in the <code class="highlighter-rouge">excludeif</code>, <code class="highlighter-rouge">excludeifnot</code>, <code class="highlighter-rouge">excludeabove</code>, and <code class="highlighter-rouge">excludebelow</code> variables. This process removed 8 volume designations from the (meta)data. The resulting output immediately above is a mixture of the author’s code and our own DEFACTORING inspection statements (marked with the comment ### DEFACTORING). We have added a print statement so we can see the IDs of the volumes being excluded in the code.</p>

<p>Beyond filtering out excluded (meta)data, this code also makes a series of normalizing decisions, that is, there are more conceptual distinctions being made (or unmade) in this code. First is the normalization of nationality, which is a clinical way of saying Underwood and Sellers lump Canada with the United States and Ireland with the UK. Nationality is not a factor in the Pace of Change analysis, but it is interesting to see this code here, it implies this code was used in other explorations of the data. Additionally, this code cell splits the <code class="highlighter-rouge">recept</code> column of the metadata file into two columns, <code class="highlighter-rouge">obscure</code> and <code class="highlighter-rouge">reviewed</code>. From what we can tell from the code and the comments, there are poems that were reviewed, and there are poems that were too obscure. Lastly, there are poems that are not in the set of reviewed poems but are nevertheless part of the canon. In the latter case the poems are set to be “known” <code class="highlighter-rouge">(obscure = 'known')</code>. According to the author’s comment this trumps the conflict when the author is known but not explicitly in the reviewed set.</p>

<p>We know that poems with the addcanon in the <code class="highlighter-rouge">recept</code> column are being excluded because they are included in the <code class="highlighter-rouge">excludeif</code> dictionary. But why? The code in the code cell above provides somewhat of an explanation:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>excludeif['recept'] = 'addcanon'
# We don't ordinarily include canonical volumes that were not in either sample.
# These are included only if we're testing the canon specifically.
</code></pre></div></div>
<p>What might be important to note here is how “great debates” in literary history about literary prestige, obscurity, and the canon are being ascribed in the code without much fanfare. There is a hard decision being made (in the blink of an eye) about the status of particular literary works. Most of these are, we suspect, fairly uncontroversial distinctions that accord with the broader community, but the code and computation enforce clear and unambiguous decisions for each an every volume. These hard decisions are pragmatically necessary to get to more interesting analysis.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="c"># Examine the original metadata file </span>

<span class="n">defactoring_volume_id</span> <span class="o">=</span> <span class="s">'wu.89099921512'</span>

<span class="n">the_croakers_metadata</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">metadata_file</span>
    <span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">metadata_file</span><span class="p">[</span><span class="s">'docid'</span><span class="p">]</span> <span class="o">==</span> <span class="n">defactoring_volume_id</span><span class="p">]</span>
    <span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="o">.</span><span class="n">sort_index</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">the_croakers_metadata</span>
</code></pre></div></div>

<div class="output output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OCLC                                         NaN
actualdate                                  1860
author                     Drake, Joseph Rodman,
birth                                       1795
canon                                        NaN
docid                             wu.89099921512
enumcron                                     NaN
firstpub                                    1860
gender                                         m
impaud                                       NaN
imprint         New York;The Bradford Club;1860.
inferreddate                                1860
judge                                        NaN
nationality                                   us
notes                                        NaN
othername                                    NaN
pubname                                      NaN
pubrev                                       NaN
recept                                    random
recordid                              8.9986e+06
title                               The croakers
yrrev                                        NaN
Name: 534, dtype: object
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="c"># Examine the cleaned metadata</span>
<span class="n">cleanmetadict</span><span class="p">[</span><span class="n">defactoring_volume_id</span><span class="p">]</span>
</code></pre></div></div>

<div class="output output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'author': 'Drake, Joseph Rodman,',
 'birthdate': 1795,
 'canonicity': 'random',
 'firstpub': 1860,
 'gender': 'm',
 'nation': 'us',
 'obscure': 'obscure',
 'pubdate': 1860,
 'pubname': '',
 'reviewed': 'not',
 'title': 'The croakers'}
</code></pre></div></div>

<p>The inspection above shows the data expressed in the CSV file has been transformed into into a Python dictionary, <code class="highlighter-rouge">cleanmetadict</code>, with additional columns for expressing more granularity about the categorizations for each poetry volume. We also observe the raw metadata csv file has additional columns that are not reflected in the Python dictionary. What we see reflected in <code class="highlighter-rouge">cleanmetadict</code> is only the metadata necessary for the analysis with any dirty or unnecessary information removed. Furthermore, the metadata now lives in a native Python data structure, a dictionary, making it easier to access specific volumes and manipulate in code.</p>

<h4 id="sorting-training-data">Sorting Training Data</h4>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING FUNCTION</span>
<span class="c">### def label_classes(metadict, category2sorton, positive_class, sizecap):</span>
<span class="s">''' This takes as input the metadata dictionary generated
by get_metadata. It subsets that dictionary into a
positive class and a negative class. Instances that belong
to neither class get ignored.
'''</span>

<span class="n">all_instances</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">metadict</span><span class="o">.</span><span class="n">keys</span><span class="p">()])</span>

<span class="c"># The first stage is to find positive instances.</span>

<span class="n">all_positives</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">metadict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">value</span><span class="p">[</span><span class="n">category2sorton</span><span class="p">]</span> <span class="o">==</span> <span class="n">positive_class</span><span class="p">:</span>
        <span class="n">all_positives</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
</code></pre></div></div>

<p>This block of code reads the metadata properties and puts a subset of all entries into a variable, <code class="highlighter-rouge">all_positives</code>, which will contain all of the volume ids for reviewed poems. If the <code class="highlighter-rouge">reviewed</code> column has a value of ‘rev’, then it is selected for inclusion. The name and value of the property are parameterized however, so technically it is more correct, but more opaque as well, to state: if a poem’s metadata has the value ‘rev’ (specified by the <code class="highlighter-rouge">positive_class</code> variable) for the reviewed property (specified by the <code class="highlighter-rouge">category2sorton</code> variable) then it is labeled as a positive. Having thus collected all the reviewed poems into the set named <code class="highlighter-rouge">all_positives</code>, the next cell populates the variable <code class="highlighter-rouge">all_negatives</code> with all the instances not in the positive set by subtracting the set of positives from the set of all instances by applying a basic mathematical set operation (-).</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">all_negatives</span> <span class="o">=</span> <span class="n">all_instances</span> <span class="o">-</span> <span class="n">all_positives</span>
<span class="n">iterator</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">all_negatives</span><span class="p">)</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">metadict</span><span class="p">[</span><span class="n">item</span><span class="p">][</span><span class="s">'reviewed'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'addedbecausecanon'</span><span class="p">:</span>
        <span class="n">all_negatives</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</code></pre></div></div>

<p>The negative labels are assigned to all instances that are not in the set of positive instances. This section includes additional code that filters out any item with <code class="highlighter-rouge">addedbecausecannon</code> set for the <code class="highlighter-rouge">reviewed</code> property, but this code should never execute because, as we have seen above, the canon should already be removed.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">sizecap</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_positives</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">sizecap</span><span class="p">:</span>
    <span class="n">positives</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">all_positives</span><span class="p">,</span> <span class="n">sizecap</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">positives</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">all_positives</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">all_positives</span><span class="p">))</span>

<span class="c"># If there's a sizecap we also want to ensure classes have</span>
<span class="c"># matching sizes and roughly equal distributions over time.</span>

<span class="n">numpositives</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_positives</span><span class="p">)</span>

<span class="k">if</span> <span class="n">sizecap</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_negatives</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">numpositives</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="s">'date'</span> <span class="ow">in</span> <span class="n">category2sorton</span><span class="p">:</span>
        <span class="n">available_negatives</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">all_negatives</span><span class="p">)</span>
        <span class="n">negatives</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">anid</span> <span class="ow">in</span> <span class="n">positives</span><span class="p">:</span>
            <span class="n">date</span> <span class="o">=</span> <span class="n">metadict</span><span class="p">[</span><span class="n">anid</span><span class="p">][</span><span class="s">'pubdate'</span><span class="p">]</span>

            <span class="n">available_negatives</span> <span class="o">=</span> <span class="n">sort_by_proximity</span><span class="p">(</span><span class="n">available_negatives</span><span class="p">,</span> 
                                                    <span class="n">metadict</span><span class="p">,</span> <span class="n">date</span><span class="p">)</span>
            <span class="n">selected_id</span> <span class="o">=</span> <span class="n">available_negatives</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">negatives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">selected_id</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c"># if we're dividing classes by date, we obvs don't want to</span>
        <span class="c"># ensure equal distributions over time.</span>

        <span class="n">negatives</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">all_negatives</span><span class="p">,</span> <span class="n">sizecap</span><span class="p">)</span>

<span class="k">else</span><span class="p">:</span>
    <span class="n">negatives</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">all_negatives</span><span class="p">)</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>360

</code></pre></div></div>

<p>Most of the code in the cell above does not actually execute because the number of entries in the <code class="highlighter-rouge">all_positives</code> and <code class="highlighter-rouge">all_negatives</code> lists are not greater than <code class="highlighter-rouge">sizecap</code>. The conditional statements on line 1 and line 12 will not be true so the the accompanying blocks of code never execute. If the <code class="highlighter-rouge">sizecap</code> variable was smaller, or the number of entries larger, this code would use random sampling to select smaller number of entries from the positives entries.</p>

<p>Looking at the block of code for the negative entries is a bit more interesting. This block of code (from lines 13 to 29) makes an unexecuted reference to a function <code class="highlighter-rouge">sort_by_proximity</code> that samples from the negative elements with an equal distribution based upon some function of proximity. Because this code is not executing we are not going to spend more time and analytical attention to exactly how this function operates. Furthermore, we have not included the code for <code class="highlighter-rouge">sort_by_proximity()</code> in the notebook because it is not part of the execution path we are tracing. In the code’s garden of forking paths, this is a path not taken.</p>

<p>These issues of code that is not executed and functions that are not called point to properties of code that make it complex and therefore difficult to review or critique. Code has a textual and a processual dimension (Hiller 2015, Van Zundert 2016). The code-as-text is what we can see and read in the source code, the processual dimension of code is tied to its execution as a software program. Code critique moves in between these two modes of existence of code. We are here, as code critics, not simply looking at code-as-text. That is, we are in this case reviewing a <em>live</em> execution of the code. This is extremely significant and, we’d argue, distinguishes defactoring as more than analysing code-as-text; we are analyzing the code, the data, and their interaction in the computation. Leveraging the affordances of the Jupyter notebook platform allow us the ability to interact with the execution environment described in the code. At each step of the incremental process building this environment we can ask it questions by inspecting the state of variables (or even change them). This is more than simply treating the code as a text, the code is but one part of a complex assemblage we have been manipulating with the authors’ code (and some of our own). However, it is also not a complete inspection of all the ways in which the code can be possibly executed. As we <em>defactor</em> the authors’s code, we make choices about how much of the code to include for the argument we are trying to make (and for the sake of our time and attention). Thus we are dealing with a code-criticism conundrum: What is the required or adequate breadth and depth of the representation of the code, and subsequently of the critique? The decision to include or not include <code class="highlighter-rouge">sort_by_proximity()</code> is a breadth issue. How broad should we be in including code that does not execute? Note that we are including code from a conditional block that does not execute, but are not going out the additional step to include non-executed functions defined elsewhere in the code. The decision to include or not include code from the standard library, code not written by the authors, is a depth issue. While there are many functions we are stepping over, like <code class="highlighter-rouge">len</code>, <code class="highlighter-rouge">list</code>, <code class="highlighter-rouge">append</code>, <code class="highlighter-rouge">pop</code>, <code class="highlighter-rouge">random.sample</code>, we argue there is no need to step into these functions because, following Hinsen’s model, they are not part of the bespoke code of Pace of Change. Again, this raises the problematic issue of our decision to step over <code class="highlighter-rouge">sort_by_proximity()</code> even though it was written by the authors’ for this particular project.</p>

<p>Obviously the ‘rules of the game’ for defactoring are not quite clear yet. Therefore we are more or less ‘feeling’ our way through an emerging methodology for code criticism. As we see vestiges of the authors’ evolution in thinking in their code, this notebook is capturing the evolution of our thinking about defactoring as a practice.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Now we have two lists of ids.</span>

<span class="n">IDsToUse</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="n">classdictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="k">print</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"We have "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">positives</span><span class="p">))</span> <span class="o">+</span> <span class="s">" positive, and"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">negatives</span><span class="p">))</span> <span class="o">+</span> <span class="s">" negative instances."</span><span class="p">)</span>

<span class="k">for</span> <span class="n">anid</span> <span class="ow">in</span> <span class="n">positives</span><span class="p">:</span>
    <span class="n">IDsToUse</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">anid</span><span class="p">)</span>
    <span class="n">classdictionary</span><span class="p">[</span><span class="n">anid</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">for</span> <span class="n">anid</span> <span class="ow">in</span> <span class="n">negatives</span><span class="p">:</span>
    <span class="n">IDsToUse</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">anid</span><span class="p">)</span>
    <span class="n">classdictionary</span><span class="p">[</span><span class="n">anid</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">metadict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">value</span><span class="p">[</span><span class="s">'reviewed'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'addedbecausecanon'</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"DEFACTORING: Adding cannon supplement"</span><span class="p">)</span> <span class="c">### DEFACTORING CODE</span>
        <span class="n">IDsToUse</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">classdictionary</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c"># We add the canon supplement, but don't train on it.</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
We have 360 positive, and
360 negative instances.

</code></pre></div></div>

<p>In this cell we are seeing yet another instance of metadata being shaped and transformed in preparation for analysis. The code first prints out the number of positive and negative instances by checking the length (using <code class="highlighter-rouge">len()</code>) of the volume ids stored in the <code class="highlighter-rouge">positives</code> and <code class="highlighter-rouge">negatives</code> variables. Two loops iterate over these lists and populate two more variables, <code class="highlighter-rouge">IDsToUse</code> and <code class="highlighter-rouge">classdictionary</code>. The first, <code class="highlighter-rouge">IDsToUse</code> contains a master list of all the volume identifiers to be used in the analysis. It is of the Python set datatype, meaning there will be no duplicate identifiers in the set list. The second, <code class="highlighter-rouge">classdictionary</code> is a dictionary that allows a simple lookup to see if a volume ID is in the positive or negative class–as indicated by a 0 or a 1. There is a final loop whose logic checks to see if any volumes have a specific metadata flag reviewed with a value of <code class="highlighter-rouge">addedbecausecanon</code>. We have added a defactoring statement to see if this logic is ever triggered. The output indicates the <code class="highlighter-rouge">if</code> statement’s conditions were never satisfied, no volumes were added because of cannon.</p>

<p>We have come to the end of the preparing metadata section. All of the code up to this point has focused on loading, normalizing, and transforming the metadata–namely the identifiers of the volumes to be analyzed. Based upon the values in the metadata fields and assumptions built into the logic of the code, the authors have assembled the list of volume ids and their associated class. Because this is a <em>supervised</em> machine learning exercise, the authors need labeled data to train the model. All of the work in this section of the code was dedicated to assigning a class label (positive or negative) to the identifiers of the data files. The next section dives into the actual data itself.</p>

<h3 id="transforming-words-into-features">Transforming Words into Features</h3>

<p>Now that we know exactly which volumes of poetry the code will be analyzing, we can venture into the datafiles and begin the work of transforming the volume data files into a data structure suitable for analysis. The logistic regression requires the data to be in a specific shape, a matrix of binary features. This section does the work of <em>getting the data into shape</em>.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING FUNCTION DEFINITIONS</span>
<span class="c">### We need to define the infer_date function</span>

<span class="k">def</span> <span class="nf">infer_date</span><span class="p">(</span><span class="n">metadictentry</span><span class="p">,</span> <span class="n">datetype</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">datetype</span> <span class="o">==</span> <span class="s">'pubdate'</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">metadictentry</span><span class="p">[</span><span class="n">datetype</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">datetype</span> <span class="o">==</span> <span class="s">'firstpub'</span><span class="p">:</span>
        <span class="n">firstpub</span> <span class="o">=</span> <span class="n">metadictentry</span><span class="p">[</span><span class="s">'firstpub'</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">firstpub</span> <span class="o">&gt;</span> <span class="mi">1700</span> <span class="ow">and</span> <span class="n">firstpub</span> <span class="o">&lt;</span> <span class="mi">1950</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">firstpub</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">metadictentry</span><span class="p">[</span><span class="s">'pubdate'</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="nb">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>This code cell defines a helper function, <code class="highlighter-rouge">infer_date()</code>, which is used in the code below to deal with differences in the <code class="highlighter-rouge">pubdate</code> and <code class="highlighter-rouge">firstpub</code> columns in the metadata. When <code class="highlighter-rouge">firstpub</code> falls between 1700 and 1950 the codes uses that as the date, otherwise it returns the value in <code class="highlighter-rouge">pubdate</code> (or it exists the script in the case of bad data).</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># make a vocabulary list and a volsize dict</span>
<span class="n">wordcounts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>

<span class="n">volspresent</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">orderedIDs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="n">positivecounts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">negativecounts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="k">for</span> <span class="n">volid</span><span class="p">,</span> <span class="n">volpath</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">volumeIDs</span><span class="p">,</span> <span class="n">volumepaths</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">volid</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">IDsToUse</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">volspresent</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">volid</span><span class="p">,</span> <span class="n">volpath</span><span class="p">))</span>
        <span class="n">orderedIDs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">volid</span><span class="p">)</span>

    <span class="n">date</span> <span class="o">=</span> <span class="n">infer_date</span><span class="p">(</span><span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">],</span> <span class="n">datetype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">date</span> <span class="o">&lt;</span> <span class="n">pastthreshold</span> <span class="ow">or</span> <span class="n">date</span> <span class="o">&gt;</span> <span class="n">futurethreshold</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">volpath</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">fields</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">fields</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">fields</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="c"># print(line)</span>
                    <span class="k">continue</span>
                <span class="n">word</span> <span class="o">=</span> <span class="n">fields</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">isalpha</span><span class="p">():</span>
                    <span class="n">count</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">fields</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                    <span class="n">wordcounts</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="c"># for initial feature selection we use the number of</span>
                    <span class="c"># *documents* that contain a given word,</span>
                    <span class="c"># so it's just +=1.</span>

<span class="n">vocablist</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">wordcounts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">numfeatures</span><span class="p">)]</span>
</code></pre></div></div>

<p>This is an important section because it contains the code that opens the data files and selects the word-features used in the logistic regression. The main block of code in the cell above loops over each data file (representing a poem) and counts the number of instances of each word. Like the metadata file, we can use defactoring to inspect a datafile to get a sense of what these data look like before they are transformed by the code.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="n">data_file</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">volumepaths</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> 
                        <span class="n">delimiter</span><span class="o">=</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> 
                        <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s">"Word"</span><span class="p">,</span> <span class="s">"Count"</span><span class="p">])</span>
<span class="n">data_file</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<div class="output output_html">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Word</th>
      <th>Count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>,</td>
      <td>3093</td>
    </tr>
    <tr>
      <th>1</th>
      <td>the</td>
      <td>1263</td>
    </tr>
    <tr>
      <th>2</th>
      <td>and</td>
      <td>988</td>
    </tr>
    <tr>
      <th>3</th>
      <td>of</td>
      <td>876</td>
    </tr>
    <tr>
      <th>4</th>
      <td>.</td>
      <td>745</td>
    </tr>
    <tr>
      <th>5</th>
      <td>!</td>
      <td>640</td>
    </tr>
    <tr>
      <th>6</th>
      <td>in</td>
      <td>521</td>
    </tr>
    <tr>
      <th>7</th>
      <td>a</td>
      <td>511</td>
    </tr>
    <tr>
      <th>8</th>
      <td>to</td>
      <td>480</td>
    </tr>
    <tr>
      <th>9</th>
      <td>i</td>
      <td>423</td>
    </tr>
  </tbody>
</table>
</div>
</div>

<p>The output above shows the first 10 lines of one of the poem data files. As we can plainly see, the volume has already been pre-processed into a list of words and their frequencies. This particular volume has 2,745 commas and 1445 instances of the word “the.” The authors’ code parses each of these files and assembles a vocabulary list of the 3200 most common words (as specified by the <code class="highlighter-rouge">numfeatures</code> variable)</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Word      Count"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"---------------"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">wordcounts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"{:8}  {:5}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">count</span><span class="p">))</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Word      Count
---------------
is          720
but         720
to          720
in          720
for         720
and         720
a           720
all         720
not         720
of          720
that        720
at          720
as          720
by          720
with        720
on          720
i           720
the         720
they        719
when        719

</code></pre></div></div>

<p>At first glance it might seem strange that the count is 720 for all of the top words in the corpus. However, when we dig deeper into the code we can see that the authors are not tabulating the total word frequencies across all volumes in the corpus, rather they are associating words and the number of volumes. The code loops over each file, opening it, and parses each line by splitting on the tab character (“”). What is interesting is that Underwood and Sellers are only paying attention to the word and ignoring the frequency within the volume. They check to see if the word is longer than zero and use the <code class="highlighter-rouge">isalpha()</code> function to make sure the characters are alphabetic as opposed to punctuation. The comments in the code explain that the authors are just using the “number of documents that contain a given word”.</p>

<p>The authors are selecting their list of features (stored in the <code class="highlighter-rouge">vocablist</code> variable) by selecting words ranked by the number of documents in which they appear. The total number of documents we are working with is 720, so the table we generated above tells us that the top ten words appear in all of the documents. If we look at more than just the top ten, we can start to see the distribution of words in documents.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">'ggplot'</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">wordcounts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">3200</span><span class="p">)])</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">bins</span><span class="o">=</span><span class="mi">72</span><span class="p">,</span> 
    <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Words  documents."</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Count"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/defactoring/070_defactoring-pace-of-change_40_0.png" alt="png" /></p>

<p>The plot above shows a histogram of the top 3,200 words and how they are expressed across corpus. The spike on the right end of this chart shows there are nearly 60 words that appear in all 720 documents (as we can see in the text table above). As a whole, the higher bars on the left side of the chart indicate most of the words appear in a smaller number of documents. Here we use defactoring as a technique to investigate and even generate intermediate representations of the data, representations implicit in the data structures created by Underwood and Sellers, but not explicitly visualized in their narrative. For our purposes, this image is an interesting chapter in the story of the data precisely because it is in the middle of Underwood and Seller’s analysis. These middle states are often glossed over in the hurried rush for analysis to generate a meaningful result. Defactoring is an effort to slow down, take a breather, and reflect upon the data-work that has happened up until this point in the code. The meandering step-by-step journey through the code sometimes reveals very interesting paths not taken, such as the commented out code block below.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># vocablist = binormal_select(vocablist, positivecounts, negativecounts, totalposvols, totalnegvols, 3000)</span>
<span class="c"># Feature selection is deprecated. There are cool things</span>
<span class="c"># we could do with feature selection,</span>
<span class="c"># but they'd improve accuracy by 1% at the cost of complicating our explanatory task.</span>
<span class="c"># The tradeoff isn't worth it. Explanation is more important.</span>
<span class="c"># So we just take the most common words (by number of documents containing them)</span>
<span class="c"># in the whole corpus. Technically, I suppose, we could crossvalidate that as well,</span>
<span class="c"># but *eyeroll*.</span>
</code></pre></div></div>

<p>The author’s code above does not actually perform any work as each line has been commented out, however we include it because it points towards an execution path not taken and an interesting rationale for why it was not followed. In the “production” code the heuristic for feature selection is to simply select the 3200 most common words by their appearance in the 720 documents. This is a simple and easy technique to implement and–more importantly–explain to a literary history and digital humanities audience. Selecting the top words is a well established practice in text analysis and it has a high degree of methodologically face validity. It is a good mechanism for removing features that have diminishing returns. However, the commented code above tells a different, and methodologically significant, story.
The comment discusses an alternative technique for feature selection using binormal selection. Because this function is commented out and not used in the analysis, we have opted to not include it as part of the defactoring. Instead, we have decided to focus on the more interesting rationale about why binormal selection is not being used in the analysis as indicated in the author’s comments:</p>

<blockquote>
  <p>There are cool things we could do with feature selection, but they’d improve accuracy by 1% at the cost of complicating our explanatory task.
The tradeoff isn’t worth it. Explanation is more important.</p>
</blockquote>

<p>This comment reveals much about the reasoning, the effort, and energy focused on the important, but in the humanities oft neglected, work of discussing methodology. As Underwood argued in <em>The literary uses of high-dimensional space</em> (Underwood 2015b), while there is enormous potential for the application of statistical methods in humanistic fields like literary history there is resistance to these methods because there is a resistance to methodology. Underwood has described the humanities disciplines relationship to methodology as an “insistence on staging methodology as ethical struggle” (Underwood 2013). In this commented code we can see the material manifestation of Underwood’s sentiment, in this case embodied by self-censorship in the decision to not use more statistically robust techniques for feature selection. We do not argue this choice compromises the analysis or final conclusions, rather we want to highlight the practical and material ways a resistance to plurality in research methods manifests in the digital humanities. By focusing on a close reading of the code and execution environment, by <em>defactoring</em>, we provide a methodology to assist with the omnipresent <em>explanatory</em> task commensurate with the use of computational research methods in the humanities.</p>

<p>In an algorithmic, data driven analysis, the selection of features is a <em>crucial</em> step because it affects the accuracy of the algorithm. In the digital humanities, feature selection is deeply embedded in the theory of the analysis and the context of the data. Claims made in and through this kind of analysis must attend to the representational configuration of the data. That is to say, we cannot take for granted how we have transformed data and what data are included or excluded from the analysis. Care, in the form of thorough documentation and thoughtful reflection, must be taken–especially at this unique moment in the development of digital humanities as we are still learning how algorithmic, data-driven techniques can be leveraged to better understand our objects of study.</p>

<h4 id="filtering-authors">Filtering Authors</h4>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">donttrainon</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="c"># Here we create a list of volumed IDs not to be used for training.</span>
<span class="c"># For instance, we have supplemented the dataset with volumes that</span>
<span class="c"># are in the Norton but that did not actually occur in random</span>
<span class="c"># sampling. We want to make predictions for these, but never use</span>
<span class="c"># them for training.</span>

<span class="k">for</span> <span class="n">idx1</span><span class="p">,</span> <span class="n">anid</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">orderedIDs</span><span class="p">):</span>
    <span class="n">reviewedstatus</span> <span class="o">=</span> <span class="n">metadict</span><span class="p">[</span><span class="n">anid</span><span class="p">][</span><span class="s">'reviewed'</span><span class="p">]</span>
    <span class="n">date</span> <span class="o">=</span> <span class="n">infer_date</span><span class="p">(</span><span class="n">metadict</span><span class="p">[</span><span class="n">anid</span><span class="p">],</span> <span class="n">datetype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reviewedstatus</span> <span class="o">==</span> <span class="s">'addedbecausecanon'</span><span class="p">:</span>
        <span class="n">donttrainon</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">date</span> <span class="o">&lt;</span> <span class="n">pastthreshold</span> <span class="ow">or</span> <span class="n">date</span> <span class="o">&gt;</span> <span class="n">futurethreshold</span><span class="p">:</span>
        <span class="n">donttrainon</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx1</span><span class="p">)</span>
</code></pre></div></div>

<p>As the comments describe, this block of code creates a list of volume IDs not to be used in the training. What that means <em>in code</em> is that any volume with the metadata label <code class="highlighter-rouge">addedbecauseofcanon</code> or with a date outside of the thresholds defined by <code class="highlighter-rouge">pastthreshold</code> and <code class="highlighter-rouge">futurethreshold</code> will be ignored. If we inspect the <code class="highlighter-rouge">donttrainon</code> variable we can see how many volumes satisfy these criteria.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The variable donttrainon contains {} volume IDs"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">donttrainon</span><span class="p">)))</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The variable donttrainon contains 0 volume IDs

</code></pre></div></div>

<p>It would appear there are no volumes to be filtered out by these criteria.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">authormatches</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">donttrainon</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">orderedIDs</span><span class="p">))]</span>
<span class="c"># For every index in authormatches, identify a set of indexes that have</span>
<span class="c"># the same author. Obvs, there will always be at least one.</span>

<span class="c"># Since we are going to use these indexes to exclude rows, we also add</span>
<span class="c"># all the ids in donttrainon to every volume</span>

<span class="k">for</span> <span class="n">idx1</span><span class="p">,</span> <span class="n">anid</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">orderedIDs</span><span class="p">):</span>
    <span class="n">thisauthor</span> <span class="o">=</span> <span class="n">metadict</span><span class="p">[</span><span class="n">anid</span><span class="p">][</span><span class="s">'author'</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">idx2</span><span class="p">,</span> <span class="n">anotherid</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">orderedIDs</span><span class="p">):</span>
        <span class="n">otherauthor</span> <span class="o">=</span> <span class="n">metadict</span><span class="p">[</span><span class="n">anotherid</span><span class="p">][</span><span class="s">'author'</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">thisauthor</span> <span class="o">==</span> <span class="n">otherauthor</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">idx2</span> <span class="ow">in</span> <span class="n">authormatches</span><span class="p">[</span><span class="n">idx1</span><span class="p">]:</span>
            <span class="n">authormatches</span><span class="p">[</span><span class="n">idx1</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">alist</span> <span class="ow">in</span> <span class="n">authormatches</span><span class="p">:</span>
    <span class="n">alist</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="c"># I am reversing the order of indexes so that I can delete them from</span>
<span class="c"># back to front, without changing indexes yet to be deleted.</span>
<span class="c"># This will become important in the modelingprocess module.</span>
</code></pre></div></div>

<p>In this block of code Underwood and Sellers group the volumes by the same author. The list <code class="highlighter-rouge">authormatches</code> is a list of lists for each volume. Each sub-list contains the IDS of all the volumes by the same author. Essentially this data structure represents the potential relations of each volume to other volumes, with that relation being “other volumes by the same author.” This raises the question, how many volumes share the same author in this corpus.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="c"># Tabular view of shared authorship</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">authormatches</span><span class="p">])</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div></div>

<div class="output output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1    579
2     82
3     30
5     15
4      8
6      6
dtype: int64
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="c"># Barchart of shared authorship</span>
<span class="n">ax</span> <span class="o">=</span> <span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">authormatches</span><span class="p">])</span>
      <span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
      <span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">"barh"</span><span class="p">,</span>
           <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">5</span><span class="p">)))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"count"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Number of volumes with same author"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/defactoring/070_defactoring-pace-of-change_52_0.png" alt="png" /></p>

<p>This histogram tells us a majority of volumes are written by unique authors but that there are some authors who have written up to six volumes in the corpus. Note, we are generating this graph by counting the length of the list containing the volume IDs of other volumes by the same author. This means volumes written by the same author are counted twice. This is not an issue for the purposes of our inspection, just that the sum total number of volumes represented by this histogram is greater than 720.</p>

<h4 id="filtering-words">Filtering Words</h4>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING DEFINITIONS</span>

<span class="n">usedate</span> <span class="o">=</span> <span class="bp">False</span>
<span class="c"># Leave this flag false unless you plan major</span>
<span class="c"># surgery to reactivate the currently-deprecated</span>
<span class="c"># option to use "date" as a predictive feature.</span>

<span class="k">def</span> <span class="nf">get_features</span><span class="p">(</span><span class="n">wordcounts</span><span class="p">,</span> <span class="n">wordlist</span><span class="p">):</span>
    <span class="n">numwords</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">wordlist</span><span class="p">)</span>
    <span class="n">wordvec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">numwords</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">wordlist</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">wordcounts</span><span class="p">:</span>
            <span class="n">wordvec</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">wordcounts</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">wordvec</span>

<span class="c"># In an earlier version of this script, we sometimes used</span>
<span class="c"># "publication date" as a feature, to see what would happen.</span>
<span class="c"># In the current version, we don't. Some of the functions</span>
<span class="c"># and features remain, but they are deprecated. E.g.:</span>

<span class="k">def</span> <span class="nf">get_features_with_date</span><span class="p">(</span><span class="n">wordcounts</span><span class="p">,</span> <span class="n">wordlist</span><span class="p">,</span> <span class="n">date</span><span class="p">,</span> <span class="n">totalcount</span><span class="p">):</span>
    <span class="n">numwords</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">wordlist</span><span class="p">)</span>
    <span class="n">wordvec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">numwords</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">wordlist</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">wordcounts</span><span class="p">:</span>
            <span class="n">wordvec</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">wordcounts</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>

    <span class="n">wordvec</span> <span class="o">=</span> <span class="n">wordvec</span> <span class="o">/</span> <span class="p">(</span><span class="n">totalcount</span> <span class="o">+</span> <span class="mf">0.0001</span><span class="p">)</span>
    <span class="n">wordvec</span><span class="p">[</span><span class="n">numwords</span><span class="p">]</span> <span class="o">=</span> <span class="n">date</span>
    <span class="k">return</span> <span class="n">wordvec</span>
</code></pre></div></div>

<p>This code cell defines two functions to be used below when opening and parsing the raw data files (in the <code class="highlighter-rouge">poems/</code> directory). The function <code class="highlighter-rouge">get_features()</code> simply takes the word counts from the parsed volume and filters out any words that are not part of <code class="highlighter-rouge">wordlist</code>, which contains the list of word features that had been selected for this analysis. We have also included a second function, <code class="highlighter-rouge">get_features_with_date()</code>, even though it is not executed. This residual code points to yet another path not taken, one that uses the volume’s publication date as a feature. As Underwood and Seller’s comment indicates, this was an experiment from an “earlier version of this script…to see what would happen.”</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">volsizes</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">voldata</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">classvector</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">for</span> <span class="n">volid</span><span class="p">,</span> <span class="n">volpath</span> <span class="ow">in</span> <span class="n">volspresent</span><span class="p">:</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">volpath</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">voldict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">totalcount</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">fields</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">fields</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">fields</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="n">word</span> <span class="o">=</span> <span class="n">fields</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">count</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">fields</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">voldict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">count</span>
            <span class="n">totalcount</span> <span class="o">+=</span> <span class="n">count</span>

    <span class="n">date</span> <span class="o">=</span> <span class="n">infer_date</span><span class="p">(</span><span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">],</span> <span class="n">datetype</span><span class="p">)</span>
    <span class="n">date</span> <span class="o">=</span> <span class="n">date</span> <span class="o">-</span> <span class="mi">1700</span>
    <span class="k">if</span> <span class="n">date</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">date</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">usedate</span><span class="p">:</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">get_features_with_date</span><span class="p">(</span><span class="n">voldict</span><span class="p">,</span> 
                                          <span class="n">vocablist</span><span class="p">,</span> 
                                          <span class="n">date</span><span class="p">,</span> 
                                          <span class="n">totalcount</span><span class="p">)</span>
        <span class="n">voldata</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">get_features</span><span class="p">(</span><span class="n">voldict</span><span class="p">,</span> <span class="n">vocablist</span><span class="p">)</span>
        <span class="n">voldata</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">features</span> <span class="o">/</span> <span class="p">(</span><span class="n">totalcount</span> <span class="o">+</span> <span class="mf">0.001</span><span class="p">))</span>


    <span class="n">volsizes</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span> <span class="o">=</span> <span class="n">totalcount</span>
    <span class="n">classflag</span> <span class="o">=</span> <span class="n">classdictionary</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span>
    <span class="n">classvector</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classflag</span><span class="p">)</span>
    
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">voldata</span><span class="p">)</span>
</code></pre></div></div>

<p>This is an important code block because we are now pulling the raw data files from the poems/ directory into memory, filtering out the unselected word features, and putting the data into a vectorized data structure. The code loops over the <code class="highlighter-rouge">volspresent</code> variable and parses each individual volume into the <code class="highlighter-rouge">voldict</code> dictionary. At this stage the code is reading in all the words of a volume including their frequencies, and it is tabulating the total number of words in that volume. Once all of the data for the volume has been read into memory, the code calls the <code class="highlighter-rouge">get_features</code> function that throws out the words not part of the selected word features stored in the <code class="highlighter-rouge">vocablist</code> variable. This is where the top 3200 words are foregrounded and the remaining, less commonly used words, are discarded.</p>

<p>At this point, any prosaic resemblance left in the data is gone and now we are dealing entirely with textual data in a numeric form.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The vector representation of {} by {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">metadict</span><span class="p">[</span><span class="n">defactoring_volume_id</span><span class="p">][</span><span class="s">'title'</span><span class="p">],</span> 
    <span class="n">metadict</span><span class="p">[</span><span class="n">defactoring_volume_id</span><span class="p">][</span><span class="s">'author'</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The vector has a length of {}."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"The first 100 elements of the vector:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">])</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The vector representation of The croakers by Drake, Joseph Rodman,
The vector has a length of 3200.
The first 100 elements of the vector:
[ 199.  146.  885.  562.  176. 1152.  509.  195.  215.  956.  288.  156.
  215.  200.  215.  159.  149. 2043.  222.   92.  171.  457.  162.  113.
  146.  118.  335.   84.  181.   93.  102.  258.   65.  159.   79.  135.
   72.   95.   61.   70.   84.   45.   73.   40.   44.   38.   88.   35.
   73.   59.   74.  100.   76.   29.   54.   55.   40.   48.   51.   54.
   21.   58.   37.   60.   63.   34.   40.   24.   63.   55.   11.   79.
   28.   47.   21.   28.   39.   19.   15.   30.   44.   15.   23.   25.
   81.   60.   33.   17.   15.   67.   33.   55.   31.   22.   55.   26.
   35.   40.   14.  117.]

</code></pre></div></div>

<p>The inspection above shows us the last volume processed by the loop, <em>The Croakers</em> by Joseph Rodman Drake. As we can see, the words for this volume of poetry are now represented as a list of numbers (representing word frequencies). However, this list of numbers still requires additional transformation in order to be consumable by logistic regression. The word frequencies need to be normalized so they are comparable across volumes. To do this Underwood and Sellers divide the frequency of each individual word (each number in the list above) by the total number of words in that volume (the <code class="highlighter-rouge">totalcount</code> variable. This makes volumes of different lengths comparable by turning absolute frequencies into relative frequencies. One thing we initially did not understand is why the value of 0.001 has been added to the <code class="highlighter-rouge">totalcount</code> variable. When we asked, it turned out this is a “lazy” way to prevent divide-by-zero errors.</p>

<p>The end result of the code we have executed thus far in the notebook is a very neat and tidy table of numbers between zero and 1.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="c"># Normalized perspective of the data</span>
<span class="n">data</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</code></pre></div></div>

<div class="output output_html">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>3190</th>
      <th>3191</th>
      <th>3192</th>
      <th>3193</th>
      <th>3194</th>
      <th>3195</th>
      <th>3196</th>
      <th>3197</th>
      <th>3198</th>
      <th>3199</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>715</th>
      <td>0.006064</td>
      <td>0.004322</td>
      <td>0.012832</td>
      <td>0.014809</td>
      <td>0.005595</td>
      <td>0.038597</td>
      <td>0.016082</td>
      <td>0.007203</td>
      <td>0.003752</td>
      <td>0.020337</td>
      <td>...</td>
      <td>0.000034</td>
      <td>0.000000</td>
      <td>0.000067</td>
      <td>0.000000</td>
      <td>0.000067</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000034</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>716</th>
      <td>0.006234</td>
      <td>0.004711</td>
      <td>0.013610</td>
      <td>0.015704</td>
      <td>0.003997</td>
      <td>0.045541</td>
      <td>0.022842</td>
      <td>0.006091</td>
      <td>0.002379</td>
      <td>0.020320</td>
      <td>...</td>
      <td>0.000048</td>
      <td>0.000048</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000143</td>
      <td>0.000048</td>
      <td>0.0</td>
      <td>0.000143</td>
      <td>0.000000</td>
      <td>0.000048</td>
    </tr>
    <tr>
      <th>717</th>
      <td>0.003002</td>
      <td>0.003458</td>
      <td>0.020750</td>
      <td>0.018532</td>
      <td>0.005024</td>
      <td>0.042088</td>
      <td>0.009266</td>
      <td>0.007243</td>
      <td>0.001436</td>
      <td>0.021403</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000261</td>
    </tr>
    <tr>
      <th>718</th>
      <td>0.007437</td>
      <td>0.004857</td>
      <td>0.011535</td>
      <td>0.011839</td>
      <td>0.003643</td>
      <td>0.019200</td>
      <td>0.012370</td>
      <td>0.004326</td>
      <td>0.003794</td>
      <td>0.022236</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000152</td>
      <td>0.000076</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000076</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>719</th>
      <td>0.004548</td>
      <td>0.003337</td>
      <td>0.020227</td>
      <td>0.012845</td>
      <td>0.004022</td>
      <td>0.026329</td>
      <td>0.011633</td>
      <td>0.004457</td>
      <td>0.004914</td>
      <td>0.021849</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000023</td>
      <td>0.000046</td>
      <td>0.000023</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000069</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 3200 columns</p>
</div>
</div>

<p>The last row in that table, 719, is the volume we have been tracking, <em>The Croakers</em> by Joseph Rodman Drake. It is just one of 720 relatively indistinguishable rows of numbers in this representation of 19th century poetry. This is a radical transformation of the original, prosaic representation literary historians are probably used to seeing:</p>

<p><img src="notebook_resources/the-croakers.png" alt="Screenshot of the Google Books site for The Croakers" /></p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sextuplets</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">volid</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">orderedIDs</span><span class="p">):</span>
    <span class="n">listtoexclude</span> <span class="o">=</span> <span class="n">authormatches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">asixtuple</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> 
                 <span class="n">classvector</span><span class="p">,</span> 
                 <span class="n">listtoexclude</span><span class="p">,</span> 
                 <span class="n">i</span><span class="p">,</span> 
                 <span class="n">usedate</span><span class="p">,</span> 
                 <span class="n">regularization</span><span class="p">)</span>
    <span class="n">sextuplets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">asixtuple</span><span class="p">)</span>
</code></pre></div></div>

<p>This is the last step before Underwood and Sellers’s code moves away from the transformation of features and into the actual analysis of the data. This bit of code gathers all of the relevant data and metadata that has been cleaned and normalized in a structure suitable for performing the statistical analysis. The <code class="highlighter-rouge">sextuplets</code> variable is a list of 720 tuples containing six elements. Each item in the <code class="highlighter-rouge">sextuplets</code> list contains the necessary data structures to model each poem. The contents of each item in the list is as follows:</p>

<ul>
  <li><code class="highlighter-rouge">data</code>: a normalized feature matrix. Word features are the columns and volumes are the rows with dimensions of 720 x 3200.</li>
  <li><code class="highlighter-rouge">classvector</code>: the classification or labels of volumes as either ‘reviewed’ (1) or ‘random’ (0).</li>
  <li><code class="highlighter-rouge">listtoexclude</code>: the list of poems to ignore because they are the same author.</li>
  <li>``i: the index of the volume</li>
  <li><code class="highlighter-rouge">usedate</code>: a flag indicating if date is a feature. It is false in this analysis.</li>
  <li><code class="highlighter-rouge">regularization</code>: a parameter for the logistic regression. This value was hardcoded at the beginning of the code in the Setting Parameters section.</li>
</ul>

<p>With all of the data assembled and in the right shape, a process we call <em>data fitness</em>, we can now venture into the algorithmic territory and perform the statistical analysis. As we can see, the <em>fitted</em> representation of features has traveled a great distance from the original poetry. One of the most important aspects of <em>distant reading</em> is the work of cleaning, preparing, and normalizing texts to be “read” by an algorithm. When considering distance, we should think not only of the perspective that we, the analyst, are reading from, but also the distance traveled in terms of successive transformations and representations of the data. If computational literary history is a triathalon, we have only completed the first endurance test.</p>

<h3 id="training-predictive-models">Training Predictive Models</h3>

<p>We are now about to dive into the very heart of the analysis, training predictive models on each volume. The code cells below do the following:</p>

<ul>
  <li>Iterate over every volume in the corpus. For each volume, do the following:
    <ul>
      <li>Create a training set by removing the selected volume and other volumes by the same author from the corpus (performed by the <code class="highlighter-rouge">sliceframe()</code> function). In the language of predictive modeling, the removed volume is the “held out” data.</li>
      <li>Normalize the training data by computing the z-score for each feature/feature set (performed by the <code class="highlighter-rouge">normalizearray()</code> function).</li>
      <li>Fit the model on the training data (performed by the <code class="highlighter-rouge">model_one_volume()</code> function).</li>
      <li>Use the fitted model to predict the probability the (normalized) held out data was “reviewed” or “random.”</li>
    </ul>
  </li>
</ul>

<p>What is important to understand is that this section of the code does not train a single model, rather it trains 720 independent models–one for each volume. This process is called “leave-one-out” cross validation. As the code loops over the volumes, it holds out one volume, trains the model on all of the remaining volumes, and then uses that model to predict the status of the held out volume. Lather, Rinse, Repeat.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING DEFINITION</span>

<span class="k">def</span> <span class="nf">sliceframe</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">yvals</span><span class="p">,</span> <span class="n">excludedrows</span><span class="p">,</span> <span class="n">testrow</span><span class="p">):</span>
    <span class="n">numrows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span>
    <span class="n">newyvals</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">yvals</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">excludedrows</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">newyvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="c"># NB: This only works if we assume that excluded rows</span>
        <span class="c"># has already been sorted in descending order !!!!!!!</span>
        <span class="c"># otherwise indexes will slide around as you delete</span>

    <span class="n">trainingset</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">excludedrows</span><span class="p">])</span>

    <span class="n">newyvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">newyvals</span><span class="p">)</span>
    <span class="n">testset</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">testrow</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">trainingset</span><span class="p">,</span> <span class="n">newyvals</span><span class="p">,</span> <span class="n">testset</span>
</code></pre></div></div>

<p>This function prepares the data for training one model by separating volumes used for training from a single volume held out to test the predictive power of the model. The function takes a dataframe containing the feature vectors, a list of the classifications for each volume, a list of volumes to exclude (because of shared authorship), and the the index of the specific volume to be held out. It returns the dataframe with the held out volume removed (<code class="highlighter-rouge">trainingset</code>), a list of the known classifications (<code class="highlighter-rouge">newyvals</code>) corresponding to the training set, and the held-out volume that will be classified once the model has been trained (<code class="highlighter-rouge">testset</code>).</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING DEFINITION</span>

<span class="k">def</span> <span class="nf">normalizearray</span><span class="p">(</span><span class="n">featurearray</span><span class="p">,</span> <span class="n">usedate</span><span class="p">):</span>
    <span class="s">'''Normalizes an array by centering on means and
    scaling by standard deviations. Also returns the
    means and standard deviations for features.
    '''</span>

    <span class="n">numinstances</span><span class="p">,</span> <span class="n">numfeatures</span> <span class="o">=</span> <span class="n">featurearray</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">means</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">stdevs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">lastcolumn</span> <span class="o">=</span> <span class="n">numfeatures</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">featureidx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numfeatures</span><span class="p">):</span>

        <span class="n">thiscolumn</span> <span class="o">=</span> <span class="n">featurearray</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span> <span class="p">:</span> <span class="p">,</span> <span class="n">featureidx</span><span class="p">]</span>
        <span class="n">thismean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">thiscolumn</span><span class="p">)</span>

        <span class="n">thisstdev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">thiscolumn</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">usedate</span><span class="p">)</span> <span class="ow">or</span> <span class="n">featureidx</span> <span class="o">!=</span> <span class="n">lastcolumn</span><span class="p">:</span>
            <span class="c"># If we're using date we don't normalize the last column.</span>
            <span class="n">means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thismean</span><span class="p">)</span>
            <span class="n">stdevs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thisstdev</span><span class="p">)</span>
            <span class="n">featurearray</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span> <span class="p">:</span> <span class="p">,</span> <span class="n">featureidx</span><span class="p">]</span> <span class="o">=</span> \
                <span class="p">(</span><span class="n">thiscolumn</span> <span class="o">-</span> <span class="n">thismean</span><span class="p">)</span> <span class="o">/</span> <span class="n">thisstdev</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'FLAG'</span><span class="p">)</span>
            <span class="n">means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thismean</span><span class="p">)</span>
            <span class="n">thisstdev</span> <span class="o">=</span> <span class="mf">0.1</span>
            <span class="n">stdevs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">thisstdev</span><span class="p">)</span>
            <span class="n">featurearray</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span> <span class="p">:</span> <span class="p">,</span> <span class="n">featureidx</span><span class="p">]</span> <span class="o">=</span> \
                <span class="p">(</span><span class="n">thiscolumn</span> <span class="o">-</span> <span class="n">thismean</span><span class="p">)</span> <span class="o">/</span> <span class="n">thisstdev</span>
            <span class="c"># We set a small stdev for date.</span>

    <span class="k">return</span> <span class="n">featurearray</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">stdevs</span>
</code></pre></div></div>

<p>This function standardizes the features by computing the z-score for the feature vectors. That is, it loops over each column of the data, subtracts the column mean from each value, and then divides that value by the standard deviation. This is an important step in the data preparation pipeline because it ensures all of the data values are on the same scale.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING DEFINITION</span>

<span class="k">def</span> <span class="nf">model_one_volume</span><span class="p">(</span><span class="n">data5tuple</span><span class="p">):</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">classvector</span><span class="p">,</span> <span class="n">listtoexclude</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">usedate</span><span class="p">,</span> <span class="n">regularization</span> <span class="o">=</span> \
        <span class="n">data5tuple</span>
    <span class="n">trainingset</span><span class="p">,</span> <span class="n">yvals</span><span class="p">,</span> <span class="n">testset</span> <span class="o">=</span> <span class="n">sliceframe</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> 
                                             <span class="n">classvector</span><span class="p">,</span> 
                                             <span class="n">listtoexclude</span><span class="p">,</span> 
                                             <span class="n">i</span><span class="p">)</span>
    <span class="n">newmodel</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="n">regularization</span><span class="p">)</span>
    <span class="n">trainingset</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">stdevs</span> <span class="o">=</span> <span class="n">normalizearray</span><span class="p">(</span><span class="n">trainingset</span><span class="p">,</span> <span class="n">usedate</span><span class="p">)</span>
    <span class="n">newmodel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingset</span><span class="p">,</span> <span class="n">yvals</span><span class="p">)</span>

    <span class="n">testset</span> <span class="o">=</span> <span class="p">(</span><span class="n">testset</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span> <span class="o">/</span> <span class="n">stdevs</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">newmodel</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">testset</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="c"># print(str(i) + "  -  " + str(len(listtoexclude)))</span>
    <span class="k">return</span> <span class="n">prediction</span>
</code></pre></div></div>

<p>In many respects, this is the most salient block of code in the entire document. The code above actually runs the logistic regression and does the algorithmic work that generates a prediction about each individual volume. This function builds upon the two previous functions to assemble a normalized set of training data (<code class="highlighter-rouge">trainingset</code>) distinct from the single volume to be predicted (<code class="highlighter-rouge">testset</code>).</p>

<p>There are three lines of code involved in the computational modeling of the data. First, Underwood and Sellers instantiate a model object with the regularization parameter (more on that below):</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>newmodel = LogisticRegression(C = regularization)
</code></pre></div></div>
<p>Then they “fit” the model using the normalized training data:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>newmodel.fit(trainingset, yvals)
</code></pre></div></div>
<p>Once a model has been “fit” to the data they can use that model to make predictions about unseen or held-out data. This is what they do with the <code class="highlighter-rouge">predict_proba()</code> function in this line:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prediction = newmodel.predict_proba(testset.reshape(1, -1))[0][1]
</code></pre></div></div>
<p>Those three lines are all it takes to do the computational part of of the analysis, the rest of the code up until this point has all been data preparation, cleaning, normalization, and re-shaping. This ratio of analytical to preparative code is interesting and indicates that claims that machines are eradicating scholars’ jobs are greatly exaggerated (Basken 2017).</p>

<h4 id="regularization-and-logistic-regression">Regularization and Logistic Regression</h4>

<p>The three lines of code above hide a significant amount of intellectual and computational work. The call to the <code class="highlighter-rouge">newmodel.fit()</code> function is a crucial step in the analytical process. Underwood and Sellers are using an implementation of Logistic Regression from the 3rd party Python library scikit-learn.</p>

<p>At a very high level, logistic regression is a machine learning algorithm for performing classification. Logistic regression works by estimating the parameters of a function, the <em>hypothesis representation</em>, that divides a multidimensional space into two parts (note, in this case we are talking about binomial or binary logistic regression, which classifies things into one of two bins). The hypothesis representation describes a line that winds its way through the space creating what is called the <em>decision boundary</em>. Every data point that lands on one side the boundary gets one label and every data point on the other side of the boundary gets the other label. Similar to linear regression, the goal is to find the best hypothesis representation, that is, the function that best draws a line dividing the space given the known data points. Once you have a good hypothesis representation, an appropriately <em>fit</em> model, you can begin to classify <em>new</em> data by dropping data points into the multidimensional space and seeing on which side of the decision boundary they land.</p>

<p>The key to logistic regression is estimating the parameters of the hypothesis representation–the parameters to the function that draws a line through the multidimensional space. We can derive the parameters by using the <em>features</em> of existing data combined with their known labels; this is called <em>training data</em>. The modeling process is executed by the function call to <code class="highlighter-rouge">newmodel.fit(trainingset, yvals)</code>. In Underwood and Sellers’s code the model uses the training data—the matrix of word features in the data variable and known labels (‘reviewed’ or ‘random’) in the classvector variable—to “learn” the parameters through a process called <em>coordinate descent</em>. How so-called optimization functions  work is well beyond the scope of the discussion.</p>

<h4 id="overfitting">Overfitting</h4>

<p>One of the problems when fitting a logistic regression model is a tendency towards <em>overfitting</em>. Crudely this means the model, the function with the learned parameters, that you estimated have tailored themselves such that they are overly optimized to the particular training data you provided. As such, the model becomes less useful for prediction or classifying new data because they are outside the fitness of the model. An overfit model is like a snug pair of jeans, once you put on a few pounds (add new data) they don’t fit. In Underwood and Sellers’s case, they are fitting models on all volumes except one, which is held out. Then they test the predictive performance of the model by seeing if it correctly classifies the held-out volume. If they overfit the models, the model will to a terrible job guessing the status of the held out volumes.</p>

<p>When Underwood and Sellers instantiated the model (<code class="highlighter-rouge">newmodel = LogisticRegression(C = regularization)</code>), they set a regularization parameter on the model. Regularization is a technique for logistic regression (and other machine learning algorithms) that smooths out the tendency toward overfitting with some more mathematical gymnastics. The diagram below shows how regularization can help with the fitness of the model:</p>

<p><img src="notebook_resources/regression_figures.png" alt="Regularization" />
<em>On the left side is a linear regression which doesn’t quite fit the data. In the middle is an overfit logistic regression. On right side is a regularized logistic regression.</em></p>

<p>As the diagrams show, the regularized logistic expression (the right side) does have a bit of error, there are pink and blue dots on the wrong sides of the decision boundary, but as more data get added it will generally be more right than the overfitted model as represented by the middle diagram (the squiggly decision boundary).</p>

<h4 id="running-the-regression">Running the Regression</h4>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Now do leave-one-out predictions.</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Beginning multiprocessing.'</span><span class="p">)</span>

<span class="n">pool</span> <span class="o">=</span> <span class="n">Pool</span><span class="p">(</span><span class="n">processes</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">pool</span><span class="o">.</span><span class="n">map_async</span><span class="p">(</span><span class="n">model_one_volume</span><span class="p">,</span> <span class="n">sextuplets</span><span class="p">)</span>

<span class="c"># After all files are processed, write metadata, errorlog, and counts of phrases.</span>
<span class="n">res</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
<span class="n">resultlist</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>

<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">resultlist</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">orderedIDs</span><span class="p">)</span>

<span class="n">logisticpredictions</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">volid</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">orderedIDs</span><span class="p">):</span>
    <span class="n">logisticpredictions</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span> <span class="o">=</span> <span class="n">resultlist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">pool</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="n">pool</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Multiprocessing concluded.'</span><span class="p">)</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Beginning multiprocessing.
0
50
100
150
200
250
300
350
450
500
400
550
600
650
700
Multiprocessing concluded.

</code></pre></div></div>

<p>This code automates the training of 720 volumes, holding out one volume, training the model on the remaining volumes, and then making a prediction for the held-out volume. As a very computation intensive process training or fitting a logistic regression model takes time, and training 720 different models obviously takes 720 times longer. Fortunately, this is a so called embarrassingly parallel computational task and so we can train the models using parallel processing instead of one after the other. Using Python’s built in parallel processing modules, this code can speed up the process. Still, this block of code takes a fair amount of time to execute, around twenty minutes on a quad core MacBook Pro (late 2013 model).</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="k">print</span><span class="p">(</span><span class="s">"There are {} predictions."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">logisticpredictions</span><span class="p">)))</span>

</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>There are 720 predictions.

</code></pre></div></div>

<p>What emerges from the other side of this computationally intensive task are a series of predictions, 720 to be specific, one for each of the modeled volumes. These predictions, stored in the <code class="highlighter-rouge">logisticpredictions</code> variable, are the model’s assertions of each volume’s reviewed status. Additionally, because we already know the status of the modeled volumes we can compare the performance of the predictive model to the “ground truth” and see if the algorithm was able to detect patterns.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">truepositives</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">truenegatives</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">falsepositives</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">falsenegatives</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">allvolumes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">outputpath</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="s">'w'</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">writer</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">header</span> <span class="o">=</span> <span class="p">[</span><span class="s">'volid'</span><span class="p">,</span> 
              <span class="s">'reviewed'</span><span class="p">,</span> 
              <span class="s">'obscure'</span><span class="p">,</span> 
              <span class="s">'pubdate'</span><span class="p">,</span> 
              <span class="s">'birthdate'</span><span class="p">,</span> 
              <span class="s">'gender'</span><span class="p">,</span> 
              <span class="s">'nation'</span><span class="p">,</span> 
              <span class="s">'allwords'</span><span class="p">,</span> 
              <span class="s">'logistic'</span><span class="p">,</span> 
              <span class="s">'author'</span><span class="p">,</span> 
              <span class="s">'title'</span><span class="p">,</span> 
              <span class="s">'pubname'</span><span class="p">,</span> 
              <span class="s">'actually'</span><span class="p">,</span> 
              <span class="s">'realclass'</span><span class="p">]</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">header</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">volid</span> <span class="ow">in</span> <span class="n">IDsToUse</span><span class="p">:</span>
        <span class="n">metadata</span> <span class="o">=</span> <span class="n">metadict</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span>
        <span class="n">reviewed</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s">'reviewed'</span><span class="p">]</span>
        <span class="n">obscure</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s">'obscure'</span><span class="p">]</span>
        <span class="n">pubdate</span> <span class="o">=</span> <span class="n">infer_date</span><span class="p">(</span><span class="n">metadata</span><span class="p">,</span> <span class="n">datetype</span><span class="p">)</span>
        <span class="n">birthdate</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s">'birthdate'</span><span class="p">]</span>
        <span class="n">gender</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s">'gender'</span><span class="p">]</span>
        <span class="n">nation</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s">'nation'</span><span class="p">]</span>
        <span class="n">author</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s">'author'</span><span class="p">]</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s">'title'</span><span class="p">]</span>
        <span class="n">canonicity</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s">'canonicity'</span><span class="p">]</span>
        <span class="n">pubname</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s">'pubname'</span><span class="p">]</span>
        <span class="n">allwords</span> <span class="o">=</span> <span class="n">volsizes</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span>
        <span class="n">logistic</span> <span class="o">=</span> <span class="n">logisticpredictions</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span>
        <span class="n">realclass</span> <span class="o">=</span> <span class="n">classdictionary</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span>
        <span class="n">outrow</span> <span class="o">=</span> <span class="p">[</span><span class="n">volid</span><span class="p">,</span> 
                  <span class="n">reviewed</span><span class="p">,</span> 
                  <span class="n">obscure</span><span class="p">,</span> 
                  <span class="n">pubdate</span><span class="p">,</span>
                  <span class="n">birthdate</span><span class="p">,</span> 
                  <span class="n">gender</span><span class="p">,</span> 
                  <span class="n">nation</span><span class="p">,</span> 
                  <span class="n">allwords</span><span class="p">,</span> 
                  <span class="n">logistic</span><span class="p">,</span> 
                  <span class="n">author</span><span class="p">,</span> 
                  <span class="n">title</span><span class="p">,</span> 
                  <span class="n">pubname</span><span class="p">,</span> 
                  <span class="n">canonicity</span><span class="p">,</span> 
                  <span class="n">realclass</span><span class="p">]</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">outrow</span><span class="p">)</span>
        <span class="n">allvolumes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">outrow</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">logistic</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="ow">and</span> <span class="n">classdictionary</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="n">truepositives</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">logistic</span> <span class="o">&lt;=</span> <span class="mf">0.5</span> <span class="ow">and</span> <span class="n">classdictionary</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="n">truenegatives</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">logistic</span> <span class="o">&lt;=</span> <span class="mf">0.5</span> <span class="ow">and</span> <span class="n">classdictionary</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="n">falsenegatives</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">logistic</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="ow">and</span> <span class="n">classdictionary</span><span class="p">[</span><span class="n">volid</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="n">falsepositives</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<p>This code is a bit simpler than its predecessors. This block writes a CSV file to disk containing 720 rows of volume metadata, the predicted classification, and the actual classification.</p>

<h3 id="modeling-coefficients">Modeling Coefficients</h3>

<p>The code below generates a single logistic regression model, trained on all of the data with nothing held-out. The properties of this model, the coefficients of the hypothesis representation, are interrogated to better understand the influence of individual features, words, on reviewed or unreviewed volumes. Thus this individual model is not using computational modeling to predict a phenomena, it is using the computational model to explore and explain patterns and features of the phenomena.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">donttrainon</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">trainingset</span><span class="p">,</span> <span class="n">yvals</span><span class="p">,</span> <span class="n">testset</span> <span class="o">=</span> <span class="n">sliceframe</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> 
                                         <span class="n">classvector</span><span class="p">,</span> 
                                         <span class="n">donttrainon</span><span class="p">,</span> 
                                         <span class="mi">0</span><span class="p">)</span>
<span class="n">newmodel</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="n">regularization</span><span class="p">)</span>
<span class="n">trainingset</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">stdevs</span> <span class="o">=</span> <span class="n">normalizearray</span><span class="p">(</span><span class="n">trainingset</span><span class="p">,</span> <span class="n">usedate</span><span class="p">)</span>
<span class="n">newmodel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingset</span><span class="p">,</span> <span class="n">yvals</span><span class="p">)</span>

<span class="n">coefficients</span> <span class="o">=</span> <span class="n">newmodel</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>

<span class="n">coefficientuples</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">coefficients</span><span class="p">,</span> 
                            <span class="p">(</span><span class="n">coefficients</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">stdevs</span><span class="p">)),</span> 
                            <span class="n">vocablist</span> <span class="o">+</span> <span class="p">[</span><span class="s">'pub.date'</span><span class="p">]))</span>
<span class="n">coefficientuples</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">coefficient</span><span class="p">,</span> <span class="n">normalizedcoef</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">coefficientuples</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">word</span> <span class="o">+</span> <span class="s">" :  "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">coefficient</span><span class="p">))</span>

<span class="k">print</span><span class="p">()</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">truepositives</span> <span class="o">+</span> <span class="n">truenegatives</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">IDsToUse</span><span class="p">)</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

</code></pre></div></div>

<p>This code functions much like the code in the <code class="highlighter-rouge">model_one_volume(</code>) function except it only trains a single model for the purposes of investigating the impact of particular words on the prediction. By inspecting the magnitude of the coefficients Underwood and Sellers can see how particular words influenced a positive or negative prediction. Looking at the code, specifically the call to <code class="highlighter-rouge">sliceframe()</code> reveals this model actually does have some hold-out data, the first volume at index zero. We suspect the cost of excluding a single volume is less than the effort of re-implementing the <code class="highlighter-rouge">sliceframe()</code> function. The code to instantiate and train the model is identical to the code above, but instead of predicting the status of the held-out data the code extracts the coefficients from the model and puts them in the <code class="highlighter-rouge">coefficients</code> variable.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="k">print</span><span class="p">(</span><span class="s">"There are {} coefficients in this model."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">coefficientuples</span><span class="p">)))</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>There are 3200 coefficients in this model.

</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="k">print</span><span class="p">(</span><span class="s">"First ten items in the list of coefficients."</span><span class="p">)</span>
<span class="n">coefficientuples</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>First ten items in the list of coefficients.

</code></pre></div></div>

<div class="output output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[(-0.451744515308058, -1531.5503056343666, "i'll"),
 (-0.42711374653208284, -7254.811741867888, 'mission'),
 (-0.4116879738928419, -862.7068493207225, 'tis'),
 (-0.37939299320003567, -1125.1778023608326, 'oft'),
 (-0.3783134051280357, -3337.335314702983, 'greet'),
 (-0.37399562365121103, -2558.9492503627807, 'wondrous'),
 (-0.3712630706903529, -222.8852087160384, 'our'),
 (-0.3695437312947107, -4356.707179481598, 'anxious'),
 (-0.35904965455922144, -2087.1252362010537, 'cheer'),
 (-0.35632804949908553, -1383.450564130784, "we'll")]
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING INSPECTION</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Last ten items in the list of coefficients."</span><span class="p">)</span>
<span class="n">coefficientuples</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span>
</code></pre></div></div>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Last ten items in the list of coefficients.

</code></pre></div></div>

<div class="output output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[(0.3726635222640156, 4799.590619119897, 'brows'),
 (0.381170570302538, 224.41690606199722, 'not'),
 (0.3825388340735533, 1149.152152033426, 'half'),
 (0.38439578227360166, 398.793247462678, 'what'),
 (0.38473402074228275, 6977.874918531145, 'whereon'),
 (0.39921722974326485, 3171.5809993565736, 'sake'),
 (0.403144385914046, 2023.7759161873807, 'slow'),
 (0.40674741624813554, 3933.908677099459, 'hollow'),
 (0.41261956910143927, 1531.930520538605, 'black'),
 (0.4525125027699761, 615.0067489318402, 'eyes')]
</code></pre></div></div>

<p>The coefficients are a list of numbers, one per word feature, that determine the shape of the line through the multidimensional space. These results tell us the influence of particular words in the classification of volumes in the corpus. Looking at the normalized values helps us understand the degree to which particular words are more or less associated with reviewed or unreviewed poetry.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coefficientpath</span> <span class="o">=</span> <span class="n">outputpath</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'.csv'</span><span class="p">,</span> <span class="s">'.coefs.csv'</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">coefficientpath</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="s">'w'</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">writer</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">triple</span> <span class="ow">in</span> <span class="n">coefficientuples</span><span class="p">:</span>
        <span class="n">coef</span><span class="p">,</span> <span class="n">normalizedcoef</span><span class="p">,</span> <span class="n">word</span> <span class="o">=</span> <span class="n">triple</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">([</span><span class="n">word</span><span class="p">,</span> <span class="n">coef</span><span class="p">,</span> <span class="n">normalizedcoef</span><span class="p">])</span>


<span class="c">### DEFACTORING NAMESPACE</span>
<span class="n">rawaccuracy</span> <span class="o">=</span> <span class="n">accuracy</span> 
</code></pre></div></div>

<p>This code generates the <code class="highlighter-rouge">mainmodelcoefficients.csv</code> output file, which contains the word, its coefficient, and its normalized coefficients.</p>

<h3 id="plotting-results">Plotting Results</h3>

<p>The final function of the analysis is to test the accuracy of the model(s). This code produces a plot giving a sense of how the model performed compared to the known classifications.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### DEFACTORING FUNCTION PARAMETERS</span>
<span class="n">modeltype</span> <span class="o">=</span> <span class="s">'linear'</span>
<span class="n">datelimits</span> <span class="o">=</span> <span class="p">[]</span>
<span class="s">''' Takes a set of predictions produced by a model that knows nothing about date,
and divides it along a line with a diachronic tilt. We need to do this in a way
that doesn't violate crossvalidation. I.e., we shouldn't "know" anything
that the model didn't know. We tried a couple of different ways to do this, but
the simplest and actually most reliable is to divide the whole dataset along a
linear central trend line for the data!
'''</span>



<span class="n">listofrows</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">classvector</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="c"># DEPRECATED</span>
<span class="c"># if modeltype == 'logistic' and len(datelimits) == 2:</span>
<span class="c">#     # In this case we construct a subset of data to model on.</span>
<span class="c">#     tomodeldata = list()</span>
<span class="c">#     tomodelclasses = list()</span>
<span class="c">#     pastthreshold, futurethreshold = datelimits</span>

<span class="k">for</span> <span class="n">volume</span> <span class="ow">in</span> <span class="n">allvolumes</span><span class="p">:</span>
    <span class="n">date</span> <span class="o">=</span> <span class="n">volume</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">logistic</span> <span class="o">=</span> <span class="n">volume</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span>
    <span class="n">realclass</span> <span class="o">=</span> <span class="n">volume</span><span class="p">[</span><span class="mi">13</span><span class="p">]</span>
    <span class="n">listofrows</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">logistic</span><span class="p">,</span> <span class="n">date</span><span class="p">])</span>
    <span class="n">classvector</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">realclass</span><span class="p">)</span>

    <span class="c"># DEPRECATED</span>
    <span class="c"># if modeltype == 'logistic' and len(datelimits) == 2:</span>
    <span class="c">#     if date &gt;= pastthreshold and date &lt;= futurethreshold:</span>
    <span class="c">#         tomodeldata.append([logistic, date])</span>
    <span class="c">#         tomodelclasses.append(realclass)</span>

<span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">listofrows</span><span class="p">)]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.02</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.02</span><span class="p">])</span>
<span class="n">reviewedx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">reviewedy</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">randomx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">randomy</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">reviewcode</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classvector</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">reviewcode</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">reviewedx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">reviewedy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">randomx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">randomy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">reviewedx</span><span class="p">,</span> <span class="n">reviewedy</span><span class="p">,</span> <span class="s">'ro'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">randomx</span><span class="p">,</span> <span class="n">randomy</span><span class="p">,</span> <span class="s">'k+'</span><span class="p">)</span>

<span class="k">if</span> <span class="n">modeltype</span> <span class="o">==</span> <span class="s">'logistic'</span><span class="p">:</span>
    <span class="c"># all this is DEPRECATED</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Hey, you're attempting to use the logistic-tilt option"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"that we deactivated. Go in and uncomment the code."</span><span class="p">)</span>

    <span class="c"># if len(datelimits) == 2:</span>
    <span class="c">#     data = pd.DataFrame(tomodeldata)</span>
    <span class="c">#     responsevariable = tomodelclasses</span>
    <span class="c"># else:</span>
    <span class="c">#     data = pd.DataFrame(listofrows)</span>
    <span class="c">#     responsevariable = classvector</span>

    <span class="c"># newmodel = LogisticRegression(C = 100000)</span>
    <span class="c"># newmodel.fit(data, responsevariable)</span>
    <span class="c"># coefficients = newmodel.coef_[0]</span>

    <span class="c"># intercept = newmodel.intercept_[0] / (-coefficients[0])</span>
    <span class="c"># slope = coefficients[1] / (-coefficients[0])</span>

    <span class="c"># p = np.poly1d([slope, intercept])</span>

<span class="k">elif</span> <span class="n">modeltype</span> <span class="o">==</span> <span class="s">'linear'</span><span class="p">:</span>
    <span class="c"># what we actually do</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="s">"b-"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">block</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float64'</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float64'</span><span class="p">)</span>
<span class="n">classvector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">classvector</span><span class="p">)</span>
<span class="n">dividingline</span> <span class="o">=</span> <span class="n">intercept</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">slope</span><span class="p">)</span>
<span class="n">predicted_as_reviewed</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">&gt;</span> <span class="n">dividingline</span><span class="p">)</span>
<span class="n">really_reviewed</span> <span class="o">=</span> <span class="p">(</span><span class="n">classvector</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">predicted_as_reviewed</span> <span class="o">==</span> <span class="n">really_reviewed</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">classvector</span><span class="p">)</span>

<span class="c">### DEFACTORING NAMESPACE</span>
<span class="n">tiltaccuracy</span> <span class="o">=</span> <span class="n">accuracy</span> 

<span class="k">print</span><span class="p">(</span><span class="s">'If we divide the dataset with a horizontal line at 0.5, accuracy is: '</span><span class="p">,</span> 
      <span class="nb">str</span><span class="p">(</span><span class="n">rawaccuracy</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Divided with a line fit to the data trend, it's "</span><span class="p">,</span> 
      <span class="nb">str</span><span class="p">(</span><span class="n">tiltaccuracy</span><span class="p">))</span>
</code></pre></div></div>

<p class="output output_png"><img src="../images/defactoring/070_defactoring-pace-of-change_91_0.png" alt="png" /></p>

<div class="output output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>If we divide the dataset with a horizontal line at 0.5, accuracy is:  0.7736111111111111
Divided with a line fit to the data trend, it's  0.7916666666666666

</code></pre></div></div>

<p>The code above generates a best fit line for all of the results. Underwood and Sellers then selects the predicted-as-reviewed volumes, which are the volumes falling above the dividing line. The accuracy of the dividing line is the ratio between the total number of volumes predicted-as-reviewed and the number of actually reviewed volumes. Note that this is the accuracy of the dividing line, not the accuracy of the model(s) to predict the reviewed status of the poems.</p>

              <nav class="c-page__nav">
  
    
    <a id="js-page__nav__prev" class="c-page__nav__prev" href="/defactoring/060_how-to-read-the-defactoring-notebook">
      〈 <span class="u-margin-right-tiny"></span> How to Read the Defactoring Notebook
    </a>
  

  
    
    <a id="js-page__nav__next" class="c-page__nav__next" href="/defactoring/080_discussion-and-conclusion">
      Discussion and Conclusion <span class="u-margin-right-tiny"></span> 〉
    </a>
  
</nav>

            </div>
          </div>
        </div>
      </main>
    </div>

  </body>
</html>
